<!doctype html><html><head><title>[DL & CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><meta name=description content="[DL & CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/neural-computation/basic-neuro-knowledge/>Neuroscience Basic Knowledge</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/deep-learning/>DL</a><ul><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/life-learning/>Life Learning</a><ul><li><a href=/posts/life-learning/lecture_james_mcclleland/>Lecture James McClleland</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/paper-reading/>Paper Reading</a><ul class=active><li><a class=active href=/posts/paper-reading/integrated_understanding_system/>Integrated understanding system</a></li><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ztqakita.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ztq.png><h5 class=author-name>Brandon Zhang</h5><p>August 18, 2021</p></div><div class=title><h1>[DL & CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</h1></div><div class=post-content id=post-content><h2 id=abstract>Abstract</h2><p>For humans, language is a part of a system for understanding and communication about situations. There are some domain-general principles of biological neural network:</p><ul><li>connection-based learning</li><li>distributed representation</li><li>context-sensitive</li><li>mutual constraint satisfaction</li></ul><p>What they propose: the organization of the brain&rsquo;s distributed understanding system, which includes <strong>a fast learning system that addresses the memory problem</strong>.</p><h2 id=principles-of-neural-computation>Principles of Neural Computation</h2><p>A certain principles in PDP(parallel distributed processing) framework is the idea that cognition depends on mutual constraint satisfaction. We can regard it as a learning process which can construct context-sensitive representations. And <strong>BERT</strong> can do it really well, depend on QBA model.</p><blockquote><p>QBA model 上下文阴影词表征之间的相似性关系可以用来重建一个句子的句法结构描述。而这不需要structure build in.</p></blockquote><p>As far as I know, BERT works in parallel, using mutual QBA simultaneously on all of the words in an input text block. But this block has limit size, indicating that we can not use all the context information. Humans appear to exploit past context and a limited window of subsequent context,suggesting a hybrid strategy.</p><blockquote><p>推荐阅读：Z. Yang et al., “Xlnet: Generalized autoregressive pretraining for language understanding” in Advances in Neural Information Processing Systems, H. Wallach et al., Eds. (Curran Associates, Inc., Red Hook, NY, 2019), vol. 32, pp. 5753–5763.</p></blockquote><h2 id=language-in-an-integrated-understanding-system>Language in an Integrated Understanding System</h2><p>The author argues that part of the solution will come from treating language as <strong>part of a larger system for understanding and communicating</strong>, and the targets of understanding are <strong>situations</strong>.
<strong>Situations</strong> are collections of entities, their properties and relations, and patterns of change in them. What it means to understand a situation is to construct a representation of it that captures aspects of the participating objects, their properties, relationships and interactions, and resulting outcomes.</p><h2 id=model-of-understanding>Model of Understanding</h2><ul><li>Assumptiom: Only focus on concrete situations involving animate beings and physical objects.</li></ul><p><img src=/images/posts/paper/5.JPG alt></p><ul><li><strong>Visual Subsystem</strong>: subserves the formation of a visual representation of the given situation.</li><li><strong>Aduitory Subsystem:</strong> subserves the formation of an auditory representation capturing the spatiotemporal structure of the co-occurring sopken language.</li><li><strong>Object Subsystem</strong>: an intermodal area, receiving visual, language, and other information aobut objects. This area is the hidden layer of an interactive, recurrent network with bidirectional connections to other layers representing different type of object properties.</li><li><strong>Language Subsytem</strong>: The understanding of microsituations depends jointly on this system and context system.</li><li><strong>Context Subsystem</strong>：There is a network of areas in the brain that capture the spatiotemporal context.</li></ul><p>Within each subsystem, and between each connected pair of subsystems, the neurons are reciprocally interconnected via learning-dependent pathways, allowing <strong>mutual constraint satisfaction</strong> among all of the elements of each of the representation types.</p><h3 id=mtlintegrated-system-state><strong>MTL(Integrated System State)</strong></h3><p>随机的新信息在很短的时间内出现，当前的brain system很难去处理并在未来任意时间去从记忆中使用。这一点正是这个单元想去解决的问题。MTL的功能就是支持新的任意关联的形成，将一个experience的elements链接在一起，including the objects and language encountered in a situation and the co-occurring spatiotemporal context.</p><blockquote><p>GPT-3当遇到新的context时，之前this word 的 representation 信息会被覆盖掉，相当于没有记忆机制。作者在原文中如此写道：Further research should explore whether augmenting a model like GPT-3 with an MTL-like memory would enable more human-like extension of a novel word encountered just once to a wider range of contexts.</p></blockquote><h2 id=implementation-of-model>Implementation of Model</h2><ul><li><strong>Input of model</strong>: sequences of microsituation each consisting of <strong>a picture-description(language)(PD)pair</strong> grouped into scenes that in turn form story-length narratives, with the language conveyed by text rather than speech.</li></ul><h3 id=process>Process</h3><ol><li>Each PD pair is processed by interacting object and language subsystems, receiving visual and text input at the same time.</li><li>Each subsystem must learn to restore missing or distorted elements by using mutual QBA as in BERT.</li><li>The context subsystem encodes a sequence of compressed represenation of the <strong>previous PD pair</strong> within the current scene.</li><li>Within the processing of a pair, the context system would engage in mutual QBA with the object and language subsystems, allowing the language and object subsystems to indirectly exploit information from past pairs within the scene.(下属的subsystem可以得到历史信息，这样以便于他们地道道更好的representation，实现所谓的mutual constraint satisfaction)</li><li>一个具有学习过的连接权重的神经网络在处理完每个PD对之后，构建了对物体、语言和语境子系统的状态以及视觉和文本子系统的状态的可逆的还原描述。这个压缩向量被存储在MTL-like memory里。</li></ol><h2 id=enhancing-uderstanding-by-using-reinforcement-leaarning>Enhancing Uderstanding by Using Reinforcement leaarning</h2><p>和强化学习结合，从而在情境中去得到语言、图片信息，从而去做出相应的选择。并且这种选择经过一些训练可以学习为一种固定的模式，再得到类似情景的输入时就会去做相同的事情，有了认知的行为。</p><p>可以拓展阅读这几篇论文：</p><ul><li>K. M. Hermann et al., <strong>Grounded language learning in a simulated 3D world</strong>. arXiv:1706.06551 (20 June 2017).</li><li>R. Das, M. Zaheer, S. Reddy, A. McCallum, <strong>“Question answering on knowledge bases and text using universal schema and memory networks”</strong> in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistic, R. Barzilay, M.-Y. Kan, Eds. (Association for Computational Linguistics, Stroudsburg, PA, 2017), pp. 358–365.</li><li>D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, R. Salakhutdinov, <strong>“Gated-attention architectures for task-oriented language grounding”</strong> in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), S. A. McIlraith, K. Q. Weinberger, Eds. (AAAI Press, Cambridge, MA, 2018), pp. 2819–2826.</li><li>J. Oh, S. Singh, H. Lee, P. Kohli, <strong>“Zero-shot task generalization with multi-task deep reinforcement learning”</strong> in Proceedings of the 34th International Conference on Machine Learning-Volume 70, D. Precup, Y. W. Teh, Eds. (JMLR.org, 2017), pp. 2661–2670.</li><li>F. Hill et al., <strong>“Environmental drivers of systematicity and generalization in a situated agent”</strong> in International Conference on Learning Representations (ICLR, 2020).</li></ul></div><div class=btn-improve-page><a href=https://github.com/ztqakita/ztqakita.github.io/edit//content/posts/paper-reading/integrated_understanding_system.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/posts/life-learning/lecture_james_mcclleland/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Lecture: Are People Still Smarter than Machines?</span></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#principles-of-neural-computation>Principles of Neural Computation</a></li><li><a href=#language-in-an-integrated-understanding-system>Language in an Integrated Understanding System</a></li><li><a href=#model-of-understanding>Model of Understanding</a><ul><li><a href=#mtlintegrated-system-state><strong>MTL(Integrated System State)</strong></a></li></ul></li><li><a href=#implementation-of-model>Implementation of Model</a><ul><li><a href=#process>Process</a></li></ul></li><li><a href=#enhancing-uderstanding-by-using-reinforcement-leaarning>Enhancing Uderstanding by Using Reinforcement leaarning</a></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>
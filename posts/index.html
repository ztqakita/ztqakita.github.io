<!doctype html><html><head><title>Posts</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/deep-learning/>DL</a><ul><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><a href=/posts/life-learning/>Life Learning</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/posts/neural-computation/basic-neuro-knowledge/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Neuroscience Basic Knowledge</h5><p class="card-text post-summary">I. The Mammalian Visual System 1. Structure</p></div><div class=card-footer><span class=float-left>August 9, 2021</span>
<a href=/posts/neural-computation/basic-neuro-knowledge/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/lecture_yao_xin/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>每日学习———类脑智能的另一种思路</h5><p class="card-text post-summary">写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。
姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。</p></div><div class=card-footer><span class=float-left>August 9, 2021</span>
<a href=/posts/life-learning/lecture_yao_xin/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/reservoir_decision_making/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[BIC] A brain-inspired computational model for spatio-temporal information processing</h5><p class="card-text post-summary">Abstract Current method: Explicit feature extraction, which requires lots of labeled data. Novel brain-inspired computational model: Reservoir Decision-making Network (RDMN) A reservoir model: projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics. (regarded it as SVM) A decision-making model: reads out neural representations via integrating information over time. Tasks: Looming pattern discrimination Gait recognition Event-based gait recognition The model Overview Summary: A spatio-temporal pattern is first processed by the reservoir module and then read out by the decision-making module.</p></div><div class=card-footer><span class=float-left>July 20, 2021</span>
<a href=/posts/paper-reading/reservoir_decision_making/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/push-pull_feedback/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[BIC] Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently</h5><p class="card-text post-summary">Abstract In addition to feedforward connections, there exist abundant feedback connections in a neural pathway. This paper investigate the role of feedback in hierarchical information retrieval. a hierarchical network storing the hierarchical categorical information of objects: Mechanism: information retrieval goes from rough to fine, aided by dynamical push-pull feedback from higher to lower layers. Function: 我们阐明，最佳反馈应该是动态的，随着时间的推移从正（推）到负（拉）而变化，它们分别抑制了来自不同类别和相同类别的模式关联所带来的干扰。 A model for Hierarchical Information Representation</p></div><div class=card-footer><span class=float-left>July 19, 2021</span>
<a href=/posts/paper-reading/push-pull_feedback/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/deep-learning/self-attention/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Self-attention</h5><p class="card-text post-summary">I. Self-attention Overview Input: A sequence of vectors (size not fixed) Output: Each vector has a label (POS tagging) The whole sequence has a label (sentiment analysis) Model decides the number of labels itself (seq2seq) Self-attention can handle global iformation, and FC can handle local information. Self-attention is the key module of Transformer, which will be shared in other articles.
II. How to work? Firstly, we should figure out the relevance between each vector.</p></div><div class=card-footer><span class=float-left>June 11, 2021</span>
<a href=/posts/deep-learning/self-attention/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/deep-learning/transformer/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Transformer</h5><p class="card-text post-summary">I. Transformer Overview Seq2seq model
Input: sequence of vector
Output: sequence (not fixed size)
II. Encoder It is actually a Self-Attention Model!
对于一个block，它的结构可以理解为以下的形式：
与self-attention不同的是，会采用residual add的方式，将self-attention得到的中间结果$a$加上输出$b$ 经过layer normalization得到新的输出 将新的output输入到FC中，并加上residual 再次经过layer normalization得到这一个block的结果。 理解了一个block以后，整个Encoder就是由n个这种block组成的network。
首先将word进行self-attention得到word embeddding，并在其中加入positional encoding信息 经过multi-head Attention或者Feed Forward Network后，都要接上residual + layer normalization，而这种设计结构就是Transformer Encoder的创新点所在。 III. Decoder —— Autoregressive(AT) The model structure of Decoder is shown as follow:
根据上图我们将逐步逐层地解释结构：
Masked Multi-head Self-attention 在产生每一个$b^i$ vector时，不能再看后面的信息，只能和前面的输入进行关系： 从细节上来看，要想得出$b^2$，我们只需将其和$k^1, k^2$做dot-product。</p></div><div class=card-footer><span class=float-left>June 11, 2021</span>
<a href=/posts/deep-learning/transformer/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/knn/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>KNN</h5><p class="card-text post-summary">原理 K近邻算法简单、直观，大致步骤为：
输入：训练集 $$T = {(x_1,y_1), (x_2, y_2), \dots, (x_N, y_N)}$$ 输出： 根据给定的距离度量，在训练集T中找出与$x$最近邻的$k$个点，涵盖这$k$个点的邻域记作$N_K(x)$； 找出最近邻的$k$个点的一个方法是搜索$kd$树； 在$N_k(x)$中根据分类决策规则（多数表决）决定$x$的类别$y$: $$y = arg \max_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j)$$ kd树</p></div><div class=card-footer><span class=float-left>April 21, 2021</span>
<a href=/posts/machine-learning/knn/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/neural-computation/1-ionic_currents/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Ionic Currents</h5><p class="card-text post-summary">I. A mathematical model of a neuron Equivalent circuit model
Parts of the circuit:
Power supplies: multiple to power different parts of the circuit to do different things Integartor(积分器) of past inputs Temporal filter to smooth inputs in time Spike generator: the sodium current(钠通路) and potassium current(钾通路) make a spike generator that generates an action potential and then talks to another neurons. Oscillator(振荡器) The wires of brain: The intracellular and extracelluar sapce is filled with salt solution.</p></div><div class=card-footer><span class=float-left>April 16, 2021</span>
<a href=/posts/neural-computation/1-ionic_currents/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/support-vector/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Support Vector Machines</h5><p class="card-text post-summary">支持向量机模型 学习策略：间隔最大化，可形式化为一个求解凸二次规划(convex quadratic programming)的最优化问题 一、线性可分支持向量机与硬间隔最大化 支持向量机的学习是在特征空间上进行的，需要从输入空间转换到特征空间上。
线性可分支持向量机：给定线性可分的训练集，通过间隔最大化或等价地求解相应的凸二次规划问题得到的分离超平面为： $$\omega^* x + b^* = 0$$ 以及相应的分类决策函数： $$f(x) = sign(\omega^* x + b^*)$$
函数间隔和几何间隔
函数间隔：对于给定的训练数据集$T$和超平面$(\omega, b)$，则 $$\hat{\gamma_i}=y_i(\omega \cdot x_i+b)$$
超平面关于数据集$T$的函数间隔为 $$\hat{\gamma}=\min_{i=1,\dots,N}\hat{\gamma_i}$$ 几何间隔：对于给定的训练数据集$T$和超平面$(\omega, b)$，则 $$\hat{\gamma_i}=y_i(\frac{\omega}{||\omega||} \cdot x_i+\frac{b}{||\omega||})$$
其中$||\omega||$是L2范数。 间隔最大化
输入：线性可分数据集$T$ 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： $$\min_{\omega,b} \quad \frac{1}{2}||\omega||^2 $$ $$ s.t. \qquad y_i(\omega \cdot x_i+b)-1 \geq 0, \quad i=1, 2, \dots, N$$ 求解得到最优解$\omega^* , b^*$。 支持向量</p></div><div class=card-footer><span class=float-left>March 29, 2021</span>
<a href=/posts/machine-learning/support-vector/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/decision-tree/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Decision Tree</h5><p class="card-text post-summary">原理 决策树模型与学习 决策树的内部节点表示一个特征或属性，叶节点表示一个类。
if-then规则：互斥且完备 本质： 从训练数据集中归纳出一组分类规则（估计出条件概率模型） 损失函数：正则化的极大似然函数 学习算法：启发式方法，得到(sub-optimal)的决策树 一、特征选择 准则：信息增益
熵： 设$X$是一个取有限个值的离散随机变量，定义为： $$H(X) = -\sum_{i=1}^n p_i\log p_i$$
条件熵： 随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为： $$H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)$$ 当熵和条件熵中的概率由极大似然估计得到时，分别成为经验熵和经验条件熵
信息增益：表示得知特征$X$的信息使得类$Y$的信息不确定性减少的程度。这种差值也称为互信息 $$g(D,A)=H(D)-H(D|A)$$
特征选择方法：对训练数据集D，计算每一个特征的信息增益，选取信息增益最大的特征。
信息增益比 上述的特征选择存在偏向选择取值较多的特征的问题，使用information gain ratio可以解决这个问题。 定义为： $$g_R(D, A) = \frac{g(D,A)}{H_A(D)}$$ 其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{D}\log_2\frac{|D_i|}{D} $，n是特征A的取值个数。 二、决策树的生成 ID3算法 C4.5算法的基础般，只用信息增益来选取特征。 C4.5算法 三、决策树的剪枝 决策树的生成很容易出现过拟合现象，所以需要利用剪枝(pruning)来简化决策树。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现： $$C_{\alpha}(T)=\sum_{i=1}^{|T|}N_tH_t(T) + \alpha|T|=C(T)+\alpha|T|$$ 其中$|T|$表示树T的叶结点个数，t是树T的叶结点，该叶结点上有$N_t$个样本点，$\alpha \geq 0$为参数。
通过式子可以看出$C(T)$代表了模型对训练数据的预测误差，即拟合程度；而$\alpha|T|$代表了模型的复杂度，可以理解为正则化的方式来增强模型的泛化能力。
而树的剪枝算法分为：
预剪枝：不足是基于贪心策略，带来欠拟合的风险 后剪枝 后剪枝与动态规划类似，生成一棵完整的决策树以后，自底向上地对非叶节点进行考察，若剪完后损失函数变小，则进行剪枝。</p></div><div class=card-footer><span class=float-left>March 22, 2021</span>
<a href=/posts/machine-learning/decision-tree/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/basic-concepts/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Basic Concepts</h5><p class="card-text post-summary">统计学习概念辨析 一、基本分类 1. 监督学习 监督学习的本质是学习输入到输出的映射的统计规律。需要注意的有以下要点：
输入空间与特征空间不一定为相同的空间，有时会将实例从输入空间映射到特征空间 训练数据由输入(特征向量)与输出对组成 任务问题分类： 回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题 $X$和$Y$具有联合概率分布就是监督学习关于数据的基本假设，即假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的 假设空间的确定意味着学习范围的确定 2. 无监督学习 无监督学习的本质是学习数据中的统计规律或潜在结构，需要注意的有以下要点：
可以用于对已有数据的分析，也可以用于对未来数据的预测 要学习的模型可以表示为$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式 3. 强化学习 强化学习的本质是学习最优的序贯决策。在学习过程中，系统不断地试错，以达到学习最优策略的目的。
强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$&lt;S,A,P,r,\gamma>$组成：
$S$是state集合 $A$是action集合 $P$是状态转移概率(transition probability)函数： $$P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$$ $r$是奖励函数(reward function): $$r(s,a)=E(r_{t+1}|s_t=s, a_t=a)$$ $\gamma$是衰减系数(discount factor): $$\gamma \in [0,1]$$ 马尔可夫决策过程具有马尔科夫性，下一个状态只依赖于前一个状态与动作，由状态转移概率函数$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，由奖励函数$r(s,a)$表示。
策略$\pi$：给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$ 价值函数/状态价值函数：策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望： $$v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s]$$ 动作价值函数：策略$\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望： $$q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s, a_t=a]$$ 强化学习的目标就是在所有可能的策略中选出价值函数最大的策略$\pi^*$。
强化学习的分类：
policy-based 不直接学习模型，试图求解最优策略$\pi^*$。学习通常从一个具体策略开始，通过搜索更优的策略进行。 value-based 试图求解最有价值函数($q^*(s,a)$)。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。 model-based 直接学习马尔科夫决策过程的模型，通过模型对环境的反馈进行预测。 4.</p></div><div class=card-footer><span class=float-left>January 27, 2021</span>
<a href=/posts/machine-learning/basic-concepts/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/perceptron/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Perceptron</h5><p class="card-text post-summary">原理 参考：统计学习方法|感知机原理剖析及实现
输入：实例的特征向量 输出：实例的类别（二分类） 模型类别：判别模型 学习策略：基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型</p></div><div class=card-footer><span class=float-left>January 27, 2021</span>
<a href=/posts/machine-learning/perceptron/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/posts/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/posts/>1</a></li><li class=page-item><a class=page-link href=/posts/page/2/>2</a></li><li class=page-item><a class=page-link href=/posts/page/3/>3</a></li><li class=page-item><a href=/posts/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/posts/page/3/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=/js/list.js></script></body></html>
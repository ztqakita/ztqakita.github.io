---
title: "Basic Concepts"
date: 2021-01-27T06:00:20+06:00
hero: /images/posts/DL/139-deep-learning.svg
menu:
  sidebar:
    name: Basic Concepts
    identifier: Basic-Concepts
    parent: machine-learning
    weight: 10
math: true
---

# 统计学习概念辨析
## 一、基本分类
### 1. 监督学习
监督学习的本质是学习输入到输出的映射的统计规律。需要注意的有以下要点：
- **输入空间**与**特征空间**不一定为相同的空间，有时会将实例从输入空间映射到特征空间
- 训练数据由输入(特征向量)与输出对组成
- 任务问题分类：
  - 回归问题：输入变量与输出变量均为**连续变量**的预测问题
  - 分类问题：输出变量为**有限个离散变量**的预测问题
  - 标注问题：输入变量与输出变量均为**变量序列**的预测问题
- $X$和$Y$具有联合概率分布就是监督学习关于数据的基本假设，即假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的
- **假设空间**的确定意味着学习范围的确定

### 2. 无监督学习
无监督学习的本质是学习数据中的统计规律或潜在结构，需要注意的有以下要点：
- 可以用于对已有数据的分析，也可以用于对未来数据的预测
- 要学习的模型可以表示为$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式

### 3. 强化学习
强化学习的本质是学习最优的序贯决策。在学习过程中，系统不断地试错，以达到学习最优策略的目的。

强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$<S,A,P,r,\gamma>$组成：
- $S$是state集合
- $A$是action集合
- $P$是状态转移概率(transition probability)函数：
  $$P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$$
- $r$是奖励函数(reward function):
  $$r(s,a)=E(r_{t+1}|s_t=s, a_t=a)$$
- $\gamma$是衰减系数(discount factor):
  $$\gamma \in [0,1]$$

马尔可夫决策过程具有**马尔科夫性**，下一个状态只依赖于前一个状态与动作，由状态转移概率函数$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，由奖励函数$r(s,a)$表示。

- 策略$\pi$：给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$
- 价值函数/状态价值函数：策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望：
  $$v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s]$$
- 动作价值函数：策略$\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望：
  $$q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s, a_t=a]$$

强化学习的目标就是在所有可能的策略中选出价值函数最大的策略$\pi^*$。

强化学习的分类：
- **policy-based**
  - 不直接学习模型，试图求解最优策略$\pi^*$。学习通常从一个具体策略开始，通过搜索更优的策略进行。
- **value-based**
  - 试图求解最有价值函数($q^*(s,a)$)。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。
- **model-based**
  - 直接学习马尔科夫决策过程的模型，通过模型对环境的反馈进行预测。

### 4. 半监督学习
利用标注数据和未标注数据学习预测模型的机器学习问题。这种学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习。

### 5. 主动学习
目标是找出对学习最有帮助的实例让教师标注，以较小的标注代价，达到更好的学习效果。

### 6. 按模型分类
**概率模型**：生成模型
- 决策树
- 朴素贝叶斯
- 隐马尔可夫模型
- 条件随机场
- 概率潜在语义分析
- 潜在狄利克雷分配
- 高斯混合模型
- *逻辑斯蒂回归*

**非概率模型**：判别模型
- 感知机
- 支持向量机
- k近邻
- AdaBoost
- k均值
- 潜在语义分析
- 神经网络
- *逻辑斯蒂回归*

**线性模型**：
- 感知机
- 线性支持向量机
- k近邻
- k均值
- 潜在语义分析

**非线性模型**：
- 核函数支持向量机
- AdaBoost
- 神经网络
  
> 条件概率分布最大化后得到函数，函数归一化得到条件概率分布。
### 7. 按学习分类
- 在线学习：每一次接受一个样本，进行预测，然后学习模型，并不断重复该操作
- 批量学习：一次接受所有数据，学习模型，之后进行预测

## 二、模型策略与选择
对于监督学习而言：
1. 经济风险最小化(ERM)
   会出现过拟合的情况
2. 结构风险最小化(SRM)
   等价于正则化

### 1. 过拟合
定义：学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测很好，但对未知数据预测很差的现象。

### 2. 正则化
1. $L_1$范数
   $$L(w)=\frac{1}{N}\sum_{i=1}^N(f(x_i;w)-y_i)^2+\lambda||w||$$
2. $L_2$范数
   $$L(w)=\frac{1}{N}\sum_{i=1}^N(f(x_i;w)-y_i)^2+\frac{\lambda}{2}||w||^2$$

### 3. 交叉验证
- 训练集：训练模型
- 验证集：选择模型
- 测试集：评估模型

交叉验证的基本思想：重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。
- 简单交叉验证
  随机分割数据集，并选出测试误差最小的模型
- **S折交叉验证(S-fold cross validation)**
  随机将已给数据切分为S个互不相交、大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型。
- 留一交叉验证
  上述方法的特殊情况，S=N，这里N代表数据集的容量，不常用的方法。

### 4. 泛化能力
1. 泛化误差的定义：所学习的模型的期望风险
2. **泛化误差上界**

## 三、生成模型与判别模型
1. 生成模型
   由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型。
   特点：
   - 可以还原出联合概率分布
   - 学习收敛速度更快
   - 当存在隐变量时，仍可以使用生产学习

2. 判别模型
   由数据直接学习决策函数或者条件概率分布作为预测的模型。
   特点：
   - 学习的准确率更高
   - 可以对数据进行各种程度上的抽象、定义特征并使用特征，所以可以简化学习。

## 四、模型评价指标
监督学习中的分类问题与标注问题：
- 准确率(accuracy)：分类器正确分类的样本数与总样本数之比
- 精确率(percision)：针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是
  $$P=\frac{TP}{TP+FP}$$
- 召回率(recall)：针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)，也就是
  $$R=\frac{TP}{TP+FN}$$
- F1：精确率和召回率的调和均值，也就是
  $$\frac{2}{F1}=\frac{1}{P}+\frac{1}{R}$$
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML on ztqakita's Blog</title><link>https://ztqakita.github.io/posts/machine-learning/</link><description>Recent content in ML on ztqakita's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 29 Mar 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://ztqakita.github.io/posts/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Decision Tree</title><link>https://ztqakita.github.io/posts/machine-learning/support-vector/</link><pubDate>Mon, 29 Mar 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/machine-learning/support-vector/</guid><description>支持向量机模型 学习策略：间隔最大化，可形式化为一个求解凸二次规划(convex quadratic programming)的最优化问题 一、线性可分支持向量机与硬间隔最大化 支持向量机的学习是在特征空间上进行的，需要从输入空间转换到特征空间上。
线性可分支持向量机：给定线性可分的训练集，通过间隔最大化或等价地求解相应的凸二次规划问题得到的分离超平面为： $$\omega^* x + b^* = 0$$ 以及相应的分类决策函数： $$f(x) = sign(\omega^* x + b^*)$$
函数间隔和几何间隔
函数间隔：对于给定的训练数据集$T$和超平面$(\omega, b)$，则 $$\hat{\gamma_i}=y_i(\omega \cdot x_i+b)$$
超平面关于数据集$T$的函数间隔为 $$\hat{\gamma}=\min_{i=1,\dots,N}\hat{\gamma_i}$$ 几何间隔：对于给定的训练数据集$T$和超平面$(\omega, b)$，则 $$\hat{\gamma_i}=y_i(\frac{\omega}{||\omega||} \cdot x_i+\frac{b}{||\omega||})$$
其中$||\omega||$是L2范数。 间隔最大化
输入：线性可分数据集$T$ 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： $$\min_{\omega,b} \quad \frac{1}{2}||\omega||^2 \
s.t. \qquad y_i(\omega \cdot x_i+b)-1 \geq 0, \quad i=1, 2, \dots, N$$ 求解得到最优解$\omega^*, b^*$。
支持向量 训练集的样本点中与分离超平面距离最近的样本点的实例： $$H_1:\omega \cdot x_i+b=1 \ H_2:\omega \cdot x_i+b=-1$$ 支持向量机由这些很少的“重要”训练样本决定。</description></item><item><title>Decision Tree</title><link>https://ztqakita.github.io/posts/machine-learning/decision-tree/</link><pubDate>Mon, 22 Mar 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/machine-learning/decision-tree/</guid><description>原理 决策树模型与学习 决策树的内部节点表示一个特征或属性，叶节点表示一个类。
if-then规则：互斥且完备 本质： 从训练数据集中归纳出一组分类规则（估计出条件概率模型） 损失函数：正则化的极大似然函数 学习算法：启发式方法，得到(sub-optimal)的决策树 一、特征选择 准则：信息增益
熵： 设$X$是一个取有限个值的离散随机变量，定义为： $$H(X) = -\sum_{i=1}^n p_i\log p_i$$
条件熵： 随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为： $$H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)$$ 当熵和条件熵中的概率由极大似然估计得到时，分别成为经验熵和经验条件熵
信息增益：表示得知特征$X$的信息使得类$Y$的信息不确定性减少的程度。这种差值也称为互信息 $$g(D,A)=H(D)-H(D|A)$$
特征选择方法：对训练数据集D，计算每一个特征的信息增益，选取信息增益最大的特征。
信息增益比 上述的特征选择存在偏向选择取值较多的特征的问题，使用information gain ratio可以解决这个问题。 定义为： $$g_R(D, A) = \frac{g(D,A)}{H_A(D)}$$ 其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{D}\log_2\frac{|D_i|}{D} $，n是特征A的取值个数。 二、决策树的生成 ID3算法 C4.5算法的基础般，只用信息增益来选取特征。 C4.5算法 三、决策树的剪枝 决策树的生成很容易出现过拟合现象，所以需要利用剪枝(pruning)来简化决策树。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现： $$C_{\alpha}(T)=\sum_{i=1}^{|T|}N_tH_t(T) + \alpha|T|=C(T)+\alpha|T|$$ 其中$|T|$表示树T的叶结点个数，t是树T的叶结点，该叶结点上有$N_t$个样本点，$\alpha \geq 0$为参数。
通过式子可以看出$C(T)$代表了模型对训练数据的预测误差，即拟合程度；而$\alpha|T|$代表了模型的复杂度，可以理解为正则化的方式来增强模型的泛化能力。
而树的剪枝算法分为：
预剪枝：不足是基于贪心策略，带来欠拟合的风险 后剪枝 后剪枝与动态规划类似，生成一棵完整的决策树以后，自底向上地对非叶节点进行考察，若剪完后损失函数变小，则进行剪枝。</description></item><item><title>Basic Concepts</title><link>https://ztqakita.github.io/posts/machine-learning/basic-concepts/</link><pubDate>Wed, 27 Jan 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/machine-learning/basic-concepts/</guid><description>统计学习概念辨析 一、基本分类 1. 监督学习 监督学习的本质是学习输入到输出的映射的统计规律。需要注意的有以下要点：
输入空间与特征空间不一定为相同的空间，有时会将实例从输入空间映射到特征空间 训练数据由输入(特征向量)与输出对组成 任务问题分类： 回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题 $X$和$Y$具有联合概率分布就是监督学习关于数据的基本假设，即假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的 假设空间的确定意味着学习范围的确定 2. 无监督学习 无监督学习的本质是学习数据中的统计规律或潜在结构，需要注意的有以下要点：
可以用于对已有数据的分析，也可以用于对未来数据的预测 要学习的模型可以表示为$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式 3. 强化学习 强化学习的本质是学习最优的序贯决策。在学习过程中，系统不断地试错，以达到学习最优策略的目的。
强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$&amp;lt;S,A,P,r,\gamma&amp;gt;$组成：
$S$是state集合 $A$是action集合 $P$是状态转移概率(transition probability)函数： $$P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$$ $r$是奖励函数(reward function): $$r(s,a)=E(r_{t+1}|s_t=s, a_t=a)$$ $\gamma$是衰减系数(discount factor): $$\gamma \in [0,1]$$ 马尔可夫决策过程具有马尔科夫性，下一个状态只依赖于前一个状态与动作，由状态转移概率函数$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，由奖励函数$r(s,a)$表示。
策略$\pi$：给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$ 价值函数/状态价值函数：策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望： $$v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s]$$ 动作价值函数：策略$\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望： $$q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s, a_t=a]$$ 强化学习的目标就是在所有可能的策略中选出价值函数最大的策略$\pi^*$。
强化学习的分类：
policy-based 不直接学习模型，试图求解最优策略$\pi^*$。学习通常从一个具体策略开始，通过搜索更优的策略进行。 value-based 试图求解最有价值函数($q^*(s,a)$)。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。 model-based 直接学习马尔科夫决策过程的模型，通过模型对环境的反馈进行预测。 4.</description></item><item><title>Perceptron</title><link>https://ztqakita.github.io/posts/machine-learning/perceptron/</link><pubDate>Wed, 27 Jan 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/machine-learning/perceptron/</guid><description>原理 参考：统计学习方法|感知机原理剖析及实现
输入：实例的特征向量 输出：实例的类别（二分类） 模型类别：判别模型 学习策略：基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型</description></item><item><title>Classification</title><link>https://ztqakita.github.io/posts/machine-learning/classification/</link><pubDate>Tue, 28 Jul 2020 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/machine-learning/classification/</guid><description>Classification I. Probabilistic Generative Models 1. Detailed Process The basic idea is estimating the probabilities form training data. Let&amp;rsquo;s consider the two classes case: First of all, we need to figure out prior class probabilities $P(C_k)$. It&amp;rsquo;s pretty easy to find that $P(C_k) = \frac{SizeOf C_k}{SizeOf Training Data}$ Then the task is to find out $P(x|C_k)$. Each data is represented as a vector by its attribute, and it exist as a point in a multidimensional space.</description></item></channel></rss>
<!doctype html><html><head><title>Introduction to Deep Learning</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><meta name=description content="Introduction to Deep Learning"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/neural-computation/basic-neuro-knowledge/>Neuroscience Basic Knowledge</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/deep-learning/>DL</a><ul class=active><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a class=active href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/life-learning/>Life Learning</a><ul><li><a href=/posts/life-learning/lecture_james_mcclleland/>Lecture James McClleland</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/integrated_understanding_system/>Integrated understanding system</a></li><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li><li><a href=/posts/paper-reading/task_representation_cognitive_tasks/>Task representations in neural networks</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ztqakita.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ztq.png><h5 class=author-name>Brandon Zhang</h5><p>August 10, 2020</p></div><div class=title><h1>Introduction to Deep Learning</h1></div><div class=post-content id=post-content><h2 id=i-basic-concepts>I. Basic Concepts</h2><h3 id=1-fully-connected-feedforward-network>1. Fully Connected Feedforward Network</h3><p><img src=/images/posts/DL/FCFN.JPG alt></p><h3 id=2-matrix-operation>2. Matrix Operation</h3><p>Every layer has weight matrix and bias matrix, using matrix operation we can accumulate the output matrix $y$.
<img src=/images/posts/DL/matrix.JPG alt></p><blockquote><p>Tips: Using GPU could speed up matrix operation.</p></blockquote><h2 id=ii-why-deep-learning>II. Why Deep Learning?</h2><h3 id=1-modularization>1. Modularization</h3><p>对neural network而言，并不是神经元越多越好，通过例子可以看出层数的增加（more deep）对于准确率的提升更有效果。这其中就是 Modularization 的思想。
For example, while you are trying to train the model below, you can use basic classifiers as module. Each basic classifier can have sufficient training examples. Therefore those classifiers can use output from the basic classifiers, which can be trained by little data.
<img src=/images/posts/DL/modular.JPG alt>
What you should notice is that the modularization is automatically learned form data.</p><h2 id=iii-backpropagation>III. Backpropagation</h2><h3 id=1-concept>1. Concept</h3><p>An Efficient way to compute gradient descent $\partial{L}/\partial{\omega}$ in neural network.</p><h3 id=2-detailed-process>2. Detailed Process</h3><p>The partial of the loss function can be presented as follow:
<img src=/images/posts/DL/partial.JPG alt>
According to Chain rule, we need to compute two partials: <em><strong>Forward pass</strong></em> and <em><strong>Backward pass</strong></em>.
<img src=/images/posts/DL/pass.JPG alt></p><ul><li><p><em><strong>Forward pass</strong></em>
$\partial{z}/\partial{\omega} = $ The value of the output connected by the <em>weight</em>.
<img src=/images/posts/DL/forward.JPG alt></p></li><li><p><em><strong>Backward pass</strong></em>
To compute $\partial{C}/\partial{z}$, we need to figure out following neurons' partial.
<img src=/images/posts/DL/back1.JPG alt></p><ul><li><p>case 1: Output Layer
When we arrive at the output layer of the network, these two partials: $\partial{C}/\partial{z'} = \partial{C}/\partial{y_1}$ and $\partial{C}/\partial{z''} = \partial{C}/\partial{y_2}$ could be solved pretty easy.
<img src=/images/posts/DL/case1.JPG alt></p></li><li><p>case 2: Not Output Layer
We could compute $\partial{C}/\partial{z}$ recursively, until we reach the output layer. Then we start to backward pass. It seems like a new backward neural network (red line in the graph).
<img src=/images/posts/DL/case2.JPG alt></p></li></ul></li></ul><p>The whole process of backpropogation is shown as following graph:
<img src=/images/posts/DL/back2.JPG alt></p><h2 id=iv-tips-for-dl>IV. Tips for DL</h2><p><img src=/images/posts/DL/tips.JPG alt>
<img src=/images/posts/DL/tips2.JPG alt></p><h3 id=1-new-activation-function>1. New activation function</h3><ul><li><strong>Vanishing Gradient Descent</strong>
This phenomenon is mainly caused by the particular activation function: <strong>sigmoid funcion</strong>.
<img src=/images/posts/DL/vgd.JPG alt>
From the analysis above when given a large $\Delta{\omega}$ as an input, the output from sigmoid function is become smaller. After going through many layers, the gradient goes to a small value, also called vanished.</li></ul><h4 id=1-relu>(1) ReLU</h4><p><img src=/images/posts/DL/relu.JPG alt>
<img src=/images/posts/DL/relu2.JPG alt></p><h4 id=2-maxout>(2) Maxout</h4><blockquote><p>ReLU is a special case of Maxout</p></blockquote><p>For Maxout function, we can design the form of activation funcion by our own.
<img src=/images/posts/DL/maxout.JPG alt></p><ul><li>Activation function in maxout network can be any piecewise linear convex function</li><li>How many pieces depending on how many
elements in a group</li></ul><p><em><strong>Question 1: How to train Maxout network?</strong></em></p><p><em><strong>Answer: 对于每一个神经元，他们先比大小取出较大$z_i$作为输入，再将其对应的linear function拿去作微分</strong></em></p><p><em><strong>Quesion 2: 有些output被舍去之后，back propogation无法再对其进行训练？</strong></em></p><p><em><strong>Answer: We can train different thin and linear network for different example.</strong></em></p><h3 id=2-adaptive-learning-rate>2. Adaptive Learning Rate</h3><blockquote><p>tips: 实际上对于local minima, 并不会存在过多，local minima的出现需要每一个dimension在附近都会产生local minima。假设一个dimension在这里出现local minima的概率为p，由于神经网路需要非常多的parameters，local minima出现的概率很低。</p></blockquote><h3 id=3-early-stopping>3. Early Stopping</h3><p>Using <em><strong>Validation Set</strong></em> to simulate testing set, then we could find where to stop properly.
<img src=/images/posts/DL/early.JPG alt></p><h3 id=4-regularization>4. Regularization</h3><p>New loss function to be minimized:</p><ul><li>Find a set of weight not only minimizing original cost but also close to zero.
<img src=/images/posts/DL/regularization.JPG alt></li><li>L2 regularization (二范数)</li></ul><blockquote><p>Weight Decay (权重衰减)
对于一些长期不需要用到的weight，它的值会随着训练次数接近0，这种模式接近于人脑神经元的构造方式。</p></blockquote><p><img src=/images/posts/DL/reg2.JPG alt></p><ul><li>L1 regularization
<img src=/images/posts/DL/reg3.JPG alt>
L1 每次减掉固定的值，使得每次迭代后保留很多较大的值，得到的结果会比较稀疏，有很多接近于0，也有很多较大的值；然而对于L2，平均值较小。</li></ul><h3 id=5-dropout>5. Dropout</h3><p><img src=/images/posts/DL/dp1.JPG alt>
<img src=/images/posts/DL/dp2.JPG alt>
<em><strong>The amazing point of dropout:</strong></em>
<img src=/images/posts/DL/dp3.JPG alt></p><blockquote><p>当你将测试集送入每一个minibatch训练的neural network，再将所有output求average，结果跟将测试集送入完整(未dropout，将所有weight乘以1-p%)的neural network得出来的结果是近似的。这点在线性模型中很容易解释，但对于非线性模型依旧有这个特性。所以Relu+dropout和Maxout+dropout的效果都会不错。</p></blockquote></div><div class=btn-improve-page><a href=https://github.com/ztqakita/ztqakita.github.io/edit//content/posts/deep-learning/Introduction-to-Deep-Learning.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/deep-learning/cnn/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>Convolutional Neural Network</span></a></div><div class="col-md-6 next-article"><a href=/posts/machine-learning/classification/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Classification</span></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#i-basic-concepts>I. Basic Concepts</a><ul><li><a href=#1-fully-connected-feedforward-network>1. Fully Connected Feedforward Network</a></li><li><a href=#2-matrix-operation>2. Matrix Operation</a></li></ul></li><li><a href=#ii-why-deep-learning>II. Why Deep Learning?</a><ul><li><a href=#1-modularization>1. Modularization</a></li></ul></li><li><a href=#iii-backpropagation>III. Backpropagation</a><ul><li><a href=#1-concept>1. Concept</a></li><li><a href=#2-detailed-process>2. Detailed Process</a></li></ul></li><li><a href=#iv-tips-for-dl>IV. Tips for DL</a><ul><li><a href=#1-new-activation-function>1. New activation function</a><ul><li><a href=#1-relu>(1) ReLU</a></li><li><a href=#2-maxout>(2) Maxout</a></li></ul></li><li><a href=#2-adaptive-learning-rate>2. Adaptive Learning Rate</a></li><li><a href=#3-early-stopping>3. Early Stopping</a></li><li><a href=#4-regularization>4. Regularization</a></li><li><a href=#5-dropout>5. Dropout</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>
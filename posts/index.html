<!doctype html><html><head><title>Posts</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/neural-computation/basic-neuro-knowledge/>Neuroscience Basic Knowledge</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/deep-learning/>DL</a><ul><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/life-learning/>Life Learning</a><ul><li><a href=/posts/life-learning/how_to_model/>how to model</a></li><li><a href=/posts/life-learning/lecture_james_mcclleland/>Lecture James McClleland</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/continuous_attractor_nn/>Continuous-attractor Neural Network</a></li><li><a href=/posts/paper-reading/integrated_understanding_system/>Integrated understanding system</a></li><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li><li><a href=/posts/paper-reading/task_representation_cognitive_tasks/>Task representations in neural networks</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/posts/paper-reading/continuous_attractor_nn/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[CN] Continuous-attractor Neural Network</h5><p class="card-text post-summary">Si Wu, Kosuke Hamaguchi, and Shun-ichi Amari. “Dynamics and computation of continuous attractors.” Neural computation 20.4 (2008): 994-1025.
本篇参考吴思老师公众号的文章《【学术思想】连续吸引子神经网络：神经信息表达的正则化网络模型》，并根据冷泉港亚洲暑期学校的讲座和智源学术沙龙的报告来详细阐释CANN的工作原理，全文用中文行文。
连续吸引子神经网络的数学模型 吸引子指的是一个动力学系统在不接受外界输入情况下靠自身动力学就能维持的非静息的稳定状态（active stationary state）。要构成一个的吸引子网络，需要两个基本条件：
神经元之间有兴奋性的互馈连接 (recurrent connection)，这样在没有外界输入的情况下，靠神经元之间的正反馈，网络就能维持住稳定活动；同时我们也要求兴奋性连接是局部的，这样才能形成有意义的空间局部活动； 网络中要有抑制性作用，这样才能避免系统活动因反复的正反馈而爆炸。 Hopfield模型采用吸引子的思想解释了大脑在收到部分或模糊信息条件下的联想式记忆机制。但经典的Hopfield模型没有考虑神经元之间连接的对称结构，因而其具有的吸引子（多个）在空间上是相互孤立的。
在吸引子网络的基础上，如果我们进一步要求神经元之间的连接具有空间平移不变性的对称结构，这时网络就将具有一簇连续的、而不是孤立的吸引子（注意，考虑到真实生物系统的神经数目足够大，为了方便，后面讨论都假设神经元数目是无穷的）；这些吸引子在参数空间上紧密排列，构成一个连续的状态子空间。
一维CANN数学模型 神经元的突触总输入$u$的动力学方程如下： $$ \tau \, \frac{du(x, t)}{dt} = -u(x,t) + \rho \int dx' J(x,x')\, r(x',t) + I_{ext} $$ 其中$x$表示神经元的参数空间位点，$u(x)$代表在参数空间位点x上的神经元的突触总输入，$r(x^′​,t)$为神经元($x'$)的发放率，由以下公式给出: $$ r(x,t) = \frac{u(x,t)^2}{1+k\rho\int{dx&rsquo;u(x',t)^2}} $$ 该模型没有单独考虑抑制性神经元，而是将其作用效果包含在除法归一化作用中。而神经元($x$)和($x'$)之间的兴奋性连接强度$J(x, x')$由高斯函数给出: $$ J(X,x') = \frac{1}{\sqrt{2\pi} \alpha} \, exp\left( - \frac{|x-x'|^2}{2\alpha^2}\right) $$ 我们看到其具有平移不变性，即其为$(x-x’)$的函数。外界输入$I_{ext}$与位置$z(t)$有关，公式如下： $$ I_{ext}=A \, exp \left[ - \frac{|x-z(t)|^2}{4\alpha^2}\right] $$</p></div><div class=card-footer><span class=float-left>August 30, 2021</span>
<a href=/posts/paper-reading/continuous_attractor_nn/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/how_to_model/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Approaching Scientific Question & How to Model</h5><p class="card-text post-summary">Marr’s 3 levels of analysis Brain: hierarchy of complexities
Computational level - 1 What is the objective of the system? How close is it to optimal? Algorithmic level - 2 What are the data structures? What are the approximations? What is the runtime? Implementation level - 3 What is the hardware? Neurons? Synapses? Molecules? Diversity of modeling goals Useful: good at solving real-world problems?</p></div><div class=card-footer><span class=float-left>August 29, 2021</span>
<a href=/posts/life-learning/how_to_model/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/task_representation_cognitive_tasks/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[CN] Task representations in neural networks trained to perform many cognitive tasks</h5><p class="card-text post-summary">Yang, G.R., Joglekar, M.R., Song, H.F. et al. Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci 22, 297–306 (2019). https://doi.org/10.1038/s41593-018-0310-2
Abstract They trained single network models to perform 20 cognitive tasks that depend on working memory, decision making, categorization, and inhibitory control.
What they find: recurrent units can develop into clusters that are functionally specialized for different cognitive processes compositionality of task representations: one task can be performed by recombining instructions for other tasks.</p></div><div class=card-footer><span class=float-left>August 24, 2021</span>
<a href=/posts/paper-reading/task_representation_cognitive_tasks/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/integrated_understanding_system/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[DL & CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</h5><p class="card-text post-summary">McClelland, James L., et al. &ldquo;Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models.&rdquo; Proceedings of the National Academy of Sciences 117.42 (2020): 25966-25974.
Abstract For humans, language is a part of a system for understanding and communication about situations. There are some domain-general principles of biological neural network:
connection-based learning distributed representation context-sensitive mutual constraint satisfaction What they propose: the organization of the brain&rsquo;s distributed understanding system, which includes a fast learning system that addresses the memory problem.</p></div><div class=card-footer><span class=float-left>August 18, 2021</span>
<a href=/posts/paper-reading/integrated_understanding_system/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/lecture_james_mcclleland/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Lecture: Are People Still Smarter than Machines?</h5><p class="card-text post-summary">How can a neural network learn to do something cognitively interesting? The Biologically Inspired Approach If neuron A participates in firing neuron B, strengthen the connection from A to B The Optimization Approach Adjust each connection to minimize the network&rsquo;s deviation from a desired output Backpropogation algorithm became the basis of sbsequent research in the PDP framework, as well as almost all of Deep Learning Their Early Success of the Approach Models that could learn to read words and generalize to pronounceable non-words, capturing human-like response patterns: they refect statistical patterns and similarity relationships Models that captured many aspects of human semantic cognition and the disintegration of human semantic abilities resulting from neurodegenerative disease Models that showed in principle how aspects of language knowledge could be captured in a simple artificial neural network Nowadays AlexNet, ResNet and so on.</p></div><div class=card-footer><span class=float-left>August 17, 2021</span>
<a href=/posts/life-learning/lecture_james_mcclleland/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/neural-computation/basic-neuro-knowledge/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Neuroscience Basic Knowledge</h5><p class="card-text post-summary">The LIF model LIF: Leaky Integrated-and -Fire
1. Membrane Equation $$ \begin{align*} \
&\tau_m\,\frac{d}{dt}\,V(t) = E_{L} - V(t) + R\,I(t) &\text{if }\quad V(t) \leq V_{th} \\
\\
&V(t) = V_{reset} &\text{otherwise}\
\
\end{align*} $$
where $V(t)$ is the membrane potential(膜电势), $\tau_m$ is the membrane time constant, $E_{L}$ is the leak potential, $R$ is the membrane resistance, $I(t)$ is the synaptic input current, $V_{th}$ is the firing threshold, and $V_{reset}$ is the reset voltage.</p></div><div class=card-footer><span class=float-left>August 9, 2021</span>
<a href=/posts/neural-computation/basic-neuro-knowledge/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/lecture_yao_xin/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>每日学习———类脑智能的另一种思路</h5><p class="card-text post-summary">写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。
姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。</p></div><div class=card-footer><span class=float-left>August 9, 2021</span>
<a href=/posts/life-learning/lecture_yao_xin/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/reservoir_decision_making/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[BIC] A brain-inspired computational model for spatio-temporal information processing</h5><p class="card-text post-summary">Lin, Xiaohan, et al. &ldquo;A brain-inspired computational model for spatio-temporal information processing.&rdquo; Neural Networks 143 (2021): 74-87.
Abstract Current method: Explicit feature extraction, which requires lots of labeled data. Novel brain-inspired computational model: Reservoir Decision-making Network (RDMN) A reservoir model: projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics. (regarded it as SVM) A decision-making model: reads out neural representations via integrating information over time.</p></div><div class=card-footer><span class=float-left>July 20, 2021</span>
<a href=/posts/paper-reading/reservoir_decision_making/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/push-pull_feedback/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[BIC] Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently</h5><p class="card-text post-summary">Liu, Xiao, et al. &ldquo;Push-pull feedback implements hierarchical information retrieval efficiently.&rdquo; Advances in Neural Information Processing Systems 32 (2019): 5701-5710.
Front Word To understand this paper, you need a strong neuroscience background, especially knowing the Hopfield model and Hebbian theory. So before reading this paper, please preview the theories mentioned above!
To be honest, I still cannot understand the details quite well LoL :)
Abstract In addition to feedforward connections, there exist abundant feedback connections in a neural pathway.</p></div><div class=card-footer><span class=float-left>July 19, 2021</span>
<a href=/posts/paper-reading/push-pull_feedback/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/deep-learning/self-attention/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Self-attention</h5><p class="card-text post-summary">I. Self-attention Overview Input: A sequence of vectors (size not fixed) Output: Each vector has a label (POS tagging) The whole sequence has a label (sentiment analysis) Model decides the number of labels itself (seq2seq) Self-attention can handle global iformation, and FC can handle local information. Self-attention is the key module of Transformer, which will be shared in other articles.
II. How to work? Firstly, we should figure out the relevance between each vector.</p></div><div class=card-footer><span class=float-left>June 11, 2021</span>
<a href=/posts/deep-learning/self-attention/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/deep-learning/transformer/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Transformer</h5><p class="card-text post-summary">I. Transformer Overview Seq2seq model
Input: sequence of vector
Output: sequence (not fixed size)
II. Encoder It is actually a Self-Attention Model!
对于一个block，它的结构可以理解为以下的形式：
与self-attention不同的是，会采用residual add的方式，将self-attention得到的中间结果$a$加上输出$b$ 经过layer normalization得到新的输出 将新的output输入到FC中，并加上residual 再次经过layer normalization得到这一个block的结果。 理解了一个block以后，整个Encoder就是由n个这种block组成的network。
首先将word进行self-attention得到word embeddding，并在其中加入positional encoding信息 经过multi-head Attention或者Feed Forward Network后，都要接上residual + layer normalization，而这种设计结构就是Transformer Encoder的创新点所在。 III. Decoder —— Autoregressive(AT) The model structure of Decoder is shown as follow:
根据上图我们将逐步逐层地解释结构：
Masked Multi-head Self-attention 在产生每一个$b^i$ vector时，不能再看后面的信息，只能和前面的输入进行关系： 从细节上来看，要想得出$b^2$，我们只需将其和$k^1, k^2$做dot-product。</p></div><div class=card-footer><span class=float-left>June 11, 2021</span>
<a href=/posts/deep-learning/transformer/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/knn/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>KNN</h5><p class="card-text post-summary">原理 K近邻算法简单、直观，大致步骤为：
输入：训练集 $$T = {(x_1,y_1), (x_2, y_2), \dots, (x_N, y_N)}$$ 输出： 根据给定的距离度量，在训练集T中找出与$x$最近邻的$k$个点，涵盖这$k$个点的邻域记作$N_K(x)$； 找出最近邻的$k$个点的一个方法是搜索$kd$树； 在$N_k(x)$中根据分类决策规则（多数表决）决定$x$的类别$y$: $$y = arg \max_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j)$$ kd树</p></div><div class=card-footer><span class=float-left>April 21, 2021</span>
<a href=/posts/machine-learning/knn/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/posts/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/posts/>1</a></li><li class=page-item><a class=page-link href=/posts/page/2/>2</a></li><li class=page-item><a class=page-link href=/posts/page/3/>3</a></li><li class=page-item><a class=page-link href=/posts/page/4/>4</a></li><li class=page-item><a href=/posts/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/posts/page/4/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=/js/list.js></script></body></html>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Life Learning on ztqakita's Blog</title><link>https://ztqakita.github.io/posts/life-learning/</link><description>Recent content in Life Learning on ztqakita's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 14 Sep 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://ztqakita.github.io/posts/life-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Architectures of neuronal circuits</title><link>https://ztqakita.github.io/posts/life-learning/architectures_of_neuronal_circuits/</link><pubDate>Tue, 14 Sep 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/architectures_of_neuronal_circuits/</guid><description>参考智源社区发布文章《Science重磅综述：神经环路结构——使大脑“计算机”高速运转的系统》 https://hub.baai.ac.cn/view/9949
常见的环路模式（circuit motifs） 一、前馈兴奋（feedforward excitation） 兴奋性神经元之间的一系列连续连接构成前馈兴奋，是信号从一个神经区域传递到另一个区域的主要方式。在每个阶段，神经元通常从多个突触前连接接收信号输入，并通过分支轴突向多个突触后连接（发散式兴奋）输出信号。
收敛式兴奋（convergent excitation）能使突触后连接选择性地对突触前连接的确定神经元作出反应。当多个输入神经元携带相同但不相关的信号时，也能提高信号的信噪比。 发散式兴奋（divergent excitation）则是将同一信号被多个下游通路处理。 比如哺乳动物的视觉系统，信号从光感受器→双极细胞→视网膜神经节细胞→外侧膝状核（LGN）神经元→第4层初级视觉皮层（V1）神经元→其他层的V1神经元→皮层上层神经元
二、前馈抑制和反馈抑制（Feedforward and feedback inhibition） 两种广泛使用的模式是前馈抑制和反馈抑制。
在前馈抑制中，抑制性神经元从突触前的兴奋性神经元接收信号输入，因此抑制性和突触前兴奋性输入都汇聚于突触后神经元。 在反馈抑制中，抑制性神经元接收来自兴奋性神经元的输入，并将其投射回兴奋性神经元。 在上述的视觉通路中，几乎每一个兴奋性连接都伴随着前馈抑制、反馈抑制或两者兼有。例如LGN神经元直接激活V1 GABA释放神经元，对第4层兴奋性神经元提供前馈抑制，第4层兴奋性神经元也激活V1 GABA释放神经元，对自身提供反馈抑制。
三、侧抑制（Lateral inhibition） 侧抑制是一种广泛存在的环路模式。它通过放大平行通路之间的活性差异来选择要传递到下游环路的信息。
例如，脊椎动物视网膜中光感受器神经元激活水平细胞（horizontal cell），对附近的许多光感受器神经元进行反馈抑制。这一行为作用于下游神经节细胞的典型中央-周围感受区域，从而增强这些下游神经元对空间或颜色对比信息的提取能力
四、相互抑制（Mutual inhibition） 相互抑制被广泛应用于具有节律性活动的环路中，如涉及运动的环路。在更长的时间范围内，相互抑制也可以用来调节大脑状态，如睡眠-觉醒周期。
具有特定功能的特殊环路结构 一、连续性拓扑映射（Continuous topographic mapping） 连续拓扑映射是神经系统中呈现信息的一种常用组织方式。相邻的输入神经元通过轴突有序的投射连接到相邻的目标神经元。由于其稳健的发育机制，拓扑映射可以提供处理连续层次信息的便捷方式。它具有很多信息处理计算上的优势。
例如，视网膜拓扑映射通过侧抑制来促进局部对比度的提取能力，从而加强物体识别。此外，通过在相邻的地方安置环路元件，映射可以通过最小化环路长度来节省能源。“回旋式神经网络”（convolutional neural networks（包含回旋计算且具有深度结构的前馈神经网络，是深度学习的代表算法之一）的设计就借鉴了拓扑映射，极大地减少了调整人工神经网络所需的变量数，从而加快了计算速度。
二、离散并行处理（Discrete parallel processing） 离散并行处理通过离散的信息通道并行呈现和处理信号。
三、维度扩展（Dimensionality expansion） 在维度扩展结构中，来自相对少量的输入神经元的信号发散到数量多得多的输出神经元上（图5C），允许输出神经元呈现不同的信号输入组合。
待求证：这是否与 reservoir network 相关呢？该网络也是将低维的神经信号模式投射到高维空间，使得这些模式特征可以在高维空间中线性可分。
一个维度扩张的例子是内嗅皮层→齿状回颗粒细胞→CA3锥体神经环路。大量的齿状回颗粒细胞可以对来自内嗅皮层的关于空间和物体的信息进行模式分离，并由海马下游环路进一步处理。但与蘑菇体和小脑皮层不同，该环路不能进行学习和训练。这可能是因为海马区微环路进行的是无监督学习，而小脑和蘑菇体微环路进行的是有监督的强化学习。
四、循环回路（Recurrent loops） 神经系统充满了循环回路，在这些回路中，神经元通常通过中间神经元相互连接。这些循环回路从特定的神经区域到大脑的大部分区域，在规模上是不均一的。</description></item><item><title>Approaching Scientific Question &amp; How to Model</title><link>https://ztqakita.github.io/posts/life-learning/how_to_model/</link><pubDate>Sun, 29 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/how_to_model/</guid><description>Marr’s 3 levels of analysis Brain: hierarchy of complexities
Computational level - 1 What is the objective of the system? How close is it to optimal? Algorithmic level - 2 What are the data structures? What are the approximations? What is the runtime? Implementation level - 3 What is the hardware? Neurons? Synapses? Molecules? Diversity of modeling goals Useful: good at solving real-world problems?</description></item><item><title>Lecture: Are People Still Smarter than Machines?</title><link>https://ztqakita.github.io/posts/life-learning/lecture_james_mcclleland/</link><pubDate>Tue, 17 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/lecture_james_mcclleland/</guid><description>How can a neural network learn to do something cognitively interesting? The Biologically Inspired Approach If neuron A participates in firing neuron B, strengthen the connection from A to B The Optimization Approach Adjust each connection to minimize the network&amp;rsquo;s deviation from a desired output Backpropogation algorithm became the basis of sbsequent research in the PDP framework, as well as almost all of Deep Learning Their Early Success of the Approach Models that could learn to read words and generalize to pronounceable non-words, capturing human-like response patterns: they refect statistical patterns and similarity relationships Models that captured many aspects of human semantic cognition and the disintegration of human semantic abilities resulting from neurodegenerative disease Models that showed in principle how aspects of language knowledge could be captured in a simple artificial neural network Nowadays AlexNet, ResNet and so on.</description></item><item><title>每日学习———类脑智能的另一种思路</title><link>https://ztqakita.github.io/posts/life-learning/lecture_yao_xin/</link><pubDate>Mon, 09 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/lecture_yao_xin/</guid><description>写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。
姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。</description></item></channel></rss>
<!doctype html><html><head><title>Recursive Neural Network</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><meta name=description content="Recursive Neural Network"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/neural-computation/basic-neuro-knowledge/>Neuroscience Basic Knowledge</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/deep-learning/>DL</a><ul class=active><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a class=active href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/life-learning/>Life Learning</a><ul><li><a href=/posts/life-learning/architectures_of_neuronal_circuits/>Architectures of neuronal circuits</a></li><li><a href=/posts/life-learning/how_to_model/>how to model</a></li><li><a href=/posts/life-learning/lecture_james_mcclleland/>Lecture James McClleland</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/continuous_attractor_nn/>Continuous-attractor Neural Network</a></li><li><a href=/posts/paper-reading/few-shot_class_incremental_learning/>Few-Shot Class-Incremental Learning</a></li><li><a href=/posts/paper-reading/integrated_understanding_system/>Integrated understanding system</a></li><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li><li><a href=/posts/paper-reading/task_representation_cognitive_tasks/>Task representations in neural networks</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ztqakita.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ztq.png><h5 class=author-name>Brandon Zhang</h5><p>October 15, 2020</p></div><div class=title><h1>Recursive Neural Network</h1></div><div class=post-content id=post-content><h2 id=i-rnn-structure-overview>I. RNN Structure Overview</h2><p>The input is a sequence of vectors.
<img src=/images/posts/DL/RNN.JPG alt></p><blockquote><p>note: Changing the <em><strong>input sequence order</strong></em> will change the output</p></blockquote><p>We use the same neural network to train, each color in NN means the same weight. When the values stored in the memory is different, the output will also be different.
<img src=/images/posts/DL/stru.JPG alt></p><h2 id=ii-types-of-rnn>II. Types of RNN</h2><ol><li><p>Elman&rsquo;s memory restore the values of hidden layer output, and Jordan&rsquo;s memory restore the values of output. Usually Jordan network have better performance because output y has target.
<img src=/images/posts/DL/types.JPG alt></p></li><li><p>Bidirectional RNN
<img src=/images/posts/DL/biRNN.JPG alt>
Using bidirectional RNN can get the information from the <em><strong>context</strong></em>. For example, for the hidden layer output $y^{t+1}$, its value is determined by not only $x^t$ and $x^{t+1}$ but also $x^{t+1}$ and $x^{t+2}$.</p></li></ol><h2 id=iii-long-short-term-memory-lstm>III. *Long Short-term Memory (LSTM)</h2><h3 id=1-lstm-overview>1. LSTM Overview</h3><p><strong>Input gate</strong>: Decide whether the input can be written in memory cell;
<strong>Output gate</strong>: Decide whether the output of memory cell can be sent to other part of the network;
<strong>Forget gate</strong>: Decide whether the content in memory cell should be erased.
<img src=/images/posts/DL/LSTMoverview.JPG alt></p><h3 id=2-detailed-process>2. Detailed process</h3><p><img src=/images/posts/DL/RNNdetailed.JPG alt>
In this figure, there is a input $z$ and three other scalar $z_i, z_f, z_o$(信号量). Activation function f is usually a <strong>sigmoid function</strong>, whose value is <strong>between 0 and 1</strong>. $f(z_i)$ controls the input value of $g(z)$, and $f(z_f)$ controls the memory cell&rsquo;s value. The new value stored in the memory cell is $c'$.</p><h3 id=3-example>3. Example</h3><p>此处强烈建议看一下PPT中的LSTM讲解：
Every scalar is determined by input vector(例子中是三维) through linear transform, which means every input vector multiply the <strong>corresponding weight and add the bias</strong>. All the weight and bias are trained by trainig data through GD.</p><h3 id=4-lstms-parameter>4. LSTM&rsquo;s parameter</h3><p>LSTM的每一块相当于原neural network的神经元，可以直接替换。LSTM的参数量为feed-forward NN的参数量的4倍。</p><h3 id=5-lstm-model>5. LSTM Model</h3><p><img src=/images/posts/DL/LSTM.JPG alt>
In this figure, every dimension of vector $z$ is a input of LSTM neuron, and the product operator is actually <strong>element-wise product</strong>.
For the standard LSTM, it has multiple layer and the input of every layer consists of <strong>current input vector $x^{t+1}$, the output of previous layer $h^t$ and the memory cell $c^t$</strong>.</p><blockquote><p>In fact, you could use model in Keras such as &ldquo;LSTM&rdquo;, &ldquo;GRU&rdquo;(simplified LSTM) and &ldquo;SimpleRNN&rdquo;.</p></blockquote><h2 id=iv-rnn>IV. RNN</h2><h3 id=1-loss-function>1. Loss Function</h3><p>The cross-entropy between RNN output and reference vector for every time slot.
<img src=/images/posts/DL/cost.JPG alt></p><blockquote><p>Reference vector就是每一个单词所对应的reference组成的向量。</p></blockquote><h3 id=2-learning>2. Learning</h3><p>Method: Backpropogation through time (BPTT)
However, RNN-based network is not always easy to learn. The error surface is either very flat or very steep. When you step into a point whose gradient is relatively large and your learning rate is also large, your learning step will make a really huge move so that the program will tell you <strong>segmentation fault.</strong>
One way to avoid this circumstance is to use <em><strong>Cliping</strong></em>, which means to <strong>set a threshold</strong> of gradient and every move can not larger than the threshold.
<img src=/images/posts/DL/threshold.JPG alt></p><p>The reason for this rough error surface is shown as below:
<img src=/images/posts/DL/example.JPG alt></p><h3 id=3-helpful-techniques>3. Helpful Techniques</h3><ol><li>RNN vs LSTM
LSTM can deal with <em><strong>gradient vanishing</strong></em>(not gradient explore).<ul><li>LSTM: Memory and input are <strong>added and stored</strong>in memory unless forget gate is closed.
RNN: Every time memory will be <strong>formated</strong>.</li><li>The influence never disappears unless forget gate is closed.</li></ul></li></ol><blockquote><p>此处老师说有国际大厂面试里考过哦！</p></blockquote><p>Note: <strong>Gate Recurrent Unit(GRU)</strong> is simpler than LSTM for it has only two gate. 它将 input gate 和 forget gate 进行了联动。</p><p>Other techniques:
<img src=/images/posts/DL/other.JPG alt></p><blockquote><p>此处老师讲述了Hinton的一篇论文，用一般training的方法，initialize的weight是random的话，sigmoid的效果要好于ReLu；他用indentity matrix来initialize weight，这样使用ReLu作为Activation函数则可以显著提高效果。利用这种方法的RNN在效果上吊打LSTM。非常的玄学</p></blockquote><h3 id=4-more-application>4. More Application</h3><ol><li>Many to one:
Input is a vector sequence, but output is only one vector</li><li>Many to many:
Both input and output are both sequences, but the output is shorter.</li></ol><ul><li><p>CTC
<img src=/images/posts/DL/CTC.JPG alt></p></li><li><p>Sequence to sequence learning
有一篇论文讲到可以直接将某种语言的语音信号作为输入，得到另一种语言文字形式的输出，并发现其有可行性。</p><ul><li>Seq2seq Auto Encoder(没听懂)</li></ul></li></ul><h2 id=v-attention-based-model>V. Attention-based Model</h2><h3 id=1-model-overview>1. Model Overview</h3><p>其中的DNN/RNN类似于计算机中的CPU概念。
<img src=/images/posts/DL/ABM.JPG alt></p><h3 id=2-model-version-2>2. Model version 2</h3><p><strong>Neural Turing Machine</strong>：
<img src=/images/posts/DL/ABM2.JPG alt></p></div><div class=btn-improve-page><a href=https://github.com/ztqakita/ztqakita.github.io/edit//content/posts/deep-learning/RNN.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/database-system/database_system_2/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>Database System Lecture Note 2</span></a></div><div class="col-md-6 next-article"><a href=/posts/operating-system/operating_system_concepts_4/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Process Synchronization</span></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#i-rnn-structure-overview>I. RNN Structure Overview</a></li><li><a href=#ii-types-of-rnn>II. Types of RNN</a></li><li><a href=#iii-long-short-term-memory-lstm>III. *Long Short-term Memory (LSTM)</a><ul><li><a href=#1-lstm-overview>1. LSTM Overview</a></li><li><a href=#2-detailed-process>2. Detailed process</a></li><li><a href=#3-example>3. Example</a></li><li><a href=#4-lstms-parameter>4. LSTM&rsquo;s parameter</a></li><li><a href=#5-lstm-model>5. LSTM Model</a></li></ul></li><li><a href=#iv-rnn>IV. RNN</a><ul><li><a href=#1-loss-function>1. Loss Function</a></li><li><a href=#2-learning>2. Learning</a></li><li><a href=#3-helpful-techniques>3. Helpful Techniques</a></li><li><a href=#4-more-application>4. More Application</a></li></ul></li><li><a href=#v-attention-based-model>V. Attention-based Model</a><ul><li><a href=#1-model-overview>1. Model Overview</a></li><li><a href=#2-model-version-2>2. Model version 2</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>
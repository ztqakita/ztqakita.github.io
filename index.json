[{"categories":null,"contents":" Yang, G.R., Joglekar, M.R., Song, H.F. et al. Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci 22, 297–306 (2019). https://doi.org/10.1038/s41593-018-0310-2\n Abstract ","date":"August 24, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/paper-reading/task_representation_cognitive_tasks/","summary":" Yang, G.R., Joglekar, M.R., Song, H.F. et al. Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci 22, 297–306 (2019). https://doi.org/10.1038/s41593-018-0310-2\n Abstract ","tags":null,"title":"[CN] Task representations in neural networks trained to perform many cognitive tasks"},{"categories":null,"contents":" McClelland, James L., et al. \u0026ldquo;Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models.\u0026rdquo; Proceedings of the National Academy of Sciences 117.42 (2020): 25966-25974.\n Abstract For humans, language is a part of a system for understanding and communication about situations. There are some domain-general principles of biological neural network:\n connection-based learning distributed representation context-sensitive mutual constraint satisfaction  What they propose: the organization of the brain\u0026rsquo;s distributed understanding system, which includes a fast learning system that addresses the memory problem.\nPrinciples of Neural Computation A certain principles in PDP(parallel distributed processing) framework is the idea that cognition depends on mutual constraint satisfaction. We can regard it as a learning process which can construct context-sensitive representations. And BERT can do it really well, depend on QBA model.\n QBA model 上下文阴影词表征之间的相似性关系可以用来重建一个句子的句法结构描述。而这不需要structure build in.\n As far as I know, BERT works in parallel, using mutual QBA simultaneously on all of the words in an input text block. But this block has limit size, indicating that we can not use all the context information. Humans appear to exploit past context and a limited window of subsequent context,suggesting a hybrid strategy.\n 推荐阅读：Z. Yang et al., “Xlnet: Generalized autoregressive pretraining for language understanding” in Advances in Neural Information Processing Systems, H. Wallach et al., Eds. (Curran Associates, Inc., Red Hook, NY, 2019), vol. 32, pp. 5753–5763.\n Language in an Integrated Understanding System The author argues that part of the solution will come from treating language as part of a larger system for understanding and communicating, and the targets of understanding are situations. Situations are collections of entities, their properties and relations, and patterns of change in them. What it means to understand a situation is to construct a representation of it that captures aspects of the participating objects, their properties, relationships and interactions, and resulting outcomes.\nModel of Understanding  Assumptiom: Only focus on concrete situations involving animate beings and physical objects.   Visual Subsystem: subserves the formation of a visual representation of the given situation. Aduitory Subsystem: subserves the formation of an auditory representation capturing the spatiotemporal structure of the co-occurring sopken language. Object Subsystem: an intermodal area, receiving visual, language, and other information aobut objects. This area is the hidden layer of an interactive, recurrent network with bidirectional connections to other layers representing different type of object properties. Language Subsytem: The understanding of microsituations depends jointly on this system and context system. Context Subsystem：There is a network of areas in the brain that capture the spatiotemporal context.  Within each subsystem, and between each connected pair of subsystems, the neurons are reciprocally interconnected via learning-dependent pathways, allowing mutual constraint satisfaction among all of the elements of each of the representation types.\nMTL(Integrated System State) 随机的新信息在很短的时间内出现，当前的brain system很难去处理并在未来任意时间去从记忆中使用。这一点正是这个单元想去解决的问题。MTL的功能就是支持新的任意关联的形成，将一个experience的elements链接在一起，including the objects and language encountered in a situation and the co-occurring spatiotemporal context.\n GPT-3当遇到新的context时，之前this word 的 representation 信息会被覆盖掉，相当于没有记忆机制。作者在原文中如此写道：Further research should explore whether augmenting a model like GPT-3 with an MTL-like memory would enable more human-like extension of a novel word encountered just once to a wider range of contexts.\n Implementation of Model  Input of model: sequences of microsituation each consisting of a picture-description(language)(PD)pair grouped into scenes that in turn form story-length narratives, with the language conveyed by text rather than speech.  Process  Each PD pair is processed by interacting object and language subsystems, receiving visual and text input at the same time. Each subsystem must learn to restore missing or distorted elements by using mutual QBA as in BERT. The context subsystem encodes a sequence of compressed represenation of the previous PD pair within the current scene. Within the processing of a pair, the context system would engage in mutual QBA with the object and language subsystems, allowing the language and object subsystems to indirectly exploit information from past pairs within the scene.(下属的subsystem可以得到历史信息，这样以便于他们地道道更好的representation，实现所谓的mutual constraint satisfaction) 一个具有学习过的连接权重的神经网络在处理完每个PD对之后，构建了对物体、语言和语境子系统的状态以及视觉和文本子系统的状态的可逆的还原描述。这个压缩向量被存储在MTL-like memory里。  Enhancing Uderstanding by Using Reinforcement leaarning 和强化学习结合，从而在情境中去得到语言、图片信息，从而去做出相应的选择。并且这种选择经过一些训练可以学习为一种固定的模式，再得到类似情景的输入时就会去做相同的事情，有了认知的行为。\n可以拓展阅读这几篇论文：\n K. M. Hermann et al., Grounded language learning in a simulated 3D world. arXiv:1706.06551 (20 June 2017). R. Das, M. Zaheer, S. Reddy, A. McCallum, “Question answering on knowledge bases and text using universal schema and memory networks” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistic, R. Barzilay, M.-Y. Kan, Eds. (Association for Computational Linguistics, Stroudsburg, PA, 2017), pp. 358–365. D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, R. Salakhutdinov, “Gated-attention architectures for task-oriented language grounding” in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th Innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), S. A. McIlraith, K. Q. Weinberger, Eds. (AAAI Press, Cambridge, MA, 2018), pp. 2819–2826. J. Oh, S. Singh, H. Lee, P. Kohli, “Zero-shot task generalization with multi-task deep reinforcement learning” in Proceedings of the 34th International Conference on Machine Learning-Volume 70, D. Precup, Y. W. Teh, Eds. (JMLR.org, 2017), pp. 2661–2670. F. Hill et al., “Environmental drivers of systematicity and generalization in a situated agent” in International Conference on Learning Representations (ICLR, 2020).  ","date":"August 18, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/paper-reading/integrated_understanding_system/","summary":"McClelland, James L., et al. \u0026ldquo;Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models.\u0026rdquo; Proceedings of the National Academy of Sciences 117.42 (2020): 25966-25974.\n Abstract For humans, language is a part of a system for understanding and communication about situations. There are some domain-general principles of biological neural network:\n connection-based learning distributed representation context-sensitive mutual constraint satisfaction  What they propose: the organization of the brain\u0026rsquo;s distributed understanding system, which includes a fast learning system that addresses the memory problem.","tags":null,"title":"[DL \u0026 CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models"},{"categories":null,"contents":"How can a neural network learn to do something cognitively interesting?  The Biologically Inspired Approach  If neuron A participates in firing neuron B, strengthen the connection from A to B   The Optimization Approach  Adjust each connection to minimize the network\u0026rsquo;s deviation from a desired output   Backpropogation algorithm became the basis of sbsequent research in the PDP framework, as well as almost all of Deep Learning  Their Early Success of the Approach  Models that could learn to read words and generalize to pronounceable non-words, capturing human-like response patterns: they refect statistical patterns and similarity relationships Models that captured many aspects of human semantic cognition and the disintegration of human semantic abilities resulting from neurodegenerative disease Models that showed in principle how aspects of language knowledge could be captured in a simple artificial neural network  Nowadays  AlexNet, ResNet and so on. Transformer Pre-trained model Alpha Zero  What changed?  The scale of computing and the size of the available data sets Neural network architecture innovations have been essential as well  Toward human level language understanding   Current language model just exploit QBA(Query-based Attention) over a finite window of text to predict missing or upcoming words\n  Human exploit language and other sources of input such as vision to form a representation of situation they are witnessing and hearing about\n  They can also exploit information from the indefinite past\n  The lecturer has recently proposed using mutual QBA within the neuroscience-informed architecture at below to address these human capabilities Each oval in the neocortex denotes a BERT-like QBA system mutually querying each of the others. All query previous system states stored in the MTL system, viewed as similar to the external memory in the DNC\n   Placing language in an integrated understanding system. PNAS. 10.1073/pnas.1910416117\n Flaws of Models Today Neural networks require massive data sets, snd show poor out-of-sample generalization.\nThoughts of Cognitive Abilities Fodor and others may be right that advanced cognitive abilities depend on the ability to reason by means of the application of category- and structure-sensitive rules.\nHowever, the ability to reason in this way may be an acquired human ability, rathrer than an inherent trait of the human mind.\nWhat will it take to build machines that can exploit intuition and acquire systematic cognitive abilities? We need to add to this by teaching neural networks to engage in discourse as people do, including teaching them to:\n Follow instructions to carry out specified tasks Adopt specified goals and/or subgoals Conform to specified constraints to allowable actions Make targeted corrections to their behavior Understand and provide explanations Connect language and symbolic expressions to an underlying conceptual framework Reason within the framework rather than simply manipulate symbols  ","date":"August 17, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/life-learning/lecture_james_mcclleland/","summary":"How can a neural network learn to do something cognitively interesting?  The Biologically Inspired Approach  If neuron A participates in firing neuron B, strengthen the connection from A to B   The Optimization Approach  Adjust each connection to minimize the network\u0026rsquo;s deviation from a desired output   Backpropogation algorithm became the basis of sbsequent research in the PDP framework, as well as almost all of Deep Learning  Their Early Success of the Approach  Models that could learn to read words and generalize to pronounceable non-words, capturing human-like response patterns: they refect statistical patterns and similarity relationships Models that captured many aspects of human semantic cognition and the disintegration of human semantic abilities resulting from neurodegenerative disease Models that showed in principle how aspects of language knowledge could be captured in a simple artificial neural network  Nowadays  AlexNet, ResNet and so on.","tags":null,"title":"Lecture: Are People Still Smarter than Machines?"},{"categories":null,"contents":"The LIF model  LIF: Leaky Integrated-and -Fire\n 1. Membrane Equation $$ \\begin{align*} \\\n\u0026amp;\\tau_m\\,\\frac{d}{dt}\\,V(t) = E_{L} - V(t) + R\\,I(t) \u0026amp;\\text{if }\\quad V(t) \\leq V_{th} \\\\\n\\\\\n\u0026amp;V(t) = V_{reset} \u0026amp;\\text{otherwise}\\\n\\\n\\end{align*} $$\nwhere $V(t)$ is the membrane potential(膜电势), $\\tau_m$ is the membrane time constant, $E_{L}$ is the leak potential, $R$ is the membrane resistance, $I(t)$ is the synaptic input current, $V_{th}$ is the firing threshold, and $V_{reset}$ is the reset voltage. We can also write $V_m$ for membrane potential, which is more convenient for plot labels.\nThe membrane equation describes the time evolution of membrane potential $V(t)$ in response to synaptic input and leaking of charge across the cell membrane. This is an ordinary differential equation (ODE).\n2. Reset Condition A spike takes place whenever $V(t)$ crosses $V_{th}$. In that case, a spike is recorded and $V(t)$ resets to $V_{reset}$ value. This is summarized in the *reset condition*: $$V(t) = V_{reset}\\quad \\text{ if } V(t)\\geq V_{th}$$\n3. Refractory Period The absolute refractory period is a time interval in the order of a few milliseconds during which synaptic input will not lead to a 2nd spike, no matter how strong.\n4. Python Code for Simulating a LIF Neuron # Simulation class class LIFNeurons: \u0026#34;\u0026#34;\u0026#34; Keeps track of membrane potential for multiple realizations of LIF neuron, and performs single step discrete time integration. \u0026#34;\u0026#34;\u0026#34; def __init__(self, n, t_ref_mu=0.01, t_ref_sigma=0.002, tau=20e-3, el=-60e-3, vr=-70e-3, vth=-50e-3, r=100e6): # Neuron count self.n = n # Neuron parameters self.tau = tau # second self.el = el # milivolt self.vr = vr # milivolt self.vth = vth # milivolt self.r = r # ohm # Initializes refractory period distribution self.t_ref_mu = t_ref_mu self.t_ref_sigma = t_ref_sigma self.t_ref = self.t_ref_mu + self.t_ref_sigma * np.random.normal(size=self.n) self.t_ref[self.t_ref\u0026lt;0] = 0 # State variables self.v = self.el * np.ones(self.n) self.spiked = self.v \u0026gt;= self.vth self.last_spike = -self.t_ref * np.ones([self.n]) self.t = 0. self.steps = 0 def ode_step(self, dt, i): # Update running time and steps self.t += dt self.steps += 1 # One step of discrete time integration of dt self.v = self.v + dt / self.tau * (self.el - self.v + self.r * i) # Spike and clamp self.spiked = (self.v \u0026gt;= self.vth) self.v[self.spiked] = self.vr self.last_spike[self.spiked] = self.vr clamped = (self.last_spike + self.t_ref \u0026gt; self.t) self.v[clamped] = self.vr self.last_spike[self.spiked] = self.t # Set random number generator np.random.seed(2020) # Initialize step_end, t_range, n, v_n and i t_range = np.arange(0, t_max, dt) step_end = len(t_range) n = 500 v_n = el * np.ones([n, step_end]) i = i_mean * (1 + 0.1 * (t_max / dt)**(0.5) * (2 * np.random.random([n, step_end]) - 1)) # Initialize binary numpy array for raster plot raster = np.zeros([n,step_end]) # Initialize neurons neurons = LIFNeurons(n) # Loop over time steps for step, t in enumerate(t_range): # Call ode_step method neurons.ode_step(dt, i[:,step]) # Log v_n and spike history v_n[:,step] = neurons.v raster[neurons.spiked, step] = 1. # Report running time and steps print(f\u0026#39;Ran for {neurons.t:.3}s in {neurons.steps} steps.\u0026#39;) # Plot multiple realizations of Vm, spikes and mean spike rate plot_all(t_range, v_n, raster) The Mammalian Visual System 1. Structure  Estimating receptive fields: spike-triggered averaging(STA)  ","date":"August 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/neural-computation/basic-neuro-knowledge/","summary":"The LIF model  LIF: Leaky Integrated-and -Fire\n 1. Membrane Equation $$ \\begin{align*} \\\n\u0026amp;\\tau_m\\,\\frac{d}{dt}\\,V(t) = E_{L} - V(t) + R\\,I(t) \u0026amp;\\text{if }\\quad V(t) \\leq V_{th} \\\\\n\\\\\n\u0026amp;V(t) = V_{reset} \u0026amp;\\text{otherwise}\\\n\\\n\\end{align*} $$\nwhere $V(t)$ is the membrane potential(膜电势), $\\tau_m$ is the membrane time constant, $E_{L}$ is the leak potential, $R$ is the membrane resistance, $I(t)$ is the synaptic input current, $V_{th}$ is the firing threshold, and $V_{reset}$ is the reset voltage.","tags":null,"title":"Neuroscience Basic Knowledge"},{"categories":null,"contents":"写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。\n姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。\n","date":"August 9, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/life-learning/lecture_yao_xin/","summary":"写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。\n姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。","tags":null,"title":"每日学习———类脑智能的另一种思路"},{"categories":null,"contents":" Lin, Xiaohan, et al. \u0026ldquo;A brain-inspired computational model for spatio-temporal information processing.\u0026rdquo; Neural Networks 143 (2021): 74-87.\n Abstract  Current method: Explicit feature extraction, which requires lots of labeled data. Novel brain-inspired computational model:  Reservoir Decision-making Network (RDMN) A reservoir model: projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics. (regarded it as SVM) A decision-making model: reads out neural representations via integrating information over time.   Tasks:  Looming pattern discrimination Gait recognition Event-based gait recognition    The model Overview  Summary: A spatio-temporal pattern is first processed by the reservoir module and then read out by the decision-making module.  The decision-making model  Summary: The decision-making model consists of several competing neurons, with each of them representing one category(pattern). Each decision-making neuron receives inputs from the reservoir module and they compete with each other via mutual inhibition, with the winner reporting the recognition result.\n   The dynamics of the module:\nEq.(3) describes the slow dynamics of the synaptic current due to the activity-dependent NMDA receptors.\n  Parameters:\n $x_i$: synaptic inputs received by the $i$th neuron. $r_i$: neural activities. $s_i$: the synapic current due to NMDA receptors. $J_E$: represents the excitatory interactions between neurons encoding the same category. $J_M$: indicates mutual inhibition. $I_i$: is the feedforward input from the reservoir module, whose form is optimized through learning.    The mechanism of decision-making\n Three types of stationary state  LAS: Low active state, all neurons are at the same low-level activity DMS: Decision-making state, in which one neuron is at high activity and the other at low activity. EAS: Explosively active state, in which all neurons are at the same high-level activity. Apparently, only the parameter regime for DMS is suitable for decision-making. The optimal region is at the bifurcation boundary between LAS and DMS, which is called DM-boundary.      论文在后续对参数的选择进行了阐述，对于decision-making model而言，参数是固定的，也就是无需训练的，它的参数选择是人为选择的。一般是平衡了激活速度和准确率的中间平衡点作为参数选择。\nThe reservoir model  Summary: The reservoir module consists of several forwardly connected layers, with each layer having a large number of recurrently connected neurons.\n   Function The decision-making model is not enough for discriminating complex spatio-temporal patterns, but the reservoir model could map different spatio-temporal patterns into spatially separated neural activities, so that the decision-making module can read out them.\n  Structure of model\n Consists of $L$ forwardly connected layers, and neurons in each layer are connected recurrently. Only Layer 1 receives the external input.    Intergrate two model  The reservoir and decision-making modules are integrated via a linear read-out matrix to carry out a discrimination task, where the read-out matrix is optimized using known examples.  $W_{lj}^{dm,i}$ denotes the connection weight from neuron j in layer l of the reservoir network to neuron i in the decision-making module. $I^*_0$ is the optimal feedforawrd input specified by the DM-boundary.   We optimize the read-out matrix Wdm by minimizing the error function E using backpropagation through time.   Model application  Looming pattern discrimination Gait recognition Event-based gait recognition A key characteristic of biological decision-making is its event-based nature, i.e., the neural system will automatically detect and recognize the presence of an input pattern. It saves the effort of signal detection.  Future work RDMN model cannot explicitly encode order information. The order of a spatio-temporal pattern contains important cause–effect information and temporal correlations, which we humans actively exploit to discriminate spatio-temporal patterns. Thus, incorporating this prior knowledge into models should significantly improve the performance of spatio-temporal pattern discrimination tasks.\n","date":"July 20, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/paper-reading/reservoir_decision_making/","summary":"Lin, Xiaohan, et al. \u0026ldquo;A brain-inspired computational model for spatio-temporal information processing.\u0026rdquo; Neural Networks 143 (2021): 74-87.\n Abstract  Current method: Explicit feature extraction, which requires lots of labeled data. Novel brain-inspired computational model:  Reservoir Decision-making Network (RDMN) A reservoir model: projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics. (regarded it as SVM) A decision-making model: reads out neural representations via integrating information over time.","tags":null,"title":"[BIC] A brain-inspired computational model for spatio-temporal information processing"},{"categories":null,"contents":" Liu, Xiao, et al. \u0026ldquo;Push-pull feedback implements hierarchical information retrieval efficiently.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019): 5701-5710.\n Front Word To understand this paper, you need a strong neuroscience background, especially knowing the Hopfield model and Hebbian theory. So before reading this paper, please preview the theories mentioned above!\nTo be honest, I still cannot understand the details quite well LoL :)\nAbstract  In addition to feedforward connections, there exist abundant feedback connections in a neural pathway. This paper investigate the role of feedback in hierarchical information retrieval. a hierarchical network storing the hierarchical categorical information of objects:  Mechanism: information retrieval goes from rough to fine, aided by dynamical push-pull feedback from higher to lower layers. Function: 我们阐明，最佳反馈应该是动态的，随着时间的推移从正（推）到负（拉）而变化，它们分别抑制了来自不同类别和相同类别的模式关联所带来的干扰。    A model for Hierarchical Information Representation The model consists of three layers which store three-level of hierarchical memory patterns. From button to top, we call the three layers:\n Child layer Parent layer Grandparent layer   Neurons in the same layer are connected recurrently with each other to function as an associative memory. Between layers, neurons communicate via feedforward and feedback connections.\n Symbol Declaration: $x_i^l(t)$: the state of neuron $i$ in layer $l$ at time $t$. value takes $\\pm 1$.\n$W_{ij}^{l}$: symmetric recurrent connections from neuron $j$ to $i$ in layer $l$.\n$W_{ij}^{l+1, l}$: the **feedforward connections** from neuron $j$ of layer $l$ to neuron $i$ in layer $l + 1$.\n$W_{ij}^{l, l+1}$: the **feedback connections** from neuron $j$ of layer $l + 1$ to neuron $i$ of layer $l$.\n There are 3 layers in this model, and each layer has $N$ neurons. The neuronal dynamics follows the Hopfield model, which is written as $$ x_i^l(t+1) = sign[h_i^l(t)]. $$ $h_i^l(t)$ is the total input received by the neuron. Using layer 1 as an example, we could have $$ h_i^l(t) = \\sum_j W_{ij}^1 x_j^1(t) + W_{ij}^{1,2}x_j^2(t). $$\nAccording to the figure above, $\\xi$ means memory pattern, and we can construct weight based on these patterns:\n recurrent connections: $$W_{ij}^1 = \\sum_{\\alpha, \\beta, \\gamma} \\xi_i^{\\alpha,\\beta,\\gamma} \\xi_j^{\\alpha,\\beta,\\gamma} / N,$$ $$W_{ij}^2 = \\sum_{\\alpha, \\beta} \\xi_i^{\\alpha,\\beta} \\xi_j^{\\alpha,\\beta} / N,$$ $$W_{ij}^3 = \\sum_{\\alpha} \\xi_i^{\\alpha} \\xi_j^{\\alpha} / N.$$ feedforward connections: $$W_{ij}^{2,1} = \\sum_{\\alpha, \\beta, \\gamma} \\xi_i^{\\alpha,\\beta} \\xi_j^{\\alpha,\\beta,\\gamma} / N,$$ $$W_{ij}^{3,2} = \\sum_{\\alpha, \\beta} \\xi_i^{\\alpha} \\xi_j^{\\alpha,\\beta} / N.$$  To quantify the retrieval performance, we define $m(t)$ to measure the overlap between neural state and memory pattern. $$ m^{\\alpha,\\beta,\\gamma}(t) = \\frac{1}{N} \\sum_{i=1}^N \\xi_i^{\\alpha,\\beta,\\gamma} x_i^1(t), $$ where $-1 \\lt m^{\\alpha,\\beta,\\gamma}(t) \\lt 1$ represents the retrieval accuracy of the memory pattern $\\xi^{\\alpha,\\beta,\\gamma}$.\nHierarchical Information Retrieval With push-pull Feedback Push Feedback Pull Feedback ","date":"July 19, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/paper-reading/push-pull_feedback/","summary":"Liu, Xiao, et al. \u0026ldquo;Push-pull feedback implements hierarchical information retrieval efficiently.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019): 5701-5710.\n Front Word To understand this paper, you need a strong neuroscience background, especially knowing the Hopfield model and Hebbian theory. So before reading this paper, please preview the theories mentioned above!\nTo be honest, I still cannot understand the details quite well LoL :)\nAbstract  In addition to feedforward connections, there exist abundant feedback connections in a neural pathway.","tags":null,"title":"[BIC] Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently"},{"categories":null,"contents":"I. Self-attention Overview  Input: A sequence of vectors (size not fixed) Output:  Each vector has a label (POS tagging) The whole sequence has a label (sentiment analysis) Model decides the number of labels itself (seq2seq)    Self-attention can handle global iformation, and FC can handle local information. Self-attention is the key module of Transformer, which will be shared in other articles.\nII. How to work? Firstly, we should figure out the relevance between each vector. Dot-product or Additive method is used to calculate relevant coefficient $\\alpha$, also called attention score. Then every vector $v$ multiply the attention score and sum them up, we could have: $$ b^1 = \\sum_{i} \\alpha_{1,i}^{'} v^i $$ 从矩阵运算的视角可以再次去理解self-attention得到结果的过程：  这里的softmax可以换成其他激活函数\n 通过上述的运算可知，在一层self-attention layer当中，我们只需要学习$W^q, W^k, W^v$。\nIII. Multi-head Self-attention 与上述不同的是，需要在得到的$q, k, v$进一步乘以参数矩阵的得到更多的$q, k, v$，只有对应的$q, k, v$才可以进行dot-product和weighted-sum操作。 多少个head就会得到多少个vector $b$，接下来我们通过矩阵乘法得到最后的$b$传给下一层。 $$ b^i = W^o b^i $$\n思考一下，可以发现No position information in self-attention，位置信息在这个机制中没有被考虑，所以为了加入位置信息，我们需要进行Positional Encoding。\nPositional Encoding  Each position has a unique positional vector $e^i$ In 《Attention is all you need》, vector $e$ is hand-crafted BUT, it can also learn from data  Truncated self-attention 只看一部分范围，不和所有的$\\alpha$进行运算。\nIV. Self-attention vs CNN CNN: self-attention that can only attends in a receptive field. （简化版的self-attention） Self-attention: CNN with learnable receptive field. (complex version of CNN)\n 《On the relationship between Self-Attention and Convolutional Layers》\n V. Self-attention vs RNN RNN: 两个相距较远的word 必须经过多个遗忘门，这种情况大大减少了两者之间的相关性。并且每一个hidden output是nonparallel输出的，有先后顺序。 Self-atttention：每一个word之间都会计算，所以位置因素不会影响。可以平行输出所有的output，训练效率更高。\n 《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》\n VI. Summary 现如今，广义的transformer就可以理解为Self-Attention，它现在有很多的变种，都会以*former来表示，而如何设计出good performance good efficiency的self-attention模型是现在的热点话题。\n 《Efficient Transformers: A Survey》\n ","date":"June 11, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/self-attention/","summary":"I. Self-attention Overview  Input: A sequence of vectors (size not fixed) Output:  Each vector has a label (POS tagging) The whole sequence has a label (sentiment analysis) Model decides the number of labels itself (seq2seq)    Self-attention can handle global iformation, and FC can handle local information. Self-attention is the key module of Transformer, which will be shared in other articles.\nII. How to work? Firstly, we should figure out the relevance between each vector.","tags":null,"title":"Self-attention"},{"categories":null,"contents":"I. Transformer Overview   Seq2seq model\n  Input: sequence of vector\n  Output: sequence (not fixed size)\n  II. Encoder It is actually a Self-Attention Model!\n对于一个block，它的结构可以理解为以下的形式：\n 与self-attention不同的是，会采用residual add的方式，将self-attention得到的中间结果$a$加上输出$b$ 经过layer normalization得到新的输出 将新的output输入到FC中，并加上residual 再次经过layer normalization得到这一个block的结果。  理解了一个block以后，整个Encoder就是由n个这种block组成的network。\n 首先将word进行self-attention得到word embeddding，并在其中加入positional encoding信息 经过multi-head Attention或者Feed Forward Network后，都要接上residual + layer normalization，而这种设计结构就是Transformer Encoder的创新点所在。  III. Decoder —— Autoregressive(AT) The model structure of Decoder is shown as follow:\n根据上图我们将逐步逐层地解释结构：\n Masked Multi-head Self-attention 在产生每一个$b^i$ vector时，不能再看后面的信息，只能和前面的输入进行关系：  从细节上来看，要想得出$b^2$，我们只需将其和$k^1, k^2$做dot-product。\n这么做的原因在于对于Decoder，它每一次的输入是来自上一次的输出，所以模型不存在右边输入，意味者self-atttention只能去考虑当前计算输出左边的输入做关联。\n而对于Decoder而言，需要模型自己判断什么时候结束输出，得到not fixed sequence。我们需要对于输出的vector中添加新的一个维度[END]，表示输出的结束，然后将这个输出的vector经过softmax得到概率最大的字符，若是END则输出结束。\nIV. Decoder —— Non Autoregressive(NAT) How to decide the output length for NAT decoder?\n Another predictor for output length  Classifier: Encoder input $\\rightarrow$ output length   Output a veryh long sequence, ignore tokens after END  Advantage: parallel, controllable output length.\nNAT: usually worse than AT\n reason: multi-modality  Encoder-Decoder 两者的连接最重要的就是Cross Attention，其工作原理如下所示：\n Encoder：提供$k, v$，两个输入 Decoder：提供$q$，一个输入  其实Decoder的每一层cross attention不一定要看encoder的最后一层输出，也可以看中间层的输出。对于cross attention的连接方式是各式各样的，可以作为一个sutdy进行研究。\nTraining  Ground Truth: one-hot encoding. Prediction: Distribution of probability. We will minimize cross entropy between ground truth and prediction.  Teacher Forcing using the ground truth as input. 在Decoder训练的时候，我们在输入的时给它正确的答案。\nExposure Bias  起因：由于Decoder会将自己的输出作为下一次的输入，所以如果一步错，就会步步错，Decoder不断得到错误的输入并训练出错误的输出。 解决方法：Scheduled Sampling  Testing 在Testing中是对整体的输出和Ground Truth句子之间计算BLEU score。但之所以不在training phase去将BLEU score作为误差是因为其不可微分，无法使用gradient descent。\n 这里李老师讲了一个口诀，如果遇到optimization无法解决的问题，使用RL硬train一发就对了hhhh，把无法微分的loss function当作是RL的Reward，把Decoder当作是Agent、\n Tips Copy Mechanism 对于一些专有词汇，如果像Decoder一样根据上文预测下一字会效果比较差，一般而言使用copy mechanism可以将专有词汇进行复制，在一些words后面直接进行输出，这样效果会好很多。\nGuided Attention 对于语音识别，Attention scores的峰值应该从左向右，为了使得模型训练没有偏差，可以直接将该模式放入模型的训练当中\n Monotonic Attention Location-aware attention\n Beam Search 从解空间树中找到一个approximate solution，使得结果近似optimal。但这种启发式搜索有时有用，有时不管用。对于一些结果特定的任务，Beam Search会很有帮助，但对于需要有创造力的任务，比如续写后文，Beam Search就没有太大用处。\n","date":"June 11, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/transformer/","summary":"I. Transformer Overview   Seq2seq model\n  Input: sequence of vector\n  Output: sequence (not fixed size)\n  II. Encoder It is actually a Self-Attention Model!\n对于一个block，它的结构可以理解为以下的形式：\n 与self-attention不同的是，会采用residual add的方式，将self-attention得到的中间结果$a$加上输出$b$ 经过layer normalization得到新的输出 将新的output输入到FC中，并加上residual 再次经过layer normalization得到这一个block的结果。  理解了一个block以后，整个Encoder就是由n个这种block组成的network。\n 首先将word进行self-attention得到word embeddding，并在其中加入positional encoding信息 经过multi-head Attention或者Feed Forward Network后，都要接上residual + layer normalization，而这种设计结构就是Transformer Encoder的创新点所在。  III. Decoder —— Autoregressive(AT) The model structure of Decoder is shown as follow:\n根据上图我们将逐步逐层地解释结构：\n Masked Multi-head Self-attention 在产生每一个$b^i$ vector时，不能再看后面的信息，只能和前面的输入进行关系：  从细节上来看，要想得出$b^2$，我们只需将其和$k^1, k^2$做dot-product。","tags":null,"title":"Transformer"},{"categories":null,"contents":"原理 K近邻算法简单、直观，大致步骤为：\n 输入：训练集 $$T = {(x_1,y_1), (x_2, y_2), \\dots, (x_N, y_N)}$$ 输出：  根据给定的距离度量，在训练集T中找出与$x$最近邻的$k$个点，涵盖这$k$个点的邻域记作$N_K(x)$； 找出最近邻的$k$个点的一个方法是搜索$kd$树； 在$N_k(x)$中根据分类决策规则（多数表决）决定$x$的类别$y$: $$y = arg \\max_{c_j} \\sum_{x_i \\in N_k(x)}I(y_i=c_j)$$    kd树 ","date":"April 21, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/machine-learning/knn/","summary":"原理 K近邻算法简单、直观，大致步骤为：\n 输入：训练集 $$T = {(x_1,y_1), (x_2, y_2), \\dots, (x_N, y_N)}$$ 输出：  根据给定的距离度量，在训练集T中找出与$x$最近邻的$k$个点，涵盖这$k$个点的邻域记作$N_K(x)$； 找出最近邻的$k$个点的一个方法是搜索$kd$树； 在$N_k(x)$中根据分类决策规则（多数表决）决定$x$的类别$y$: $$y = arg \\max_{c_j} \\sum_{x_i \\in N_k(x)}I(y_i=c_j)$$    kd树 ","tags":null,"title":"KNN"},{"categories":null,"contents":"I. A mathematical model of a neuron   Equivalent circuit model\n  Parts of the circuit:\n Power supplies: multiple to power different parts of the circuit to do different things Integartor(积分器) of past inputs Temporal filter to smooth inputs in time Spike generator: the sodium current(钠通路) and potassium current(钾通路) make a spike generator that generates an action potential and then talks to another neurons. Oscillator(振荡器)    The wires of brain: The intracellular and extracelluar sapce is filled with salt solution.(细胞内液和细胞外溶液的电流)\n  ","date":"April 16, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/neural-computation/1-ionic_currents/","summary":"I. A mathematical model of a neuron   Equivalent circuit model\n  Parts of the circuit:\n Power supplies: multiple to power different parts of the circuit to do different things Integartor(积分器) of past inputs Temporal filter to smooth inputs in time Spike generator: the sodium current(钠通路) and potassium current(钾通路) make a spike generator that generates an action potential and then talks to another neurons. Oscillator(振荡器)    The wires of brain: The intracellular and extracelluar sapce is filled with salt solution.","tags":null,"title":"Ionic Currents"},{"categories":null,"contents":"支持向量机模型  学习策略：间隔最大化，可形式化为一个求解凸二次规划(convex quadratic programming)的最优化问题  一、线性可分支持向量机与硬间隔最大化  支持向量机的学习是在特征空间上进行的，需要从输入空间转换到特征空间上。\n   线性可分支持向量机：给定线性可分的训练集，通过间隔最大化或等价地求解相应的凸二次规划问题得到的分离超平面为： $$\\omega^* x + b^* = 0$$ 以及相应的分类决策函数： $$f(x) = sign(\\omega^* x + b^*)$$\n  函数间隔和几何间隔\n 函数间隔：对于给定的训练数据集$T$和超平面$(\\omega, b)$，则 $$\\hat{\\gamma_i}=y_i(\\omega \\cdot x_i+b)$$\n超平面关于数据集$T$的函数间隔为 $$\\hat{\\gamma}=\\min_{i=1,\\dots,N}\\hat{\\gamma_i}$$ 几何间隔：对于给定的训练数据集$T$和超平面$(\\omega, b)$，则 $$\\hat{\\gamma_i}=y_i(\\frac{\\omega}{||\\omega||} \\cdot x_i+\\frac{b}{||\\omega||})$$\n其中$||\\omega||$是L2范数。    间隔最大化\n 输入：线性可分数据集$T$ 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： $$\\min_{\\omega,b} \\quad \\frac{1}{2}||\\omega||^2 $$ $$ s.t. \\qquad y_i(\\omega \\cdot x_i+b)-1 \\geq 0, \\quad i=1, 2, \\dots, N$$ 求解得到最优解$\\omega^* , b^*$。    支持向量\n 训练集的样本点中与分离超平面距离最近的样本点的实例： $$H_1:\\omega \\cdot x_i+b=1 $$ $$H_2:\\omega \\cdot x_i+b=-1$$  支持向量机由这些很少的“重要”训练样本决定。\n  求解方法：应用拉格朗日对偶法，通过求解对偶问题得到原始问题的最优解。\n 输入：线性可分训练集$T$； 输出：分离超平面和分类决策函数   构造并求解约束最优化问题： $$\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)-\\sum_{i=1}^{N}\\alpha_i $$ $$ s.t. \\qquad \\sum_{i=1}^{N} \\alpha_iy_i=0 $$ $$ \\alpha_i \\geq 0, i = 1,2,\\dots,N $$ 并求出最优解$\\alpha^*$。 计算 $$\\omega^* = \\sum_{i=1}^N\\alpha_i^*y_ix_i$$ 并选择$\\alpha^*$的其中一个正分量$\\alpha^*\\gt 0$，计算 $$b^*=y_i- \\sum_{i=1}^N\\alpha_i^*y_i(x_i\\cdot x_j)$$ 求得分离超平面 $$\\omega^* \\cdot x+ b^* =0$$ 分类决策函数： $$f(x)=sign(\\omega^* \\cdot x+b^* )$$    二、线性支持向量机与软间隔最大化   线性支持向量机：给定线性不可分的训练集，通过软间隔最大化或等价地求解相应的凸二次规划问题得到的分离超平面为： $$\\omega^* x + b^* = 0$$ 以及相应的分类决策函数： $$f(x) = sign(\\omega^* x + b^*)$$\n  软间隔最大化 输入：线性可分数据集$T$ 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： $$\\min_{\\omega,b} \\quad \\frac{1}{2}||\\omega||^2 + C\\sum_{i=1}^N\\xi_i$$ $$ s.t. \\qquad y_i(\\omega \\cdot x_i+b)\\geq 1-\\xi_i, \\quad i=1, 2, \\dots, N$$ $$ \\xi_i \\ge 0,\\quad i=1, 2, \\dots, N $$ 求解得到最优解$\\omega^*, b^*$。\n  求解方法\n 输入：线性可分训练集$T$； 输出：分离超平面和分类决策函数   构造并求解约束最优化问题： $$\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)-\\sum_{i=1}^{N}\\alpha_i $$ $$ s.t. \\qquad \\sum_{i=1}^{N} \\alpha_iy_i=0 $$ $$ 0 \\leq \\alpha_i \\leq C, i = 1,2,\\dots,N $$ 并求出最优解$\\alpha^*$。 计算 $$\\omega^* = \\sum_{i=1}^N\\alpha_i^*y_ix_i$$ 并选择$\\alpha^*$的一个适合条件$0\\lt\\alpha^*\\lt C$，计算 $$b^*=y_i- \\sum_{i=1}^N\\alpha_i^*y_i(x_i\\cdot x_j)$$ 求得分离超平面 $$\\omega^* \\cdot x+b^* =0$$ 分类决策函数： $$f(x)=sign(\\omega^* \\cdot x+b^*)$$  上述过程需要满足KKT(Karush-Kuhn-Tucker)条件，即： 对于任意样本$(x_i, y_i)$，总有$\\alpha_i=0$ 或 $y_if(x_i)-1=0$。当$\\alpha_i \\gt 0$时，则必有$y_if(x_i)=1$，则所有的样本点位于最大间隔边界，是一个支持向量。所以最终模型只和支持向量有关。\n  合并损失函数(linge loss function) 线性支持向量机的另一种最优化问题是，最小化以下函数： $$\\sum_{i=1}^{N}[1-y_i(\\omega \\cdot x_i + b)]_++\\lambda||\\omega||^2$$\n其中第一项为合页损失函数。\n  三、非线性支持向量机与核函数   核技巧\n基本想法：通过一个非线性变换将输入空间（欧式空间$\\mathcal{R}^n$或离散集合）对应于一个特征空间（希尔伯特空间$\\mathcal{H}$），使得在输入空间$\\mathcal{R}^n$中的超曲面模型对应特征空间$\\mathcal{H}$中的超平面模型。\n 核函数的定义：     在支持向量机中的应用：\n在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。\n  正定核\n这部分需要泛函分析的知识，目前来看可能需要研究生再看懂了。 对称函数$K(x,z)$为正定核的充要条件如下： 对任意$x_i \\in \\mathcal{X}, i=1,2,\\dots,m$，任意正整数$m$，对称函数$K(x,z)$对应的Gram矩阵是半正定的。\n  常用核函数\n 多项式核函数(polynomial kernel function) $$K(x,z)=(x\\cdot z+1)^p$$ 高斯核函数(Gaussian kernel function) $$K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$$ 字符串核函数(string kernel function) 用于文本分类、信息检索、生物信息学等方面。直观上，两个字符串相同的子串越多，它们就越相似、字符串核函数的值就越大。    求解方法\n 输入：训练集$T$； 输出：分离超平面和分类决策函数   构造并求解约束最优化问题： $$\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_i \\alpha_j y_i y_j K(x_i \\cdot x_j)-\\sum_{i=1}^{N}\\alpha_i $$ $$ s.t. \\qquad \\sum_{i=1}^{N} \\alpha_iy_i=0 $$ $$ 0 \\leq \\alpha_i \\leq C, i = 1,2,\\dots,N $$ 并求出最优解$\\alpha^*$。 计算 $$\\omega^* = \\sum_{i=1}^N\\alpha_i^*y_ix_i$$ 并选择$\\alpha^*$的一个适合条件$0\\lt\\alpha^*\\lt C$，计算 $$b^*=y_i- \\sum_{i=1}^N\\alpha_i^*y_iK(x_i\\cdot x_j)$$ 求得分离超平面 $$\\omega^* \\cdot x+b^*=0$$ 分类决策函数： $$f(x)=sign\\left( \\sum_{i=1}^N\\alpha_i^* y_iK(x\\cdot x_i+b^*)\\right)$$    四、序列最小最优算法SMO 算法描述如下：\n  输入：训练集$T$\n  输出：近似解$\\hat{\\alpha}$\n    取初值$\\alpha^{(0)}=0$，令$k=0$。\n  选取优化变量$\\alpha_1^{(k)}, \\alpha_2^{(k)}$，解析求解两个变量的最优化问题：   求得最优解$\\alpha_1^{(k+1)}, \\alpha_2^{(k+1)}$，更新$\\alpha$为$\\alpha^{(k+1)}$。\n若在精度$\\epsilon$范围内满足停机条件:   其中， $$g(x_i)=\\sum_{j-1}^N\\alpha_jy_jK(x_j,x_i)+b$$ 则转(4)；否则令$k=k+1$，转(2)；\n取$\\hat{\\alpha}=\\alpha^{(k+1)}$。  ","date":"March 29, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/machine-learning/support-vector/","summary":"支持向量机模型  学习策略：间隔最大化，可形式化为一个求解凸二次规划(convex quadratic programming)的最优化问题  一、线性可分支持向量机与硬间隔最大化  支持向量机的学习是在特征空间上进行的，需要从输入空间转换到特征空间上。\n   线性可分支持向量机：给定线性可分的训练集，通过间隔最大化或等价地求解相应的凸二次规划问题得到的分离超平面为： $$\\omega^* x + b^* = 0$$ 以及相应的分类决策函数： $$f(x) = sign(\\omega^* x + b^*)$$\n  函数间隔和几何间隔\n 函数间隔：对于给定的训练数据集$T$和超平面$(\\omega, b)$，则 $$\\hat{\\gamma_i}=y_i(\\omega \\cdot x_i+b)$$\n超平面关于数据集$T$的函数间隔为 $$\\hat{\\gamma}=\\min_{i=1,\\dots,N}\\hat{\\gamma_i}$$ 几何间隔：对于给定的训练数据集$T$和超平面$(\\omega, b)$，则 $$\\hat{\\gamma_i}=y_i(\\frac{\\omega}{||\\omega||} \\cdot x_i+\\frac{b}{||\\omega||})$$\n其中$||\\omega||$是L2范数。    间隔最大化\n 输入：线性可分数据集$T$ 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： $$\\min_{\\omega,b} \\quad \\frac{1}{2}||\\omega||^2 $$ $$ s.t. \\qquad y_i(\\omega \\cdot x_i+b)-1 \\geq 0, \\quad i=1, 2, \\dots, N$$ 求解得到最优解$\\omega^* , b^*$。    支持向量","tags":null,"title":"Support Vector Machines"},{"categories":null,"contents":"原理 决策树模型与学习 决策树的内部节点表示一个特征或属性，叶节点表示一个类。\n if-then规则：互斥且完备 本质： 从训练数据集中归纳出一组分类规则（估计出条件概率模型） 损失函数：正则化的极大似然函数 学习算法：启发式方法，得到(sub-optimal)的决策树  一、特征选择 准则：信息增益\n  熵： 设$X$是一个取有限个值的离散随机变量，定义为： $$H(X) = -\\sum_{i=1}^n p_i\\log p_i$$\n  条件熵： 随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为： $$H(Y|X) = \\sum_{i=1}^np_iH(Y|X=x_i)$$ 当熵和条件熵中的概率由极大似然估计得到时，分别成为经验熵和经验条件熵\n  信息增益：表示得知特征$X$的信息使得类$Y$的信息不确定性减少的程度。这种差值也称为互信息 $$g(D,A)=H(D)-H(D|A)$$\n  特征选择方法：对训练数据集D，计算每一个特征的信息增益，选取信息增益最大的特征。\n信息增益比 上述的特征选择存在偏向选择取值较多的特征的问题，使用information gain ratio可以解决这个问题。 定义为： $$g_R(D, A) = \\frac{g(D,A)}{H_A(D)}$$ 其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{D}\\log_2\\frac{|D_i|}{D} $，n是特征A的取值个数。  二、决策树的生成  ID3算法 C4.5算法的基础般，只用信息增益来选取特征。 C4.5算法   三、决策树的剪枝 决策树的生成很容易出现过拟合现象，所以需要利用剪枝(pruning)来简化决策树。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现： $$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_tH_t(T) + \\alpha|T|=C(T)+\\alpha|T|$$ 其中$|T|$表示树T的叶结点个数，t是树T的叶结点，该叶结点上有$N_t$个样本点，$\\alpha \\geq 0$为参数。\n通过式子可以看出$C(T)$代表了模型对训练数据的预测误差，即拟合程度；而$\\alpha|T|$代表了模型的复杂度，可以理解为正则化的方式来增强模型的泛化能力。\n而树的剪枝算法分为：\n 预剪枝：不足是基于贪心策略，带来欠拟合的风险 后剪枝  后剪枝与动态规划类似，生成一棵完整的决策树以后，自底向上地对非叶节点进行考察，若剪完后损失函数变小，则进行剪枝。\n四、CART决策树   回归树的生成   分类树的生成\n 基尼系数：假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则 $$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)$$ 若样本集合$D$根据特征$A$是否取值$\\alpha$被分割为$D_1$和$D_2$两部分，则在特征$A$下，集合$D$的基尼指数为： $$Gini(D, A)=\\frac{|D_1|}{D}Gini(D_1)+\\frac{|D_2|}{D}Gini(D_2)$$     剪枝 后剪枝法，从生成的决策树$T_0$开始不断剪枝，一直到根节点，形成一个子树序列${T_0, T_1, \\dots, T_n}$，然后通过交叉验证法在独立的验证数据集上对子树序列进行测试。\n  ","date":"March 22, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/machine-learning/decision-tree/","summary":"原理 决策树模型与学习 决策树的内部节点表示一个特征或属性，叶节点表示一个类。\n if-then规则：互斥且完备 本质： 从训练数据集中归纳出一组分类规则（估计出条件概率模型） 损失函数：正则化的极大似然函数 学习算法：启发式方法，得到(sub-optimal)的决策树  一、特征选择 准则：信息增益\n  熵： 设$X$是一个取有限个值的离散随机变量，定义为： $$H(X) = -\\sum_{i=1}^n p_i\\log p_i$$\n  条件熵： 随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为： $$H(Y|X) = \\sum_{i=1}^np_iH(Y|X=x_i)$$ 当熵和条件熵中的概率由极大似然估计得到时，分别成为经验熵和经验条件熵\n  信息增益：表示得知特征$X$的信息使得类$Y$的信息不确定性减少的程度。这种差值也称为互信息 $$g(D,A)=H(D)-H(D|A)$$\n  特征选择方法：对训练数据集D，计算每一个特征的信息增益，选取信息增益最大的特征。\n信息增益比 上述的特征选择存在偏向选择取值较多的特征的问题，使用information gain ratio可以解决这个问题。 定义为： $$g_R(D, A) = \\frac{g(D,A)}{H_A(D)}$$ 其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{D}\\log_2\\frac{|D_i|}{D} $，n是特征A的取值个数。  二、决策树的生成  ID3算法 C4.5算法的基础般，只用信息增益来选取特征。 C4.5算法   三、决策树的剪枝 决策树的生成很容易出现过拟合现象，所以需要利用剪枝(pruning)来简化决策树。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现： $$C_{\\alpha}(T)=\\sum_{i=1}^{|T|}N_tH_t(T) + \\alpha|T|=C(T)+\\alpha|T|$$ 其中$|T|$表示树T的叶结点个数，t是树T的叶结点，该叶结点上有$N_t$个样本点，$\\alpha \\geq 0$为参数。\n通过式子可以看出$C(T)$代表了模型对训练数据的预测误差，即拟合程度；而$\\alpha|T|$代表了模型的复杂度，可以理解为正则化的方式来增强模型的泛化能力。\n而树的剪枝算法分为：\n 预剪枝：不足是基于贪心策略，带来欠拟合的风险 后剪枝  后剪枝与动态规划类似，生成一棵完整的决策树以后，自底向上地对非叶节点进行考察，若剪完后损失函数变小，则进行剪枝。","tags":null,"title":"Decision Tree"},{"categories":null,"contents":"统计学习概念辨析 一、基本分类 1. 监督学习 监督学习的本质是学习输入到输出的映射的统计规律。需要注意的有以下要点：\n 输入空间与特征空间不一定为相同的空间，有时会将实例从输入空间映射到特征空间 训练数据由输入(特征向量)与输出对组成 任务问题分类：  回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题   $X$和$Y$具有联合概率分布就是监督学习关于数据的基本假设，即假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的 假设空间的确定意味着学习范围的确定  2. 无监督学习 无监督学习的本质是学习数据中的统计规律或潜在结构，需要注意的有以下要点：\n 可以用于对已有数据的分析，也可以用于对未来数据的预测 要学习的模型可以表示为$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式  3. 强化学习 强化学习的本质是学习最优的序贯决策。在学习过程中，系统不断地试错，以达到学习最优策略的目的。\n强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$\u0026lt;S,A,P,r,\\gamma\u0026gt;$组成：\n $S$是state集合 $A$是action集合 $P$是状态转移概率(transition probability)函数： $$P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$$ $r$是奖励函数(reward function): $$r(s,a)=E(r_{t+1}|s_t=s, a_t=a)$$ $\\gamma$是衰减系数(discount factor): $$\\gamma \\in [0,1]$$  马尔可夫决策过程具有马尔科夫性，下一个状态只依赖于前一个状态与动作，由状态转移概率函数$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，由奖励函数$r(s,a)$表示。\n 策略$\\pi$：给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$ 价值函数/状态价值函数：策略$\\pi$从某一个状态$s$开始的长期累积奖励的数学期望： $$v_{\\pi}(s)=E_{\\pi}[r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+\\dots|s_t=s]$$ 动作价值函数：策略$\\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望： $$q_{\\pi}(s,a)=E_{\\pi}[r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+\\dots|s_t=s, a_t=a]$$  强化学习的目标就是在所有可能的策略中选出价值函数最大的策略$\\pi^*$。\n强化学习的分类：\n policy-based  不直接学习模型，试图求解最优策略$\\pi^*$。学习通常从一个具体策略开始，通过搜索更优的策略进行。   value-based  试图求解最有价值函数($q^*(s,a)$)。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。   model-based  直接学习马尔科夫决策过程的模型，通过模型对环境的反馈进行预测。    4. 半监督学习 利用标注数据和未标注数据学习预测模型的机器学习问题。这种学习旨在利用未标注数据中的信息，辅助标注数据，进行监督学习。\n5. 主动学习 目标是找出对学习最有帮助的实例让教师标注，以较小的标注代价，达到更好的学习效果。\n6. 按模型分类 概率模型：生成模型\n 决策树 朴素贝叶斯 隐马尔可夫模型 条件随机场 概率潜在语义分析 潜在狄利克雷分配 高斯混合模型 逻辑斯蒂回归  非概率模型：判别模型\n 感知机 支持向量机 k近邻 AdaBoost k均值 潜在语义分析 神经网络 逻辑斯蒂回归  线性模型：\n 感知机 线性支持向量机 k近邻 k均值 潜在语义分析  非线性模型：\n 核函数支持向量机 AdaBoost 神经网络   条件概率分布最大化后得到函数，函数归一化得到条件概率分布。\n 7. 按学习分类  在线学习：每一次接受一个样本，进行预测，然后学习模型，并不断重复该操作 批量学习：一次接受所有数据，学习模型，之后进行预测  二、模型策略与选择 对于监督学习而言：\n 经济风险最小化(ERM) 会出现过拟合的情况 结构风险最小化(SRM) 等价于正则化  1. 过拟合 定义：学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测很好，但对未知数据预测很差的现象。\n2. 正则化  $L_1$范数 $$L(w)=\\frac{1}{N}\\sum_{i=1}^N(f(x_i;w)-y_i)^2+\\lambda||w||$$ $L_2$范数 $$L(w)=\\frac{1}{N}\\sum_{i=1}^N(f(x_i;w)-y_i)^2+\\frac{\\lambda}{2}||w||^2$$  3. 交叉验证  训练集：训练模型 验证集：选择模型 测试集：评估模型  交叉验证的基本思想：重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择。\n 简单交叉验证 随机分割数据集，并选出测试误差最小的模型 S折交叉验证(S-fold cross validation) 随机将已给数据切分为S个互不相交、大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型。 留一交叉验证 上述方法的特殊情况，S=N，这里N代表数据集的容量，不常用的方法。  4. 泛化能力  泛化误差的定义：所学习的模型的期望风险 泛化误差上界  三、生成模型与判别模型   生成模型 由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型。 特点：\n 可以还原出联合概率分布 学习收敛速度更快 当存在隐变量时，仍可以使用生产学习    判别模型 由数据直接学习决策函数或者条件概率分布作为预测的模型。 特点：\n 学习的准确率更高 可以对数据进行各种程度上的抽象、定义特征并使用特征，所以可以简化学习。    四、模型评价指标 监督学习中的分类问题与标注问题：\n 准确率(accuracy)：分类器正确分类的样本数与总样本数之比 精确率(percision)：针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是 $$P=\\frac{TP}{TP+FP}$$ 召回率(recall)：针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)，也就是 $$R=\\frac{TP}{TP+FN}$$ F1：精确率和召回率的调和均值，也就是 $$\\frac{2}{F1}=\\frac{1}{P}+\\frac{1}{R}$$  ","date":"January 27, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/machine-learning/basic-concepts/","summary":"统计学习概念辨析 一、基本分类 1. 监督学习 监督学习的本质是学习输入到输出的映射的统计规律。需要注意的有以下要点：\n 输入空间与特征空间不一定为相同的空间，有时会将实例从输入空间映射到特征空间 训练数据由输入(特征向量)与输出对组成 任务问题分类：  回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题   $X$和$Y$具有联合概率分布就是监督学习关于数据的基本假设，即假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的 假设空间的确定意味着学习范围的确定  2. 无监督学习 无监督学习的本质是学习数据中的统计规律或潜在结构，需要注意的有以下要点：\n 可以用于对已有数据的分析，也可以用于对未来数据的预测 要学习的模型可以表示为$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式  3. 强化学习 强化学习的本质是学习最优的序贯决策。在学习过程中，系统不断地试错，以达到学习最优策略的目的。\n强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$\u0026lt;S,A,P,r,\\gamma\u0026gt;$组成：\n $S$是state集合 $A$是action集合 $P$是状态转移概率(transition probability)函数： $$P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$$ $r$是奖励函数(reward function): $$r(s,a)=E(r_{t+1}|s_t=s, a_t=a)$$ $\\gamma$是衰减系数(discount factor): $$\\gamma \\in [0,1]$$  马尔可夫决策过程具有马尔科夫性，下一个状态只依赖于前一个状态与动作，由状态转移概率函数$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，由奖励函数$r(s,a)$表示。\n 策略$\\pi$：给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$ 价值函数/状态价值函数：策略$\\pi$从某一个状态$s$开始的长期累积奖励的数学期望： $$v_{\\pi}(s)=E_{\\pi}[r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+\\dots|s_t=s]$$ 动作价值函数：策略$\\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望： $$q_{\\pi}(s,a)=E_{\\pi}[r_{t+1}+\\gamma r_{t+2}+\\gamma^2r_{t+3}+\\dots|s_t=s, a_t=a]$$  强化学习的目标就是在所有可能的策略中选出价值函数最大的策略$\\pi^*$。\n强化学习的分类：\n policy-based  不直接学习模型，试图求解最优策略$\\pi^*$。学习通常从一个具体策略开始，通过搜索更优的策略进行。   value-based  试图求解最有价值函数($q^*(s,a)$)。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。   model-based  直接学习马尔科夫决策过程的模型，通过模型对环境的反馈进行预测。    4.","tags":null,"title":"Basic Concepts"},{"categories":null,"contents":"原理  参考：统计学习方法|感知机原理剖析及实现\n  输入：实例的特征向量 输出：实例的类别（二分类） 模型类别：判别模型 学习策略：基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型  ","date":"January 27, 2021","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/machine-learning/perceptron/","summary":"原理  参考：统计学习方法|感知机原理剖析及实现\n  输入：实例的特征向量 输出：实例的类别（二分类） 模型类别：判别模型 学习策略：基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型  ","tags":null,"title":"Perceptron"},{"categories":null,"contents":"Chapter 12 Query Processing 12.1 Overview 12.2 Measures of Query Cost Disk access is the predominant cost. Use the number of block transfered from disk and the number of seeks as the cost measures.\n12.3 Selection Operation  Types of query conditions (查询条件类型)  equality(等值), e.g.salary = 100 range (范围), e.g. salary between 50 and 400 comparison (比较), e.g. salary \u0026gt;300     NOTE: B+索引可以使用全部查询类型，而Hash索引只能等值查询。\n Several file scan algorithms  linear search/scan – A1 无索引、乱序：扫描全部blocks，才能找到全部满足查询条件的数据。 Cost estimate = $b_r$ block transfers + 1 seek selections using indices – A2, A3, A4  A2 主索引：Cost = $(h_i + 1) \\times (t_T + t_S)$ A3 聚集索引不唯一：Cost = $h_i \\times (t_T + t_S) + t_S + t_T\\times b$ A4 非聚集索引不唯一：Cost = $(h_i + n) \\times (t_T + t_S)$   selections involving comparisons – A5, A6  A5: 主索引，找叶子节点 A6: 辅索引，找叶子节点所指向的记录   complex selections – A7, A8, A9, A10    12.4 Sorting merge sort，排序、去重\n12.5 Join Operation nested-loop join, merge join, hash join\n12.6 Other Operations project, distinct（去重）, order by, outer join, aggregation\n12.7 Evaluation of Expressions  Materialization serial evaluating   start from the lowest-level, i.e. at the bottom of the tree, evaluate one operation at a time. the results of each evaluation (i.e. intermediate computing results) are stored in temporal relations on the disk for subsequent evaluation .  Pipelined parallel evaluating evaluate several operations simultaneously in a pipeline, with the results of one operation passed to the next, without the need to store temporary relations in disk  ","date":"December 14, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/database-system/database_system_4/","summary":"Chapter 12 Query Processing 12.1 Overview 12.2 Measures of Query Cost Disk access is the predominant cost. Use the number of block transfered from disk and the number of seeks as the cost measures.\n12.3 Selection Operation  Types of query conditions (查询条件类型)  equality(等值), e.g.salary = 100 range (范围), e.g. salary between 50 and 400 comparison (比较), e.g. salary \u0026gt;300     NOTE: B+索引可以使用全部查询类型，而Hash索引只能等值查询。\n Several file scan algorithms  linear search/scan – A1 无索引、乱序：扫描全部blocks，才能找到全部满足查询条件的数据。 Cost estimate = $b_r$ block transfers + 1 seek selections using indices – A2, A3, A4  A2 主索引：Cost = $(h_i + 1) \\times (t_T + t_S)$ A3 聚集索引不唯一：Cost = $h_i \\times (t_T + t_S) + t_S + t_T\\times b$ A4 非聚集索引不唯一：Cost = $(h_i + n) \\times (t_T + t_S)$   selections involving comparisons – A5, A6  A5: 主索引，找叶子节点 A6: 辅索引，找叶子节点所指向的记录   complex selections – A7, A8, A9, A10    12.","tags":null,"title":"Database System Lecture Note 4"},{"categories":null,"contents":"Chapter 10 File System Chapter 11 File System Inplementation ","date":"December 10, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_6/","summary":"Chapter 10 File System Chapter 11 File System Inplementation ","tags":null,"title":"File System"},{"categories":null,"contents":"Chapter 12 Mass-Storage Structure  hard disk drives(HDDs) and nonvolatile memory (NVM)\n 12.1-12.2 Disk Structure Mapping between disk and blocks 并行快速读取多个连续的blocks/sectors\n  Transfer rate: the rate at which data flow between the driver and the computer\n  The position time:\n seek time: the time for the disk to move the disk arm to the cylinder containing the desired sector.平均为移动1/2*(总磁道数)所需的时间 rotational latency: the time for the desired sector to rotate to the disk head. 磁盘旋转半圈所需时间(depend on RPM)    外部传输速率\u0026gt;内部传输速率    外部传输速率：硬盘缓存cache（磁盘控制器中的I/O寄存器）与内存间的传输速率； 内部传输速率：磁头在盘片上读写数据速率，取决于Position time  Drive attached to computer via I/O bus, including   IDE (Integrated Drive Electronics); ATA SATA (Serial ATA ), 适用于PC 机、服务器 mSATA SCSI (Small Computer System Interface)，中、高端服务器和高档工作站中 Fibre Channe (光纤通道)，服务器、工作站 USB接口  Disk Controller Interfaces between the computer system and the disk drive hardware. Function:  checksum bad sector remapping    12.4 Disk Scheduling having a fast access time and disk bandwidth\n  FCFS scheduling\n  SSTF scheduling Shortest-seek-time-first\n  SCAN scheduling SCAN是每一次都要到达最外圈/最内圈才转向，转向后继续扫描并读block\n C-SCAN schduling C-SCAN代表当走到最内圈以后迅速移到另一头的最外圈    LOOK LOOK是电梯调度算法，每一次方向都到达request中的最大值，然后立刻转向并继续扫描读block\n C-LOOK代表每一次到达request中的最大值后迅速移到另一头的request最小值开始继续按原方向扫描    Note:\n About preventing starvation in scheduling.  About NVM scheduling Random access.  12.5 Disk Management 12.5.1 Disk formatting (格式化)   physical formatting: Dividing a disk into sectors that the disk controller can read and write. 划分硬盘的磁柱面、建立扇区、选择扇区间隔比\n  Logical formatting making a file system on the partitions.\n   格式化：将分区所有磁道扫描一遍，清除所有扇区内容，移除文件；扫描同时，检查磁盘是否有坏扇区；加载文件系统(exFAT、NTFS、FAT32) 快速格式化：清除FAT表内容，使系统认为盘上没有文件了，并不真正格式化全部硬盘。  To increase efficiency, most file systems group blocks into larger chunks, called clusters(簇)\n12.5.2 Boot Block  bootstrap -\u0026gt; Boot Block -\u0026gt; 内核程序\n The boot block initializes system\n the bootstrap is stored in ROM. bootstrap loader program.  12.5.3 Bad Blocks 12.3 Disk Attachment Disks may be attached via one of two ways\n host attached via an I/O port talking to I/O busses, e.g. SCSI network attached via a network connection e.g. SAN, NAS, DAS  12.3.1 Network-Attached Storage(NAS)  NFS and CIFS are common protocols Implemented via remote procedure calls (RPCs) between host and storage New iSCSI protocol uses IP network to carry the SCSI protocol  12.3.2 Storage Area network(SAN) 12.3.3 Direct Attached Storage(DAS) 12.5 RAID  Def: A data storage method in which data, along with information used for error correction, such as parity bits or Hamming codes, is distributed among two or more hard disk drives in order to improve performance and reliability 特点：  可以存两个副本，用于防止数据丢失，增加可靠性； 将连续相关的数据放在相邻disk中，可以一并读取。 RAID填补了CPU速度快与磁盘设备速度慢之间的间隙    12.6 Swap-Space Management  Swap-space: a part of disk space used as an extension of main memory to implement virtual memory, as a raw partition  12.8 Stable-Storage Implementation Write-ahead log：典型例子数据库的事务故障恢复log文件。 Step:\n Replicate information on more than one nonvolatile storage media with independent failure modes; Update information in a controlled manner to ensure that we can recover the stable data after any failure during data transfer or recovery.  Chapter 13 I/O System including devices, device controllers, and I/O subsystem\n13.1 I/O Hardware  device controller registers：  status register control register data-in register data-out register     reading and writing device registers are conducted by device drivers!!!\n  The processor control the device in 2 ways:\n direct I/O instructions memory-mapped I/O: (用访问内存的方式访问I/O设备)  device-controller registers are mapped into address space of the CPU CPU executes I/O requests using the standard memory instructions to access controller registers      The main control modes for data tranfer between memory/CPU and devices are:\n polling interrupt DMA    13.2 Application I/O Interface Linux：driver放在内核空间中，其他：driver放在用户态中（微内核） 封装I/O system calls\n 应用接口通用性 I/O 软件的设备独立性  I/O设备访问接口与文件读写类似：write(#device, block_address)\n Driver\n 由CPU执行，完成对具体I/O操作的管理和控制功能； 将函数调用read、write与driver中的文件操作函数相匹配。 Device driver向device controller的控制寄存器CSR设置读写命令和参数，controller据此完成具体I/O操作    Block devices and character-stream devices Block: read(), write(), seek(); stream: get(), put();\n  Network devices: socket\n  Blocking and Nonblocking I/O\n 阻塞I/O 非阻塞I/O  多线程 利用系统提供的非阻塞I/O系统调用 异步I/O系统调用：the process runs while I/O executes.      13.3 I/O subsystem I/O subsystem consists of\n kernel I/O subsystem  a component that conducts scheduling, buffering, cacheing, and spooling, error handlinig a general device-driver interface   drivers for specific hardware devices  DST：设备开关表： 对外提供的接口\n13.3.1 Kernel I/O subsystem  Scheduling Buffering: 存储输入/输出数据  To cope with I/O device\u0026rsquo;s speed mismatch with that of CPU memory access. (内部速率与外部速率不匹配) To cope with I/O device transfer size mismatch     used for Block device\n  Caching 目的：做备份，提高访问速度\n  Spooling 类似超线程，同时进行外围操作，使得从逻辑模拟出多台共享设备，实现由串行-\u0026gt;并发，且无需信号量。 经典例子：打印机队列 在一个物理设备上模拟出多个逻辑设备。每一个进程共享虚拟设备的存储区 daemon：调度程序。\n  Error Handling Status register\n  I/O protection 通过系统调用间接使用I/O instruction\n  13.4 Transforming I/O Request to Hardware Operations ","date":"December 10, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_7/","summary":"Chapter 12 Mass-Storage Structure  hard disk drives(HDDs) and nonvolatile memory (NVM)\n 12.1-12.2 Disk Structure Mapping between disk and blocks 并行快速读取多个连续的blocks/sectors\n  Transfer rate: the rate at which data flow between the driver and the computer\n  The position time:\n seek time: the time for the disk to move the disk arm to the cylinder containing the desired sector.平均为移动1/2*(总磁道数)所需的时间 rotational latency: the time for the desired sector to rotate to the disk head.","tags":null,"title":"Mass-Storage Structure \u0026 I/O System"},{"categories":null,"contents":"Chapter 10 Storage and File Structure  对于应用程序和disk而言，中间会有data buffer作为数据交换的缓冲池，替换算法一般采用LRU算法，注意两种可控的参数，在实验中会用到：\n 连接时长 buffer大小   逻辑结构：\n 流式文件、基于记录文件、基于索引文件  物理结构：\n 以block为单位进行存储 contiguous linked indexed  10.5 File Organization Each file is a sequence of records, and a relational table is a set of tuples. A tuple is stored as a record in the DB file.\n10.5.1 Fixed-Length Records   a block contains several records. Store record i starting from byte n * (i – 1), where n is the size of each record. 通过下标迅速定位record。 ==all records are stored in a contiguous space==\n  Free lists ==all records are stored in a noncontiguous space==\n  10.5.2 Variable-Length Records  定长在block的head，变长在block的tail(length、offset等) 通过bitmap记录NULL值 \u0026lt;offset, length\u0026gt; 的定长元组放在前面   Slotted Page  一个slot对应一个或多个record 将变长record存放在page之中 Slotted Page header contains：  number of record entries end of free space in the block location and size of each record        delete and move the pointer to $r_i$ should point to the entry for the record in header 需要更改三个地方  10.6 Organization of Records in Files  Heap 破坏逻辑顺序、但增删改很快。   Typically, there is a single file for a relation 一般以纪录的输入顺序为序，决定了文件中记录顺序 纪录的存储顺序与记录中的主键无关 E.g. 创建新关系表student(ID, name, total-credits)，但不在student上定义主键、候选键、各类索引，student被组织为heap file；通过insert，将记录/元组加到堆文件中。  Sequential 加入索引后，查询很快，增删改变慢。   Records are logically ordered by search-keys. Chain together records by pointers. Need to reorganize the file from time to time to restore sequential order.  Hashing   The file records are stored in a number of buckets, the #bucket is the address of the record    Multitable Clustering File Organization motivation: store related records on the same block to minimize I/O.   这种方式对于包含join操作的查询很有好处，方便natural join  重点！： 索引对于\n select有正作用 增删改有负作用(若没有用到索引，则无作用) update需要分情况讨论，有些update操作既利用查询也用到改，这种正+负的作用最后往往是整体负效果。  10.7 Data Dictionary Storage The Data dictionary (also called system catalog) stores metadata\n Information about relations  names of relations names, types and lengths of attributes of each relation names and definitions of views integrity constraints   User and accounting information, including passwords Statistical and descriptive data  number of tuples in each relation    10.8 Buffer Management LRU算法替换\nAppendix I. 在一些小型关系数据库系统(dbase, Foxbase)  DB关系表中元组/数据行 到 DB文件中数据的逻辑地址（文件名，记录号）间的映射 DB文件记录号到外设上数据物理地址（记录所在的物理块号和具体地址）的映射  II. 在一些大型关系数据库系统(SQL Server, Oracle) SQL Server利用索引分配映象IAM数据结构纪录每个数据库文件的页与该页所在物理盘区间的对应关系 Chapter 11 Indexing and Hashing 11.1 Basic Concepts   Search Key attributes or a set of attributes used to look up the records in a file\n  Indexing mapping from search-key to storage locations of the file records    ordered indices: the index file is used to store the index entries in which the search key of the records and the address of the records are stored in sorted order hash indices: the “hash function” is used to map the the search key of the records to the address of the records. The records are stored in the “buckets”   索引支持等值、比较、范围查询\n 11.2 Ordered Indices DB files with indexing mechanism include 2 parts:\n indexed file: in which data records are stored; index file: in which index entries are included.   In index file, the index entries in the index file are stored in accordance with the order of the search key.\n 11.2.1 Primary and Secondary Indices  Primary Index (Clustering Index) 索引文件的搜索键所规定的顺序和被索引的顺序文件中的记录顺序一致。   聚集索引 vs 主索引 主索引：建立在主键的索引。 一张表只能一个聚集索引，因为数据文件根据索引项进行排序，只能有一种排序顺序。但可以建立多个非聚集索引。 主索引一定是聚集索引，聚集索引不一定是主索引。\n Secondary Index (non-clustering Index) Def: an index whose search key specifies an order different from the sequential order of the file.\nSecondary indices have to be dense indices.    Dense index: the index record in the index file appears for every search-key value in the indexed file.   Sparse Index: index file contains index entries for only some search-key values in the indexed file   Primary vs Secondary Indices Sequential scan using primary index is efficient, but a sequential scan using a secondary index is expensive.  11.2.2 Multi-level Indices Sometimes the index file may be very large, and cannot be entirely kept in memory. Solution: treat primary index kept on disk as a sequential file and construct a sparse index on it.\n outer index: a sparse index of primary index file inner index: the primary index file  11.2.5 Indices on multiple keys Composite search keys are search keys containing more than one attribute. E.g. (dept_name, salary) E.g. create index Mutiple-index on takes(course_id, semester, year)\n左前缀原则：  三个重要概念： table scan: 表组织为堆文件，依次扫描判断满不满足where查询条件。 index seek: 用索引寻找(从上至下)，对应聚集索引的查找，效率较高。 index scan: 不用索引寻找，对应利用聚集索引建起来的树进行非聚集索引的查找，在索引顺序文件的叶子结点从左向右遍历。\n 11.3/11.4 B+树 / B树 此处考试不做要求，但对于数据库而言是很重要的组织索引的数据结构，一定要理解！\n Primary index 文件记录直接存储在叶节点 Secondary index 叶节点存储指向文件记录的指针  11.5/11.6 Hashing Index Files  Hash Indices Hashing can be used not only for file organization, but also for index-structure creation.\nA hash index organizes the search keys, with their associated record pointers, into a hash file structure.    Hash indices are always secondary indices\n  Static hashing hash function h cannot be modified, while being used\n  Dynamic hashing hash function h to be modified dynamically\n  11.8 Comparison of Ordered Indexing and Hashing  等值查询：Hashing 范围查询：Ordered Indexing  11.10 Index Definition and Usage in SQL ATTENTION 使用索引不一定真的能提高select查询速度。 对于非聚集索引的查找对比直接扫描数据文件，可能后者更快！\n","date":"November 23, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/database-system/database_system_3/","summary":"Chapter 10 Storage and File Structure  对于应用程序和disk而言，中间会有data buffer作为数据交换的缓冲池，替换算法一般采用LRU算法，注意两种可控的参数，在实验中会用到：\n 连接时长 buffer大小   逻辑结构：\n 流式文件、基于记录文件、基于索引文件  物理结构：\n 以block为单位进行存储 contiguous linked indexed  10.5 File Organization Each file is a sequence of records, and a relational table is a set of tuples. A tuple is stored as a record in the DB file.\n10.5.1 Fixed-Length Records   a block contains several records. Store record i starting from byte n * (i – 1), where n is the size of each record.","tags":null,"title":"Database System Lecture Note 3"},{"categories":null,"contents":"贪心算法 算法+证明\n一、最优装载问题  算法：  void Loading(int x[], int w[], int c, int n) { int *t = new int [n+1]; Sort(w, t, n); for(int i = 0; i \u0026lt; n; i++) x[i] = 0; for(int i = 0; i \u0026lt; n \u0026amp;\u0026amp; w[t[i]] \u0026lt; c; i++) { x[t[i]] = 1; c -= w[t[i]]; } } 证明：  最优子结构性质：    二、哈夫曼编码 三、最小生成树 回溯法 子集树：0-1背包问题，从包含n个全集当中去选择一个子集，所有可能解是$O(2^n)$ 排序树：TSP问题，解是一个排列，所有可能解的规模是$O(n!)$\n 子集树：当所给的问题是从n个元素的集合S中找出满足某种性质的子集时，相应的解空间称为子集树。 遍历子集树 时间复杂度：$O(2^n)$  void backtrack (int t) { if (t\u0026gt;n) output(x); else { for (int i=0;i\u0026lt;=1;i++) { x[t]=i; if (legal(t)) backtrack(t+1); } } }  排列树：当所给的问题是确定n个元素满足某种性质的排列时，相应的解空间树成为排列树。 遍历排序树 时间复杂度：$O(n!)$  void backtrack (int t) { if (t\u0026gt;n) output(x); else { for (int i=t;i\u0026lt;=n;i++) { swap(x[t], x[i]); if (legal(t)) backtrack(t+1); swap(x[t], x[i]); } } } 剪枝设计：\n 问题的约束条件不满足时 当大于当前的最优解时 在构造排序树时，为了达到很好的剪枝效果，深度遍历的第一个叶子节点是由贪心方法得出的。 贪心方法+分支限界：效果很好  1. 装载问题 2. 批处理作业调度问题 回溯法考试题目步骤： 设计出解向量 画出解空间树 给出剪枝的方案 写出伪代码 画出剪枝\n3. n皇后问题 此时的解空间树不再是传统的子集树、排序树，而是自己独有的树。\n4. 图的着色问题   解空间： n维向量，每一维代表一个顶点，值代表其的颜色值。\n  解空间树 根节点从顶点1开始，\n  迭代方式：\n  GraphColor(int n,int m,int color[],bool c[][5]) { int i,k; for (i=0; i\u0026lt;n; i++ ) //将解向量color[n]初始化为0  color[i]=0; k=0; while (k\u0026gt;=0) { color[k]=color[k]+1; //使当前颜色数加1  while ((color[k]\u0026lt;=m) \u0026amp;\u0026amp; (!ok(color,k,c,n))) //当前颜色是否有效  color[k]=color[k]+1; //无效，搜索下一个颜色  if (color[k]\u0026lt;=m) //求解完毕，输出解  { if (k==n-1) break; //是最后的顶点，完成搜索  else k=k+1; //否，处理下一个顶点  } else //搜索失败，回溯到前一个顶点  { color[k]=0; k=k-1; } } } 递归方式：  void GraphColor(int k) { if(k \u0026gt; n) { sum++; for(int i = 1; i \u0026lt;= n; i++) cout \u0026lt;\u0026lt; x[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; cout \u0026lt;\u0026lt; endl; } else { for(int i = 1; i \u0026lt;= m; i++) { x[k] = i; if(ok(k)) GraphColor(k+1); } } } void ok(int k) { for(int i = 1; i \u0026lt;= n; i++) { if((c[i][k]==1) \u0026amp;\u0026amp; (x[i]==x[k])) return false; return true; } } 分支限界法 常用的分支限界法\n 队列式(FIFO)分支限界法 优先队列式分支限界法  1. 0/1背包问题 上界的作用：叶子结点得到的当前最优值可以剪去很多在优先队列中尚未拓展的结点，与这些结点的理想值ub进行比较再删减。\n 先进先出队列：每次队列中总是取出头元素，不考虑元素间的大小关系； 优先队列：每次取出的头元素均是经过比较后得到的队列中值最优的元素。更为常用  2. TSP问题  用贪心法得到的近似解作为问题的上限，从而进行剪枝； 考虑TSP一个完整的解，每一个城市都会进去一次，出来一次，所以从邻接矩阵中每一行取出两个最小的元素(代表从这个城市进出的最小代价)，此处牢记若已经有确定的边加入，就要加上该边和其他权值最小的边。这样加起来以后除以2，就得到下界。  以大题为主，分支限界法不考，\n","date":"November 16, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/algorithms/algorithm_analysis-3/","summary":"贪心算法 算法+证明\n一、最优装载问题  算法：  void Loading(int x[], int w[], int c, int n) { int *t = new int [n+1]; Sort(w, t, n); for(int i = 0; i \u0026lt; n; i++) x[i] = 0; for(int i = 0; i \u0026lt; n \u0026amp;\u0026amp; w[t[i]] \u0026lt; c; i++) { x[t[i]] = 1; c -= w[t[i]]; } } 证明：  最优子结构性质：    二、哈夫曼编码 三、最小生成树 回溯法 子集树：0-1背包问题，从包含n个全集当中去选择一个子集，所有可能解是$O(2^n)$ 排序树：TSP问题，解是一个排列，所有可能解的规模是$O(n!)$\n 子集树：当所给的问题是从n个元素的集合S中找出满足某种性质的子集时，相应的解空间称为子集树。 遍历子集树 时间复杂度：$O(2^n)$  void backtrack (int t) { if (t\u0026gt;n) output(x); else { for (int i=0;i\u0026lt;=1;i++) { x[t]=i; if (legal(t)) backtrack(t+1); } } }  排列树：当所给的问题是确定n个元素满足某种性质的排列时，相应的解空间树成为排列树。 遍历排序树 时间复杂度：$O(n!","tags":null,"title":"Greedy \u0026 Back-track \u0026 Branch and Bound"},{"categories":null,"contents":"Chapter 8 Memeory Management 8.1 Background   Address Space: the memory scope that the process can access(可访问的(内)存储区范围)\n  A pair of base register and limit register define the logical address space   8.1.2 Address Binding Chapter 9 Virtual Memory ","date":"November 13, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_5/","summary":"Chapter 8 Memeory Management 8.1 Background   Address Space: the memory scope that the process can access(可访问的(内)存储区范围)\n  A pair of base register and limit register define the logical address space   8.1.2 Address Binding Chapter 9 Virtual Memory ","tags":null,"title":"Memory Management"},{"categories":null,"contents":"语法分析 知识点：\n运行环境 知识点：\n 活动记录、控制栈 栈式存储分配 非局部名字的访问 参数传递方式  7.1 程序运行时的存储组织 7.1.1 程序运行空间的划分   活动与过程的概念 过程：静态概念 活动：一次过程的每次执行，是动态概念 两者可以为1:1或1:m的关系，递归过程中可能一若干个活动活着。\n  活动的生存期 活动的生存期要么是不重叠的，要么是嵌套的(区别于进程的地方！活动不可以并发执行！)\n  对于一个程序，它会向操作系统申请一块内存空间： 活动记录会在控制栈中根据栈式存储分配策略来实现。 7.1.2 活动记录与控制栈  控制栈 局部数据的安排 编址限制的影响(padding)  7.1.3 名字的作用域及名字绑定   名字的作用域： 最近嵌套原则\n  名字绑定： note: 对于递归程序中定义的名字，即使是同一个名字也可能映射到不同的存储空间。\n   左值：存储空间的地址 右值：存储空间的内容  7.2 存储分配策略 7.2.1 静态存储分配  条件：源程序中声明的各种数据对象所需存储空间的大小在编译时都可以确定。 存储分配：编译时,为他们分配固定的存储空间。 地址绑定：程序装入内存时进行。 运行期间：名字的左值保持不变   不允许递归调用和建立动态数据结构\n 7.2.2 栈式存储分配(重点) 递归调用：同一个过程在不同存储位置出现。\n  调用序列   返回序列 其中top_sp = top_ep' - C1 - C2; P是通过top_sp找到返回值并放入局部数据域中。\n  7.2.3 堆式存储分配 当活动记录的释放不需要遵循先进后出的原则 控制链代表着调用与被调用者的关系，无需出栈操作。这种存储分配用于动态创建或撤销一个数据结构使用。\n7.3 非局部名字的访问 对非局部名字的访问通过访问链实现，关键在于如何创建、使用、维护访问链。\n7.3.1 静态作用域规则   非嵌套过程 引用的名字只有两类：\n 局部的：放入局部数据域中，通过top_ep访问 全局的：放入静态数据区中(不在活动记录)    嵌套过程\n    嵌套关系：嵌套深度的概念\n  访问链：被调用过程活动记录的访问链指向其直接外层过程的最新活动的活动记录\n 过程p引用非局部名字a，则$n_a \u0026lt; n_p$；  符号表中，变量名字的目标地址变为： \u0026lt;嵌套深度，偏移量\u0026gt;    访问链的建立：\n q直接嵌套在p中：$n_q = n_p + 1$ q不嵌套在p中：$n_q \\le n_p$    display表 为了减少指针操作，提高访问非局部名字的速度。    组织形式：\n d[i]指向嵌套深度为i的过程的最新活动的活动记录 前i-1个元素指向按静态规则包围过程P的那些过程的最新的活动记录    维护：\n 过程调用时，调用序列中 将q的最新活动记录压入栈顶，并将其插入到$d[n_q]$所指链表中，使其成为该链表的首结点。(插在链首) 活动结束时，返回序列中 删除$d[n_q]$链表的首结点。    7.4 参数传递机制   传值调用 在过程中，参数和局部变量一样可以被赋值，但其结果不影响过程体之外的变量的值。\n  引用调用 调用过程把实参存储单元的地址（即一个指向实参存储单元的指针）传递给被调用过程的相应形参。 被调用过程执行时，通过形参间接地引用实参。\n  复制恢复 传值调用和引用调用的一种混合形式。效率高\n 过程调用时，调用过程对实参求值，将实参的右值传递给被调用过程，写入其活动记录的参数域中(copy in)，并记录与形参相应的实参的左值。 被调用过程执行时，对形参的操作在自己的活动记录的参数域空间上进行。 控制返回时，被调用过程根据所记录的实参的左值把形参的当前右值复制到相应实参的存储空间中(copy out) 。当然，只有具有左值的那些实参的值被复制出来。    传名调用 直接换名字。\n  中间代码生成 知识点：\n 三地址代码 语句的翻译 布尔表达式的翻译 回填技术  中间代码作为过渡的优点：便于编译程序的建立和移植。\n8.1 中间代码形式 8.1.1 图形表示   语法树\n  dag图\n  8.1.2 三地址代码 三地址语句的一般形式：x := y op z\n x可以是名字、临时变量 y、z 可以是名字、常数、或临时变量 op 代表运算符号，如算数运算符、或逻辑运算符等 语句中，最多有三个地址。   四元式 三元式 间接三元式  8.2 赋值语句的翻译 8.2.1 仅涉及简单变量的赋值语句 8.2.2 涉及数组元素的赋值语句 8.2.3 记录结构中域的访问 8.3 布尔表达式的翻译 8.3.1 数值表达式 类似于算术表达式的求值：\n 与或非运算与加法、乘法、负号运算一致； 关系表达式有固定的三地址代码形式：   8.3.2 控制流表达式 关心程序应该跳转到的位置。 布尔表达式被翻译为一系列条件转移和无条件转移三地址语句。\n  继承属性：\n E.true：为真时转移到的三地址语句的标号 E.false：为假时转移到的三地址语句的标号    短路运算   测试目的：找错 对测试有很大影响：可以控制测试代码的部分\n 黑盒测试：功能测试 白盒测试：结构测试  条件组合测试：针对短路代码  E1：真，E2：真 E1：真，E2：假 E1：假，E2：真 E1：假，E2：假      用控制流表示法翻译布尔表达式时，在一遍扫描中： 当生成某些转移指令时，目标地址可能还不知道\n8.3.4 回填技术  先产生没有填写目标标号的转移指令； 建立一个链表，把转向这个目标的所有转移指令的标号填入该链表； 目标地址确定后，再把目标地址填入该链表中记录的所有转移指令中； 记录未填地址的语句的地址序号，并标注真假.t / .f； 填完以后从链表删除。删除以后对链表进行合并，合并出只有.t / .f 两个表项  8.4 控制语句的翻译 ","date":"October 27, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/compiler/compilers_3/","summary":"语法分析 知识点：\n运行环境 知识点：\n 活动记录、控制栈 栈式存储分配 非局部名字的访问 参数传递方式  7.1 程序运行时的存储组织 7.1.1 程序运行空间的划分   活动与过程的概念 过程：静态概念 活动：一次过程的每次执行，是动态概念 两者可以为1:1或1:m的关系，递归过程中可能一若干个活动活着。\n  活动的生存期 活动的生存期要么是不重叠的，要么是嵌套的(区别于进程的地方！活动不可以并发执行！)\n  对于一个程序，它会向操作系统申请一块内存空间： 活动记录会在控制栈中根据栈式存储分配策略来实现。 7.1.2 活动记录与控制栈  控制栈 局部数据的安排 编址限制的影响(padding)  7.1.3 名字的作用域及名字绑定   名字的作用域： 最近嵌套原则\n  名字绑定： note: 对于递归程序中定义的名字，即使是同一个名字也可能映射到不同的存储空间。\n   左值：存储空间的地址 右值：存储空间的内容  7.2 存储分配策略 7.2.1 静态存储分配  条件：源程序中声明的各种数据对象所需存储空间的大小在编译时都可以确定。 存储分配：编译时,为他们分配固定的存储空间。 地址绑定：程序装入内存时进行。 运行期间：名字的左值保持不变   不允许递归调用和建立动态数据结构\n 7.2.2 栈式存储分配(重点) 递归调用：同一个过程在不同存储位置出现。\n  调用序列   返回序列 其中top_sp = top_ep' - C1 - C2; P是通过top_sp找到返回值并放入局部数据域中。","tags":null,"title":"Semantic Analysis \u0026 Runtime Environment"},{"categories":null,"contents":"语法制导翻译技术 整体思路：\n 首先，根据翻译目标来确定每个产生式的语义； 其次，根据产生式的含义，分析每个符号的语义； 再次，把这些语义以属性的形式附加到相应的文法符号上（即把语义和语言结构联系起来）； 然后，根据产生式的语义给出符号属性的求值规则 （即语义规则），从而形成语法制导定义。  ==翻译目标决定产生式的含义、决定文法符号应该具有的属性，也决定了产生式的语义规则。==\n两种描述语法制导翻译的形式：\n 语法制导定义：是对翻译的高层次说明，它隐蔽了一些实现细节，无须指明翻译时语义规则的计算次序 L-属性可以一遍扫描，省去分析树和依赖图的步骤。 翻译方案：指明了语义规则的计算次序，规定了语义动作的执行时机。  5.1 语法制导定义以及翻译方案 5.1.1 语法制导定义  对上下文无关文法的推广\n   综合属性 左部符号的综合属性是从该产生式右部文法符号的属性值计算出来的；在分析树中，一个内部结点的综合属性是从其子结点的属性值计算出来的。 在分析树中，若一个结点的某一属性由其子节点属性决定，则为综合属性 若在一个语法制导定义仅仅使用综合属性，则称之为S-属性定义。而对于这种属性，通常采用自底向上的方法进行注释。   继承属性 出现在产生式右部的某文法符号的继承属性是从其所在产生式的左部非终结符号和/或右部文法符号的属性值计算出来的；在分析树中，一个结点的继承属性是从其兄弟结点和/或父结点的属性值计算出来的。 在分析树中，一个结点的继承属性由其父节点属性或它的兄弟节点属性值决定。 可以用继承属性表示程序设计语言中的上下文之间的依赖关系。   当一个语义规则的唯一目的是产生某个副作用，则通常写成过程调用或程序段，看成是产生式左部非终结符的虚拟综合属性\n  5.1.2 依赖图 在依赖图中：\n 为每个属性设置一个结点 如果属性b依赖于c，那么从属性c的结点有一条有向边连 到属性b的结点。   5.1.3 拓扑排序 首先找到入度为0的节点，删除该点及其边，重复该过程至所有点均被删除。\n5.1.4 计算顺序 根据拓扑排序的顺序进行求值得到翻译。\n总结：最基本的文法用于建立输入符号串的分析树；\n 为分析树构造依赖图； 对依赖图进行拓扑排序； 从这个序列得到语义规则的计算顺序； 照此计算顺序进行求值，得到对输入符号串的翻译。  5.1.5 S-属性与L-属性  S属性定义：仅仅涉及综合属性，是L属性的子集 L属性定义：要么是综合属性，要么是由父节点和左兄弟节点决定的继承属性。(右边的不行) 对于L属性计算顺序：深度优先遍历分析树  进入节点前，计算它的继承属性 从节点返回时，计算它的综合属性    5.1.6 翻译方案   S属性翻译方案设计   L属性翻译方案设计\n 计算该继承属性的动作必须出现在相应文法符号之前。 一个语义动作不能引用这个动作右边的文法符号的综合属性 综合属性的计算动作放在产生式的右端末尾    5.2 S-属性定义的自底向上翻译 5.2.1 S-属性定义的自底向上实现  修改分析栈    实际上文法符号不需要存入栈中，只是看栈的状态即可。\n 修改分析程序  属性随状态一起入栈 每个语义规则编写一条代码 归约前执行产生式相关的代码 当pop和push值相同时，无需代码 当进行归约时，ntop=top-|b|+1 每一段代码被执行后，top=ntop    注意：一种方式是在分析栈中写出state和val，还有一种更接近人的思维模式，是用文法符号代替状态。(你要是想，可以三个一起写在栈里)\n5.2.2 为表达式构造语法树的语法制导定义 语法树：分析树的抽象(压缩)定义 翻译目标：为表达式创造语法树 文法符号的属性：记录所建结点指向相应子树根结点的指针\n大致过程如下： 根据每次栈中的运算规则，自底向上进行归约和属性运算。方向如同蓝色箭头所指，这里主要涉及的加法和乘法运算是建立内部结点并将结点中的左右指针指向叶子节点或内部节点。 (如果无法理解，打开PPT看看动画过程！相信你会想起来的！) 注意：这是一种后序的深度优先搜索!\n表达式的有向非循环图(dag) dag与语法树不同之处在于对应一个公共子表达式的结点具有多个父节点(这个性质决定了dag不是树)。换言之，若先前建立过结点，则不会再新建结点，而是直接将指针指向已有结点。 Example： 5.3 L-属性定义的自顶向下翻译 5.3.1 消除翻译方案中的左递归(难点) 对于消除左递归之后新生成的状态R，有以下两种属性：\n 继承属性R.i：表示在R之前已经推导出的子表达式的值 综合属性R.s：表示在R完全展开之后得到的表达式的值    消除翻译方案中左递归的一般方法 记住右图中计算属性的顺序，红色的为归约时计算继承属性的线路。\n  翻译方案： 动作执行时机：在文法符号充分展开之后，在执行产生式后面的部分。\n  此处看看PPT中例子的动画展示，充分理解在进行归约时如何计算属性\n5.3.2 预测翻译程序的设计   为每个非终结符号A建立一个函数(可以是递归函数)\n A的每一个继承属性对应函数的一个形参 A的综合属性作为函数的返回值 A中的每一个属性都记作局部变量(这里注意继承属性可是要成为形参的！)    A的函数代码由多个分支组成，对应多个产生式   5.4 L属性定义的自底向上翻译 5.4.1 移走翻译方案中嵌入的语义规则 思想： 等价变换：将所有嵌入的动作都出现在产生式的右端末尾\n 变换前后的翻译方案是等价的\n 方法：\n 在基础文法中引入新的产生式，形如：$M \\rightarrow \\epsilon$ M：标记非终结符号，用来代替嵌入在产生式中的动作 把被M替代的动作放在产生式$M \\rightarrow \\epsilon$的末尾  5.4.2 直接使用分析栈中的继承属性 复制规则的重要作用:\n 利用栈中的T.type来赋值给未知的L.in。  5.4.3 变换继承属性的计算规则 要想从栈中取得继承属性，当且仅当文法允许属性值在栈中存放的位置可以预测。 但往往继承属性在栈中的位置不可预测！ 所以，用标记非终结符号模拟非复制规则的语义规则 为了在分析的同时进行翻译，每个文法符号的属性都要入栈，综合属性可以放在与文法符号对应的栈中Val数组中，继承属性则放在标记非终结符对应的Val数组中，由于是L-属性定义，在标记非终结符处一定可以计算继承属性\n引入标记非终结符对语法分析的影响\n LL(1)文法引入标记非终结符后仍是LL(1)文法，不会产生分析冲突 LR(1)文法引入标记非终结符后不能保证还是LR(1)文法，可能导致分析冲突  5.4.4 改写语法制导定义为S属性定义 ","date":"October 27, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/compiler/compilers_2/","summary":"语法制导翻译技术 整体思路：\n 首先，根据翻译目标来确定每个产生式的语义； 其次，根据产生式的含义，分析每个符号的语义； 再次，把这些语义以属性的形式附加到相应的文法符号上（即把语义和语言结构联系起来）； 然后，根据产生式的语义给出符号属性的求值规则 （即语义规则），从而形成语法制导定义。  ==翻译目标决定产生式的含义、决定文法符号应该具有的属性，也决定了产生式的语义规则。==\n两种描述语法制导翻译的形式：\n 语法制导定义：是对翻译的高层次说明，它隐蔽了一些实现细节，无须指明翻译时语义规则的计算次序 L-属性可以一遍扫描，省去分析树和依赖图的步骤。 翻译方案：指明了语义规则的计算次序，规定了语义动作的执行时机。  5.1 语法制导定义以及翻译方案 5.1.1 语法制导定义  对上下文无关文法的推广\n   综合属性 左部符号的综合属性是从该产生式右部文法符号的属性值计算出来的；在分析树中，一个内部结点的综合属性是从其子结点的属性值计算出来的。 在分析树中，若一个结点的某一属性由其子节点属性决定，则为综合属性 若在一个语法制导定义仅仅使用综合属性，则称之为S-属性定义。而对于这种属性，通常采用自底向上的方法进行注释。   继承属性 出现在产生式右部的某文法符号的继承属性是从其所在产生式的左部非终结符号和/或右部文法符号的属性值计算出来的；在分析树中，一个结点的继承属性是从其兄弟结点和/或父结点的属性值计算出来的。 在分析树中，一个结点的继承属性由其父节点属性或它的兄弟节点属性值决定。 可以用继承属性表示程序设计语言中的上下文之间的依赖关系。   当一个语义规则的唯一目的是产生某个副作用，则通常写成过程调用或程序段，看成是产生式左部非终结符的虚拟综合属性\n  5.1.2 依赖图 在依赖图中：\n 为每个属性设置一个结点 如果属性b依赖于c，那么从属性c的结点有一条有向边连 到属性b的结点。   5.1.3 拓扑排序 首先找到入度为0的节点，删除该点及其边，重复该过程至所有点均被删除。\n5.1.4 计算顺序 根据拓扑排序的顺序进行求值得到翻译。\n总结：最基本的文法用于建立输入符号串的分析树；\n 为分析树构造依赖图； 对依赖图进行拓扑排序； 从这个序列得到语义规则的计算顺序； 照此计算顺序进行求值，得到对输入符号串的翻译。  5.1.5 S-属性与L-属性  S属性定义：仅仅涉及综合属性，是L属性的子集 L属性定义：要么是综合属性，要么是由父节点和左兄弟节点决定的继承属性。(右边的不行) 对于L属性计算顺序：深度优先遍历分析树  进入节点前，计算它的继承属性 从节点返回时，计算它的综合属性    5.","tags":null,"title":"Syntax-directed Translation"},{"categories":null,"contents":"Chapter 7 Database Design and E-R Model 7.2 Concepts  Entity sets Relationship sets 对于多元联系$\\Rightarrow$二元联系$\\Rightarrow$关系表 Attribute  7.3 Constraints  Mapping Cardinalities  one to one one to many many to one many to many   Keys  superkey candidate key primary key     Chapter 8 Schema Normalization 核心思想：将large schemas decompose to smaller schema. 建立functional dependency, 并利用两表查询的的方式进行查询(此处详见第7章)。 在第7章通过将E-R图进行转换，得到面向特定的初始关系模式集，这些关系模式集可能存在多种数据依赖关系：\n Functional Dependencies Multivalued Dependencies Join Dependencies (略)  如果直接根据初始关系模式构造DBS，由于初始关系模式中数据依赖关系的存在，可能会违反DB的完整性约束\n pitfalls：插入、更新、删除问题  因此，对初始关系模式集，需要根据关系规范化理论，在保证关系模式的\n lossless join dependency preservation  步骤大致为：\n 根据函数依赖的Armstrong’s 公理系统 ( §8.4.1 )和多值依赖 的公理系统 ，从初始关系模式集中已知的函数依赖和多值依赖出发，推导出初始关系模式集中所有的函数依赖（§8.4)和多值依赖(不作要求) 模式分解算法，对其进行（等价）分解和变换，将其转换为各种范式形式。  8.2 Atomic Domains and First Normal Form   Atomic Domains: its elements are considered to be indivisible units. 反例：集合(非原子域)、复合属性等\n  First Normal Form: A relational schema R is in first normal form if the domains of all attributes of R are atomic\n  8.3 Decomposition Using Functional Dependencies 8.3.1 Functional Dependencies   Def1: Functional dependency(FD) holds on R   Keys in relational schema can be defined in terms of FD\n K is a superkey for relation schema R, if and only if $k \\rightarrow R$ K is a candidate key for R, if and only if  $k \\rightarrow R$, and for no $\\alpha \\subset K, \\alpha \\rightarrow R$      Def2: (A particular relation instance) r(R) satisfy FD, or FD is satisfied by r(R).\n   How to guarantee the FD in DBS? answer：用SQL检查2个属性之间的FD\n  FD holds on R: 整体要求，定义在R的属性间的语义约束 FD is satisfied by r(R): 部分满足 意思是对于关系R而言，即使它的关系实例r(R)满足某些FD，也不能代表整个关系R就满足该FD！可能只是该关系实例的数据凑巧满足而已。(容易出判断题考察)\n  Trivial FD: 所有关系实例均满足FD，大多是右边属性包含于左边\n  Transitive dependency (传递函数依赖)\n  Partial dependency (部分函数依赖) 即对于$\\alpha \\rightarrow \\beta$中，$\\alpha$含有冗余项。\n  Closure of FD The set of all functional dependencies logically implied by F is the closure of F, denoted as $F^+$.\n  8.3.2 Boyce-Codd Normal Form  Def: 每一个FD需要满足其中之一的性质 Decomposing a Schema into BCNF: 相当于外键关联  ATTENTION：BCNF 并不能保持函数依赖，所以需要考虑上一级的3NF\n8.3.4 Third Normal Form   Def:在BCNF的定义中添加该性质   性质：(必要条件)\n R is also in 2NF 不存在非主属性对候选键的部分和传递依赖，i.e.每一个非主属性都不传递依赖于R的任何候选键    重要！！！！ 检测关系模式是否满足函数依赖的方法 8.4 Functional-Dependency Theory 8.4.1 Closure of a Set of FD 定义在上面已经给出\n  Logically implied: Given a schema R, a functional dependency f on R is logically implied by a set of FD F on R , if every instance r(R) that satisfies F also satisfies f. 简而言之，某个函数依赖f可以被F这个集合推出来\n  Armstrong\u0026rsquo;s Axioms   8.4.2 Closure of Attribute Sets   Def: An attribute B is functionally determined by $\\alpha$ if $\\alpha \\rightarrow B$.\n  Def 注意该集合是有限的\n  若通过某些属性推出R，说明其为superkey，接下来判断其是否是candidate key.   Uses of Attribute Closure 这里的第二种用途是第二种测试函数依赖的方法，相比SQL方法更为简单方便。\n  等价的概念\n  8.4.3 Canonical Cover 用于去除FD set中函数依赖的左右两端的冗余项。正则覆盖是最小的与F等价的函数依赖集合，记作$F^+ = F^+_c$。   Extraneous Attributes   两种情况的解决问题方案\n 当冗余元素在左边出现的时候 在原有函数依赖下，求属性闭包，用于判断冗余属性  当冗余元素在右边出现的时候 在新的函数依赖之下求属性闭包     A canonical cover for F is a set of dependencies Fc such that 所有函数依赖的左边不会相同，这种判断方法来源于求出canonical cover的算法。\n  Compute a canonical cover   8.4.4 Lossless-join Decomposition 不同子表间存在外键关联。 该定理意思是对于两个关系表，他们的公共属性可以推出至少其中一张关系表的所有属性，则根据公共元素分解后的分解方式为无损分解。\n8.4.5 Dependency Preserving   Restriction of F to Ri   Dependency Preserving 子模式FD成立可以推出原模式FD成立，用于测试dependency preserving。   注意以下这种情况： 看似$A\\rightarrow D$并不能满足，但有以下关系： $A|F_1 \\rightarrow B, B|F_2 \\rightarrow D$, so $A\\rightarrow D$ is preserved.\n $A|F_1$ 代表在函数依赖集$F_1$下A可以推出的attributes sets\n 总结：范式之间的关系   Second Normal Form 相当于第二范式的每一个函数依赖的左边不能出现可分的超键。\n  Third Noramal Form   BCNF   8.5 Algorithms for Decomposition 8.5.1 BCNF Decomposition 算法步骤如下：\n 找出non-BCNF子模式Ri，进一步分解； 求出F在Ri上的投影和该关系的候选键； Ri是non-BCNF的原因在于$\\alpha \\rightarrow \\beta$中$\\alpha$并不是superkey。 将Ri中分解成2部分，公共属性$\\alpha$，其中$\\alpha\\rightarrow \\beta$单独组成BCNF模式  Note: to determine whether or not Ri is in BCNF, the restriction of F to Ri and the candidate keys of Ri should be computed !!! 在分解出新的Ri以后，首先求出F在Ri上的投影和该关系的候选键，然后再用算法进行判断\n8.5.2 3NF Decomposition  Functional dependencies can be checked on individual relations without computing a join. There is always a lossless-join, dependency-preserving decomposition into 3NF.  Note:\n All candidate keys should be founded out. Only one Fc should be computed at first  如何用SQL语句表达、测试函数依赖?\n 先分组再count，但无法找出不符合函数依赖的元组  select b from r group by b having count(distinct c) \u0026gt; 1 利用两表查询的方式  select ? from r as T, r as S where T.B == S.B and T.C \u0026lt;\u0026gt; S.C 8.5.3 Computing of Candidate Keys   将R的所有属性分为四类： L类：仅出现在F中函数依赖左部的属性 R类：仅出现在F中函数依赖右部的属性 N类：在F中函数依赖左右两边均未出现的属性(该属性一定为候选键) LR类：在F中函数依赖左右两边均出现的属性\n  X_set代表L、N类，Y_set代表LR类   本章练习题：\n 给定关系表r(R)和若干函数依赖，判断r是否满足函数依赖 运用SQL语句 判断关于函数依赖的一些公式是否成立： Armstrong Axiom 求候选键 求属性闭包 求函数依赖集F的最小正则集 给定关系模式R和定义在R上的函数依赖集F，判断R属于第几范式 判断一个模式分解是否为无损连接、函数依赖保持 3NF、BCNF范式分解  ","date":"October 23, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/database-system/database_system_2/","summary":"Chapter 7 Database Design and E-R Model 7.2 Concepts  Entity sets Relationship sets 对于多元联系$\\Rightarrow$二元联系$\\Rightarrow$关系表 Attribute  7.3 Constraints  Mapping Cardinalities  one to one one to many many to one many to many   Keys  superkey candidate key primary key     Chapter 8 Schema Normalization 核心思想：将large schemas decompose to smaller schema. 建立functional dependency, 并利用两表查询的的方式进行查询(此处详见第7章)。 在第7章通过将E-R图进行转换，得到面向特定的初始关系模式集，这些关系模式集可能存在多种数据依赖关系：\n Functional Dependencies Multivalued Dependencies Join Dependencies (略)  如果直接根据初始关系模式构造DBS，由于初始关系模式中数据依赖关系的存在，可能会违反DB的完整性约束","tags":null,"title":"Database System Lecture Note 2"},{"categories":null,"contents":"I. RNN Structure Overview The input is a sequence of vectors.  note: Changing the input sequence order will change the output\n We use the same neural network to train, each color in NN means the same weight. When the values stored in the memory is different, the output will also be different. II. Types of RNN   Elman\u0026rsquo;s memory restore the values of hidden layer output, and Jordan\u0026rsquo;s memory restore the values of output. Usually Jordan network have better performance because output y has target.   Bidirectional RNN Using bidirectional RNN can get the information from the context. For example, for the hidden layer output $y^{t+1}$, its value is determined by not only $x^t$ and $x^{t+1}$ but also $x^{t+1}$ and $x^{t+2}$.\n  III. *Long Short-term Memory (LSTM) 1. LSTM Overview Input gate: Decide whether the input can be written in memory cell; Output gate: Decide whether the output of memory cell can be sent to other part of the network; Forget gate: Decide whether the content in memory cell should be erased. 2. Detailed process In this figure, there is a input $z$ and three other scalar $z_i, z_f, z_o$(信号量). Activation function f is usually a sigmoid function, whose value is between 0 and 1. $f(z_i)$ controls the input value of $g(z)$, and $f(z_f)$ controls the memory cell\u0026rsquo;s value. The new value stored in the memory cell is $c'$.\n3. Example 此处强烈建议看一下PPT中的LSTM讲解： Every scalar is determined by input vector(例子中是三维) through linear transform, which means every input vector multiply the corresponding weight and add the bias. All the weight and bias are trained by trainig data through GD.\n4. LSTM\u0026rsquo;s parameter LSTM的每一块相当于原neural network的神经元，可以直接替换。LSTM的参数量为feed-forward NN的参数量的4倍。\n5. LSTM Model In this figure, every dimension of vector $z$ is a input of LSTM neuron, and the product operator is actually element-wise product. For the standard LSTM, it has multiple layer and the input of every layer consists of current input vector $x^{t+1}$, the output of previous layer $h^t$ and the memory cell $c^t$.\n In fact, you could use model in Keras such as \u0026ldquo;LSTM\u0026rdquo;, \u0026ldquo;GRU\u0026rdquo;(simplified LSTM) and \u0026ldquo;SimpleRNN\u0026rdquo;.\n IV. RNN 1. Loss Function The cross-entropy between RNN output and reference vector for every time slot.  Reference vector就是每一个单词所对应的reference组成的向量。\n 2. Learning Method: Backpropogation through time (BPTT) However, RNN-based network is not always easy to learn. The error surface is either very flat or very steep. When you step into a point whose gradient is relatively large and your learning rate is also large, your learning step will make a really huge move so that the program will tell you segmentation fault. One way to avoid this circumstance is to use Cliping, which means to set a threshold of gradient and every move can not larger than the threshold. The reason for this rough error surface is shown as below: 3. Helpful Techniques  RNN vs LSTM LSTM can deal with gradient vanishing(not gradient explore).  LSTM: Memory and input are added and storedin memory unless forget gate is closed. RNN: Every time memory will be formated. The influence never disappears unless forget gate is closed.     此处老师说有国际大厂面试里考过哦！\n Note: Gate Recurrent Unit(GRU) is simpler than LSTM for it has only two gate. 它将 input gate 和 forget gate 进行了联动。\nOther techniques:  此处老师讲述了Hinton的一篇论文，用一般training的方法，initialize的weight是random的话，sigmoid的效果要好于ReLu；他用indentity matrix来initialize weight，这样使用ReLu作为Activation函数则可以显著提高效果。利用这种方法的RNN在效果上吊打LSTM。非常的玄学\n 4. More Application  Many to one: Input is a vector sequence, but output is only one vector Many to many: Both input and output are both sequences, but the output is shorter.    CTC   Sequence to sequence learning 有一篇论文讲到可以直接将某种语言的语音信号作为输入，得到另一种语言文字形式的输出，并发现其有可行性。\n Seq2seq Auto Encoder(没听懂)    V. Attention-based Model 1. Model Overview 其中的DNN/RNN类似于计算机中的CPU概念。 2. Model version 2 Neural Turing Machine： ","date":"October 15, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/rnn/","summary":"I. RNN Structure Overview The input is a sequence of vectors.  note: Changing the input sequence order will change the output\n We use the same neural network to train, each color in NN means the same weight. When the values stored in the memory is different, the output will also be different. II. Types of RNN   Elman\u0026rsquo;s memory restore the values of hidden layer output, and Jordan\u0026rsquo;s memory restore the values of output.","tags":null,"title":"Recursive Neural Network"},{"categories":null,"contents":"Chapter 6 Process Synchronization 6.1 Background 6.1.1 Shared resources Def:\n the resources (e.g., data, CPU, I/O ports, memory) that can be accessed by several cooperating processes concurrently the shared resources cannot be used by several processes simultaneously (or in parallel), only be used mutual exclusively(互斥)  信号量的三种用法：\n 资源互斥使用：资源只有1个实例，各个进程通过二元信号量mutex互斥地进入临界区，使用资源。 Mutex：代表资源的控制权，初值为1。  mutex = 1：buffer空闲 mutex = 0：buffer阻塞   资源竞争使用： 资源有多个实例，允许多个进程竞争使用资源。 多元信号量表示：(1)资源可用数目，(2)资源使用权 e.g. empty, full 进程间同步： 进程间的执行步骤需要有先后顺序关系 同步二元信号sync，初值为0  sync = 0：未开始跑 sync = 1：前面进程跑完    6.1.2 Producer-Consumer Problem 又名Bounded-Buffer problem。\n Atomic operation: an operation that completes in its entirely without interruption. Otherwise, data inconsistency will occur.  Consider this situation:\n//Producer process while (1) { /* producing an item in nextProduced */ while (counter == BUFFER_SIZE) ; /* do nothing */ buffer[in] = nextProduced; in = (in + 1) % BUFFER_SIZE; counter++; } //Consumer process while (1) { while (counter == 0) ; /* do nothing */ nextConsumed = buffer[out]; out = (out + 1) % BUFFER_SIZE; counter--; /* consume the item in nextConsumed */ } If the producer and the consumer access counter concurrently, data consistency can NOT be maintained.\nRace condition:\n the situation where several processes access and manipulate shared data concurrently the final value of the shared data depends upon which process finishes last  6.2 The Critical-Section Problem 6.2.1 Concepts   Critical section(临界区) each process has a code segment, called critical section, in which the shared data is accessed\n  Critical resources resources accessed by Critical section. 要求：保证该进程进入临界区时，没有其他进程在访问。\n  6.2.2 General structure of process 6.2.3 Solution to Critical-Section Problem  Mutual exclusion if process Pi is executing in its critical section specific to the resource R, then no other processes can be executing in their critical sections specific to R. Progress  只考虑那些正处于entry(申请进入)、critical sections和exit sections 的进程。 挑选过程不应被无限期阻止   Bounded Waiting a bound or limit must exist on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted.  为了避免starvation\n    note：进程在临界段内只应停留有限时间，不然会导致其他进程饿死。\n 6.3 Peterson Solution Peterson\u0026rsquo;s solution is restricted to two processes that alternate execution between their critical sections and remainder sections.\nint turn; //资源访问权 bool flag[2]; //ready to enter its critical section  // for process Pi do { flag[i] = true; /*表明自身请求进入临界区*/ turn = j; /*假设先轮到对方进入临界区 */ while (flag [j] and turn = j) { do-nothing }; /*对方正在临界区中, Pi忙等待*/ critical section flag [i] = false /*表明自身不再需要进入临界区*/ remainder section } while (1); 6.3.1 Busy-waiting Pi alternatively stays in running and ready states. (一直处于running态，无法进入waiting态) Example： 问题分析：在ts[3]时间片中，此时flag[j]=true, turn=i, 导致while循环无法进入，直接进入临界区执行；但并没有执行完，时间片用完强行切换到下一个进程。在ts[4]时间片中，此时flag[i]=true, turn=i, 满足while循环条件，则一直处于忙等待状态。 解决办法：将running改为waiting，第4片给Pi，这样避免忙等待。\n6.3.2 Bakery Algorithm A software solution for synchronize among n processes. Principles:\n before entering its critical section, process receives a number (denoted for ticket #,) the holder of the smallest number enters the critical section. if processes Pi and Pj receive the same number, if i \u0026lt; j, then Pi is served first; else Pj is served first. the numbering scheme always generates numbers in increasing order of enumeration; i.e., 1,2,3,3,3,3,4,5\u0026hellip;   利用二元组比大小：(a, b) \u0026lt; (c, d)\n bool choosing[n]; /* Pi begins/wants to apply for ticket*/ int number[n]; /* for Pi, number[i]\u0026gt;0 表示Pi需要进入临界区: */ do { choosing[i] = true; /*begin to apply for ticket*/ number[i] = max(number[0], number[1], …, number[n – 1]) + 1; //分配新的number，要求number单调不减  choosing[i] = false; /* end application */ for (j = 0; j \u0026lt; n; j++) /*考虑所有进程*/ { while (choosing[j]) ; /*如有其它进程正在申请ticket,等待Pj 申请完*/ while ((number[j] != 0) \u0026amp;\u0026amp; {(number[j], j) \u0026lt; (number[i,], i)}) ; /*如有票号小于自身的其它进程Pj 正在或等待运行， 等待Pj访问完*/ //此处是二元组判断  } critical section; number[i] = 0; //丢弃票，退出临界区  remainder section; }while(1); 6.4 Synchronization Hardware 6.4.1 Principles of TestAndSet Instruction and Swap Instruction  每个临界资源设置一个布尔变量lock，初值为false；lock用机器字来实现 CPU提供专门的硬件指令TestAndSet 或Swap，允许对一个字的内容进行检测和修正，或交换两个字的内容 硬件指令可以解决共享变量的完整性和正确性，防止出现race condition  6.4.2 Principles of Interrupt masking based hardware synchronization  “开关中断”指令保证用户态下的进程在临界区执行时不被其它进程中断,从而实现多个进程互斥访问临界区 进入临界区前执行：“关中断”指令 离开临界区后执行：执行“开中断”指令  6.5 Semaphores 6.5.1 Background  Def: 由操作系统在内核空间提供的一种用于多个合作进程间==同步与互斥==的机制。   信号量也被称为锁(lock)，但锁专指二元信号量。\n Wait() / P() 请求分配一个单位的资源S给执行wait操作的进程  wait(S) //这种进程仍会有忙等待的问题 { while(S \u0026lt;= 0) do no-op; S--; } Signal() / V() 进程释放一个单位的资源S  signal(S) { S++; } These two operations are performed atomatically.\n6.5.2 Usage Two types of semaphores:\n counting semaphore (计数、一般信号量) binary semaphore (二元信号量，又称互斥锁)   Mutual exclusion semaphore mutex: 1 资源可用，0 资源不可用  //Process Pi: do { wait(mutex); //critical section  signal(mutex); //remainder section } while (1); Synchronizing semaphore synch  P1: M1; S1; signal(synch); // 0-\u0026gt;1 P2: M2; wait(synch); // 1-\u0026gt;0, P1执行完后执行S2 S2;  PPT中的例子记得看！400米接力跑\n 6.5.3 Semaphores without busy waiting Two simple operations:\n block(): The process are changed from running to waiting. (挂起) wakeup(): Resumes the execution of a blocked process P. (唤醒)  //多元信号量版本 wait(S) {\tS.value--; /*申请一个单位的资源*/ if (S.value \u0026lt; 0) { /*无可用资源*/ add this process to S.list;\tblock(); } } Signal(S) {\tS.value++; /*释放一个单位的资源*/ if (S.value \u0026lt;= 0) /*仍有被阻塞进程*/ { remove a process P from S.list; wakeup(P); /*唤醒被阻塞进程*/ } } 6.5.4 Deadlock and Starvation  Example of deadlock   6.6 Classical Problems of Synchronization 此处只列举典型问题及其应用的简单描述，具体分析过程及答案请详见叶文版PPT 6-例题与作业\n  Bounded-Buffer Problem\n producer-consumer problem 和尚取水问题 此问题要归结为两个生产者消费者问题 Extended problem:  One producer and one consumer are permitted to simultaneously enter their critical sections to access the buffer. mutex1 for producer, mutex2 for consumer \u0026ldquo;父母-子女-橘子/苹果\u0026quot;问题 多条串行生产线/流程中的中间工序 process 同时具有生产者、消费者的角色 生产者每次向buffer放入m (\u0026gt;2)个数据项，消费者每次只消费1个数据项 [或：消费者每次消费n个数据项]； 设置计数变量count和mutex2，替代empty的计数功能，控制生产者      Readers-Writers Problem\n Soldiers in two queues, passing a bridge 过桥问题 最多允许M个读者同时读 每当M个读者依次读完后，应允许写者进入临界段 最多允许M个读者同时读，或者最多允许N个写者同时写 控制队列中读者数目 写者/读者公平竞争 通过添加优先信号量wp 写者/读者优先    Dining-Philosophers Problem\n 多类型资源申请  int Mnum = 3, IOnum = 3; semaphore self[6]; semaphore mutex = 1; enum {idle, apply, running} state[6]; void process(int i) { while(1) { idle(); apply(i); running(); release(i); } } void apply(int i) { wait(mutex); state[i] = apply; test(i); signal(mutex); //这里的释放顺序很重要，互换会导致死锁  wait(self[i]); } void test(int i) { if(Mnum \u0026gt; 0 \u0026amp;\u0026amp; IOnum \u0026gt; 0 \u0026amp;\u0026amp; state[i] == apply) { state[i] = running; Mnum--; IOnum--; signal(self[i]); } } void release(int i) { wait(mutex); state[i] = idle; Mnum++; IOnum++; for(int i=0;i\u0026lt;6;i++) test(i); signal(mutex); }   The sleeping-barber problem 问题特点： （1）资源竞争 顾客竞争chairs 顾客竞争barbers （2）同步 顾客-barber\n  The basic version:\n#define CHAIR 5 semaphore barber = 1, customer = 1; //这对信号量很关键 semaphore mutex = 1; int waiting = 0; void barber() { wait(customer); wait(mutex); waiting--; signal(mutex); signal(barber); cuthair(); } void customer() { wait(mutex); if(waiting \u0026lt; CHAIR) { waiting++; signal(customer); //通知理发师有顾客  signal(mutex); wait(barber); getcut(); } else { signal(mutex); //离开店铺  } } 6.7 Monitors (管程) 第一题：选择题 第二题：简答题——概念，用硬件命令实现互斥 第三题：大题：调度、信号量、死锁(银行家算法)\n","date":"October 14, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_4/","summary":"Chapter 6 Process Synchronization 6.1 Background 6.1.1 Shared resources Def:\n the resources (e.g., data, CPU, I/O ports, memory) that can be accessed by several cooperating processes concurrently the shared resources cannot be used by several processes simultaneously (or in parallel), only be used mutual exclusively(互斥)  信号量的三种用法：\n 资源互斥使用：资源只有1个实例，各个进程通过二元信号量mutex互斥地进入临界区，使用资源。 Mutex：代表资源的控制权，初值为1。  mutex = 1：buffer空闲 mutex = 0：buffer阻塞   资源竞争使用： 资源有多个实例，允许多个进程竞争使用资源。 多元信号量表示：(1)资源可用数目，(2)资源使用权 e.g. empty, full 进程间同步： 进程间的执行步骤需要有先后顺序关系 同步二元信号sync，初值为0  sync = 0：未开始跑 sync = 1：前面进程跑完    6.","tags":null,"title":"Process Synchronization"},{"categories":null,"contents":"Chapter 5 CPU Scheduling 5.1 Concepts on Scheduling 5.1.1 basic concepts   CPU Scheduling: (短期调度) ==selecting + allocating + enabling, in kernel mode.== The procedure of selecting running entities in main memory ( i.e., processes or threads in the ready queue) according to some criteria, allocating CPU to the selected running entities, and then enabling them to run on CPU\n  CPU burst occurs: running state\n  I/O burst occurs: waiting state\n  Process execution cycle: CPU burst + I/O burst\n  5.1.2 Scheduler (Focus)  selects from the processes in memory that are ready to execute — using scheduling algorithms allocates the CPU to one of them — 组织和维护(就绪)进程/线程队列  5.1.3 Dispatcher The dispatcher gives control of the CPU to the process selected by the scheduler, i.e. starts the selected process:\n process context switching (取出PCB) switching to user mode jumping to the proper location in the user program to restart that program  5.1.4 When Scheduling occurs？ When CPU is idle or some events occur in systems, the OS scheduler should make scheduling decision:\n process terminates the running process switches from running to waiting state (for example, I/O requests) the running process switches from running to ready state ( e.g., interrupts occurs, or the timeslot is out) (maybe) switches from waiting to ready (e.g. , completion of I/O)   Scheduling under condition 1 and 2 is nonpreemptive. Scheduling under condition 3 and 4 is preemptive\n 5.1.5 Dispatch latency (调度等待时间)  system calls: 保护现场 scheduler：选择下一个进程 dispatcher：启动下一个进程 整个t0-t3的时间被称为调度等待时间  5.2 Scheduling Criteria  CPU utilization Def：CPU运行在用户模式下的时间占比  Throughput Def: the number of processes that complete their execution per time unit Turnaround time Def: the length of the procee lifetime cycle. Waiting time Def: amount of time a process has been waiting in the ready queue. NOT waiting state!!! Response time the time it takes to start responding, not the time it takes to output the response    想达到吞吐量最大，则在进行调度时，先执行CPU burst小的进程。\n 5.3 Scheduling Algorithm 5.3.1 FCFS Implementation\n the new created process is inserted into a FIFO ready queue, i.e. inserted into the tail of the ready queue. scheduler selects the first process in the ready queue  Features\n nonpreemptive the average waiting time under FCFS is generally not minimal  5.3.2 Shortest-Job-First scheduling(SJF) Implementation\n CPU burst 最短的优先 CPU burst相同时，采用FCFS   如何预测下一个CPU burst：指数平滑估计\n Feature\n with respect to minimum average waiting time for a given set of processes, SJF is optimal  Types\n nonpreemptive  preemptive(SRTF)   5.3.3 Priority scheduling smaller interger = higher priority Types\n nonpreemptive or preemptive static or dynamic  Starvation Problems low priority processes may never execute.\n Using dynamic priority can avoid this problem.\n 5.3.4 Round Robin 时间片大约为10-100ms，选择队列中的第一个进程放入时间片，当这个时间片完成之后，将其终止并将PCB从队列头挂到队列尾。 Features\n typically, higher average turnaround than SJF, but better response time commonly used in time-sharing systems    time slice q 的值需要权衡，太大变为FCFS，太小的话context switch过于频繁，开销过大。\n If there are n processes in the ready queue and the time quantum is q, then\n each process gets 1/n of the CPU time in chunks of at most q time units at once no process waits more than (n-1)q time units  5.3.5 Mutilevel Queue The ready queue is partitioned into separate queues and different algorithms\n foreground (interactive) —— RR background (batch) —— FCFS ==优先服务前端队列==  But, starvation problems again!!!\n Mutilevel Feedback Queue 进程优先级变化：高优先级Queue $\\rightarrow$ 低优先级Queue  5.3.6 Highest Response Rate First(HRRF)  在这里的响应时间与上面的并不相同\n 响应时间 = 进程进入系统后的等待时间 + 估计计算时间(CPU burst) 响应比 = 响应时间/估计计算时间 = 1 + 等待时间/估计计算时间 特点：\n 非抢占式 SJF \u0026lt; HRRF 的平均周转时间 \u0026lt; FCFS  5.3.7 Scheduling in Real-time systems  Def: a computer and/or software system that reacts to external events in limited time intervals (deadline) before the events become obsolete (失效)   deadline 是绝对时间\n  deadline到达时，如果进程没有结束，应继续执行，不会被强行终止\n  Real-time system is ==event-driven== and ==time-critical== system external events $\\rightarrow$ interrupts(硬中断) $\\rightarrow$ scheduling among critical processes $\\rightarrow$ the critical process responsible for reacting to and processing these events in limited time ( before the deadline).\n  Dispatch latency consists of two phases:\n conflict phase 调度算法优先处理事件 dispatch phase     Scheduling in hard real-time system 硬实时，绝对不能超过DDL；通过可调度性分析判断能否得到及时处理 Scheduling in soft real-time system    5.4 Mutiple-Processor Scheduling 2级调度\n 任务分配给CPU 在CPU上调度process  5.5 Thread Scheduling 略\n 结合第五章作业与例题PPT进行复习！\n ","date":"October 12, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_3/","summary":"Chapter 5 CPU Scheduling 5.1 Concepts on Scheduling 5.1.1 basic concepts   CPU Scheduling: (短期调度) ==selecting + allocating + enabling, in kernel mode.== The procedure of selecting running entities in main memory ( i.e., processes or threads in the ready queue) according to some criteria, allocating CPU to the selected running entities, and then enabling them to run on CPU\n  CPU burst occurs: running state\n  I/O burst occurs: waiting state","tags":null,"title":"CPU Scheduling"},{"categories":null,"contents":"Chapter 3 Dynamic Programming I. 基本思想  聪明的遍历方法  为全遍历，不同于贪心算法的单一路径   关键：==自底向上== 寻找最优子结构： ==该问题的最优解包含着其子问题的最优解== 验证方法：反证法 建立递推关系  II. 矩阵连乘问题  最优解 == 以最少的数乘次数计算出矩阵连乘的乘积\n  引子想法：改进分治法 原本的分治法： 通过递归树，可以发现在递归中经常会重复计算： 思路是将重复计算的部分存储进二维数组之中，这种方法是基于分治法的改进方法  int LookupChain(int i，int j) { if (m[i][j] \u0026gt; 0) return m[i][j]; if (i == j) return 0; int u = LookupChain(i，i) + LookupChain(i+1，j) + p[i-1]*p[i]*p[j]; s[i][j] = i; for (int k = i+1; k \u0026lt; j; k++) { int t = LookupChain(i，k) + LookupChain(k+1，j) + p[i-1]*p[k]*p[j]; if (t \u0026lt; u) { u = t; s[i][j] = k; } } m[i][j] = u; return u; } 动态规划法 动态规划先从最小的子问题开始计算，==自底向上==进行合并运算。这种方法不需要递归，而且也可以避免重复计算。 (1) 分析最优子结构 原问题的最优解可以由子问题的最优解解决，对于动态规划问题，首先需要证明该问题==满足最优子结构性质==(利用反证法)   矩阵连乘问题符合最优子结构的证明： 假设$A[i:j]=A[i:k]+A[k+1:j]$是最小的计算量，且有$A[i:m]+A[m+1:j] \u0026lt; A[i:k]+A[k+1:j]$，则可以发现原$A[i:j] \u0026lt; A[i:m]+A[m+1:j]$，此时不满足假设条件，矛盾！\n (2)建立递归关系 void MatrixChain(int *p，int n，int **m，int **s) { for (int i = 1; i \u0026lt;= n; i++) m[i][i] = 0; for (int r = 2; r \u0026lt;= n; r++) // r代表了对角线，如图a  for (int i = 1; i \u0026lt;= n - r + 1; i++) { //代表了对每条对角线的数值  int j = i + r - 1; m[i][j] = m[i+1][j] + p[i-1]*p[i]*p[j]; s[i][j] = i; for (int k = i+1; k \u0026lt; j; k++) { int t = m[i][k] + m[k+1][j] + p[i-1]*p[k]*p[j]; if (t \u0026lt; m[i][j]) { m[i][j] = t; s[i][j] = k; } } } } 接下来需要从S[i][j]矩阵中得到加括号的方案：\nvoid trace(int i, int j, int **s) { if(i == j) return; trace(i, s[i][j], s); trace(s[i][j]+1, j, s); } III. 最长公共子序列   问题描述： 给定两个序列：X={ x1, x2, …..,xm}，Y={ y1, y2, ……,yn} 找出X和Y的一个最长公共子序列。\n  划分最优子结构： 利用从后向前考虑的突破口   建立递归结构 用c[i][j]记录序列和的最长公共子序列的长度   代码\n  void LCSLength(int m，int n，char *x，char *y，int **c，int **b) { int i，j; c[0][0] = 0; for (i = 1; i \u0026lt;= m; i++) c[i][0] = 0; for (i = 1; i \u0026lt;= n; i++) c[0][i] = 0; for (i = 1; i \u0026lt;= m; i++) for (j = 1; j \u0026lt;= n; j++) { if (x[i]==y[j]) { c[i][j]=c[i-1][j-1]+1; b[i][j]=1; } else if (c[i-1][j]\u0026gt;=c[i][j-1]) //此处有问题  { c[i][j]=c[i-1][j]; b[i][j]=2; } else { c[i][j]=c[i][j-1]; b[i][j]=3; } } } 扩展：求最长上升子序列  void LIS(int a[], int b[], int n) { int c[n][n], d[n][n]; for(int i = 0; i \u0026lt; n; i ++) b[i] = a[i]; quickSort(a, 1, n); LCSLength(n, a, b, c, d); } 扩展：利用最长公共子序列求解回文词的构造问题 解决思路：首先将给定的字符串 X 翻转得到它的逆串 Y，然后求 X 与 Y 的最长公共子序列，那么 X 的字符个数 n 减去最长公共子序列的长度即为将 X 变成回文串时最少需要添加的字符个数。  IV. 最大子段和 代码：\nint maxSum(int n, int *a) { int sum = 0, b = 0; for(int i = 1; i \u0026lt;= n; i++) { if(b \u0026gt; 0) b += a[i]; else b = a[i]; if(b \u0026gt; sum) sum = b; } } V. 凸多边形最优三角剖分 VI. 0-1背包问题  利用等长向量来表示状态\n ","date":"October 5, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/algorithms/algorithm_analysis-2/","summary":"Chapter 3 Dynamic Programming I. 基本思想  聪明的遍历方法  为全遍历，不同于贪心算法的单一路径   关键：==自底向上== 寻找最优子结构： ==该问题的最优解包含着其子问题的最优解== 验证方法：反证法 建立递推关系  II. 矩阵连乘问题  最优解 == 以最少的数乘次数计算出矩阵连乘的乘积\n  引子想法：改进分治法 原本的分治法： 通过递归树，可以发现在递归中经常会重复计算： 思路是将重复计算的部分存储进二维数组之中，这种方法是基于分治法的改进方法  int LookupChain(int i，int j) { if (m[i][j] \u0026gt; 0) return m[i][j]; if (i == j) return 0; int u = LookupChain(i，i) + LookupChain(i+1，j) + p[i-1]*p[i]*p[j]; s[i][j] = i; for (int k = i+1; k \u0026lt; j; k++) { int t = LookupChain(i，k) + LookupChain(k+1，j) + p[i-1]*p[k]*p[j]; if (t \u0026lt; u) { u = t; s[i][j] = k; } } m[i][j] = u; return u; } 动态规划法 动态规划先从最小的子问题开始计算，==自底向上==进行合并运算。这种方法不需要递归，而且也可以避免重复计算。 (1) 分析最优子结构 原问题的最优解可以由子问题的最优解解决，对于动态规划问题，首先需要证明该问题==满足最优子结构性质==(利用反证法)   矩阵连乘问题符合最优子结构的证明： 假设$A[i:j]=A[i:k]+A[k+1:j]$是最小的计算量，且有$A[i:m]+A[m+1:j] \u0026lt; A[i:k]+A[k+1:j]$，则可以发现原$A[i:j] \u0026lt; A[i:m]+A[m+1:j]$，此时不满足假设条件，矛盾！","tags":null,"title":"Dynamic Programming"},{"categories":null,"contents":"Chapter 3: Process 3.1 Process Concept 3.1.1 Process Definition A process is\n a program in execution the unit of resources (CPU, memory, I/O devices) allocation runs concurrently with other processes   Another definition: 具有一定独立功能的某个程序关于某个数据集合的一次运行活动。是系统进行资源分配和调度的独立单位。(又称task，job)\n 进程的特点：\n 动态性：可动态地创建、结束进程 并发性：进程可以被独立调度并占用处理机运行(并发\u0026amp;并行) 独立性：不同进程的工作不相互影响 制约性：因访问共享数据/资源或进程同步而产生制约(进程之间有交互、共享等情况) Process = program + data + PCB.   3.1.2 Process State A process is dynamic, and has its lifetime. As a process executes, it changes among the following states:  new: the process is being created. ready: the process is waiting to be assigned to a process. running: instructions are being executed. waiting: the process is waiting for some event to occur. terminated: the process has finished execution.  3.1.3 Process Control Block  Def: a data structure used for OS to describe processes, according to which OS manages processes.   操作系统用于管理控制进程的一个专门数据结构 进程表：所有进程的PCB集合\n Organization:    process state: running, waiting, etc. program counter: location of instruction to next execute CPU registers: contents of all process-centric registers CPU scheduling information: priorities, scheduling queue pointers memory-management information: locations the process resides in the memory accounting information I/O status information    这张图清楚地表示了不同进程和系统调用的转换关系，以及在操作系统kernel mode中利用PCB保存进程现场的过程。从系统效率来看，引入线程的概念会大大增加系统运行的效率，若多进程工作，这种mode的转换很浪费计算机的资源。\n 3.2 Processing Schedule 3.2.1 Scheduling Queues a series of processes waiting for being allocated some type of resources by OS schedulers, including\n ready queue the set of all processes residing in main memory, ready and waiting to execute, i.e. waiting for CPU. device queue the set of processes waiting for an I/O device. job queue\nthe set of all processes in the system.   3.2.2 Schedulers  Def:   a software module in OS responsible for allocating resources (e.g. CPU, devices) among different processes  Three types of scheduling   short–term scheduling selecting which process should be executed next and allocates CPU, that is, selecting a process in ready state and running this process on CPU.   short-term scheduler is invoked very frequently.\n  long–term scheduling selects process from the pool and loads them into memory for execution.   long-term scheduler is invoked very infrequently pool: Many processes are spooled to a mass-storage device, where they are kept for later execution.\n  图的说明：MMPD类似Degree of multiprogramming，左边是LTS，其中一共分配了23个jobs，放在jobs queue中的有8个；剩下都在STS中分配：7个在ready queue等待CPU调度(ready)，6个在device/waiting queue等待I/O操作(waiting)，而m=2个CPU正在处理jobs(running)，加起来15个。\n 补充：A process can be described as either\n I/O-bound process – spends more time doing I/O than computations, many short CPU bursts, or CPU-bound process – spends more time doing computations; few very long CPU bursts  It\u0026rsquo;s important that the long-term scheduler select a good process mix of I/O bound and CPU-bound processes.\n medium–term scheduling to control the degree of multiprogramming, guaranteeing the resources (such as main memory) demanded by all processes in memory has not overcommitted available resources.  swapping out  to reduce degree of multiprogramming and free memory or resources, removing processes in ready or running states from main memory and into swapping section on disks the process that is swapped out is delayed to execute   swapping in  reintroducing the processes on the disk into memory into memory, restarting their executing      3.2.3 Context Switch When scheduler allocate CPU to a new process, the kernel saves the context of the old process in PCB, and loads the saved context of the new process scheduled to run.\nContext-switch time is overhead！！！ the system does no useful work while switching.\n3.3 Operations on Processes 3.3.1 Process Creation  Def: OS and other processes use creation primitive /system call to create a new process Creation Step:  load the program code into main memory allocated to this process allocate resources (memory, I/O devices, files) to the process build the PCB for this process insert the PCB into ready queue, (and the process change into ready state)     Notice: The process being created is in ==new== state\n 3. 父进程与子进程\n 问题1：子进程资源分配问题  child process obtain resources directly from OS obtain resources from its parent, being constrained to a subset of the sources of its parent, such as in Linux   问题2：子进程的地址空间分配问题  the child copy address space of its parent the child has its independent address space     进程的地址空间构成： - 进程代码段 (text section) - 数据段 ( data segment) - 调用参数区与环境区的起始地址 - $\\dots$\n   问题3：子进程如何执行\n parent and children execute concurrently parent waits until children terminate    fork()： A new process is created by the fork() system call. Both processes(the parent and the child) continue execution at the instruction after the fork(), with one difference: The return code for the fork() is 0 for the new(child) process, whereas the nonzero pid of the child is returned to the parent. 子进程执行fork()以后的代码。\n  exec(): 利用exec原语，将子进程的自身代码加载到父进程的代码段中。\n  3.3.2 Process Termination  Def: OS and parent processes can use the termination primitive/ system call (e.g. exit( ) in Linux ) to terminate a child process. Termination Step:  output data from child to parent (via wait). child’s resources(e.g. devices) are de-allocated by the operating system release main memory allocated to the child remove the child’s PCB from ready queue or other queues     Notice: The process being terminated is in ==terminated== state\n abort():  the child has exceeded its usage of some of allocated resources. tasks assigned to child is no longer required. the parent is exiting, so the child should also exit  operating system does not allow child to continue after its parent terminates. cascading（串级，级联） termination.     exit(): Terminate a process. wait(): Parent process may wait for the termination of a child process. It returns the pid of a terminated child so that the parent could tell which of its possibly many children has terminated.  3.4 Interprocess Communication (IPC) A process runs concurrently with others, it may be\n independent process, which cannot affect or be affected by the execution of another process, e.g. idle cooperating process, which can affect or be affected by the execution of another process  Producer-Consumer Problem 协同要求： .\n 生产者写数据时有空缓冲块，如果不满足，则阻塞。 2个生产者不能同时向同一 空缓冲块写数据 消费者取数据时，有满缓冲块，如果不满足，则阻塞。 2个消费者不能同时从同一 满缓冲块取数据 生产者和消费者不能同时对同一缓冲块进行读写操作  3.4.1 Shared-Memory System The processes communicate with each others, by means of a region of shared memory. 用到的主要技术为==Producer-Consumer Model==。(b) in the figure is Shared-Memory System.  The unbounded buffer places no practical limit on the size of the buffer. The consumer may have to wait for new items, but the producer can always produce new items. The bounded buffer assumes a fixed buffer size. In this case, the consumer must wait if the buffer is empty, and the producer must wait if the buffer is full.  3.4.2 Message-Passing Systems A IPC mechanism provided by OS, i.e. implemented in kernel space, via system calls, such as send and receive (a) in the figure is Message-Passing Systems.\n Naming    Direct Communication For sender process P and receiver process Q, they must explicitly name each other, using two system calls.\n send (Q, message) – send a message to process Q receive(P, message) – receive a message from process P Properties of communication link links are established automatically. a link (e.g.队列) is associated with exactly one pair of communicating processes. between each pair of processes there exists exactly one link. the link may be unidirectional, but is usually bi-directional.     Indirect Communication messages are directed and received from mailboxes (or ports) in kernel space.\n each mailbox has a unique id. processes can communicate via send and receive only if they share a mailbox. Properties of communication link the link is established only if processes share a common mailbox a link (Mailbox) may be associated with many processes, i.e., ==a mailbox is not specific to a particular pair of processes.== each pair of processes may share several communication links. links may be unidirectional or bi-directional.    Synchronization   Blocking send: The sending process is blocked until the message is received by the receiving process or by the mailbox. Nonblocking send: The sending process sends the message and resumes operation. Blocking receive: The receiver blocks until a message is available. Nonblocking receive: The receiver retrieves either a valid message or a null.  Buffering  zero capacity bounded capacity unbounded capacity    3.6 Client-Server Communication  Sockets(知道) Remote Procedure Call(RPC) (掌握)    Benefits\n a way to abstract procedure calls mechanism for use in network environments independent on OS for the purpose of IPC, more convenient than I/O operations or message-passing system     Some concepts:\n Stubs (存根）  client-side proxy for the actual procedure on the server，locates the server and marshalls the parameters. server-side stub receives this message, unpacks the marshalled parameters, and performs the procedure on the server   Binding (联编) in procedure call  replace the called procedure’s name with the memory address of this procedure   Binding in RPC  how to locate the called procedure in server, i.e. for client how to know the port number of the called procedure       1.客户程序（caller process）按通常的（类似于本地的）调用方式，调用客户存根 2.客户存根创建一个消息，封装参数，并陷入内核 3.内核将该消息发送给服务器端内核 4.服务器端内核将该消息传递给服务器存根 5.服务器存根从消息中获取参数，并调用服务器程序（called process）6.服务器程序完成工作，将结果返回给服务器存根 7.服务器存根将结果打包进消息，并陷入OS内核。 8.服务器内核将消息返回给客户端内核 9.客户端内核将消息传递给客户存根 10.客户存根取出结果，返回给客户端调用程序\n Remote Method Invocation(了解)   Chapter 4 Threads 4.1 Overview 引入线程的原因：提高CPU资源利用率\n 减少内核态下的CPU switch的时间，降低管理成本 快速启动程序(分配资源、创建线程)  进程的上下文切换示意图 进程的上下文切换开销包括\n 直接开销 间接开销：==进程切换后局部性失效==  进程是资源调度的单元，线程是CPU调度单元和运行实体。\n 给线程分配CPU资源，并共享进程资源 切换、创建开销小 提高并行性  更为直观的表示进程切换和线程切换的时间开销对比  最后为不同进程的线程切换，时间开销最大\n 协程：用户级线程\n 与内核级线程的对比：协程创建的内存开销远小于内核级线程的内存开销。  注意：一个进程可以对应多个线程 thread = thread ID + program counter + a register set + stack heavyweight process =\u0026gt; 进程 lightweight process =\u0026gt; 线程\n进程内容为线程的共享内容。对于每一个线程有独立的寄存器和栈空间。 多线程内存占用及执行轨迹 4.2 Multithreading Models  User-level threads(ULT) e.g Coroutine(协程). Thread management (creating, scheduling,..) is conducted by the user-level threads library, or by programming environments.  例如 pthread 用户级线程库\nKernel-level threads(KLT) OS is responsible for creating and managing TCB  核心问题：For the ULT, how to occupy the CPU?\n mapping between user threads and kernel threads mapping is conducted by the compilers  3.Thread Scheduling Two-level scheduling for ULT\n local scheduling how the threads library decides which thread to put onto an available LWP global scheduling how the kernel decides which kernel thread to run next  ULT-to-KLT Mapping   此处PPT中有3个实例图，切记Linux没有标准线程的概念\n  Many-to-one 获得CPU概率最小，并行性最低 One-to-one 获得CPU概率最大，并行性最高，可以分配给重要线程，利用CPU绑定的方法 Many-to-many 一般情况下是用户线程数大于内核线程数  4.3 Thread Libraries A thread library provides the programmer an API for creating and managing thread.\n First approach, ULT: invoking a function in the library results in a local function call in user space Second approach, KLT: invoking a function in the API for the library results in a system call to the kernel   Linux Thread ==Linux refers to them as tasks rather than threads.== Thread creation is done through clone() system call. clone() allows a child task to share the address space of the parent task (process)   完全在用户空间模拟对线程的支持\n 4.4 Threads Issues   Thread Creation If one thread in the parent process P1 calls fork to create a new child process P2, the new process P2 will   Thread cancellation\n   asynchronous cancellation 立即停止当前线程 deferred cancellation 等待子进程的结束后再结束进程  Signal   a facility provided by OS to notify a process that a particular event occurred. event-driven mechanism   事件驱动、软件驱动、由OS处理(类似软中断)\n Two types of signal\n synchronous signals：由内部事件产生 asynchronous signals：由外部事件产生  从User mode转到Kernel mode执行信号服务程序。\nThread Pools   a set of pre-created threads, the number of threads in this set is limited used to bound the number of threads concurrently active in the system    空闲态(收到request)$\\rightarrow$激活态 激活态(收到complete)$\\rightarrow$空闲态\n Question：How to determine the number of threads in pool?\n 首先区分任务/进程类型：CPU-bound和I/O-bound。 CPU计算时间记成CT，I/O计算时间记成WT M：线程池中线程数量M，后端服务器CPU核数N  根据以上条件，可以得出M的取值为\n CPU密集型任务：M = N I/O密集型任务：M = N*(1+WT/CT)  Thread-specific data Data in thread includes   shared data (进程块信息) Thread-specific data  4.5 线程的状态 3个基本进程状态：就绪、运行、等待/阻塞 与进程的区别：\n 无挂起状态，线程不是资源拥有者，无权决定从主存中撤出 进程内某个线程阻塞时，整个进程并不阻塞，进程内其它线程仍维持原各自状态，参与调度  4.6 HT(超线程)技术  Concept   在一个物理核上模拟两个逻辑核 两个逻辑核具有各自独立的寄存器（eax、ebx、ecx、msr等等）和APIC，但共享使用物理核的执行资源，包括执行引擎、L1/L2缓存、TLB和系统总线等   性能分析 HT通过物理核上的计算资源高效复用，提高了进程/线程的整体吞吐量。性能提高大约1.3倍。 有HT竞争时的计算时间：无HT竞争时的计算时间 = 1:0.65\n  超线程技术的图解：   4.7 多核CPU技术 双核CPU的优势：双核系统平均较单核系统快==60-70%==\n  多核/HT CPU速度提升 (1) 方法一：n核m线程(保守估计) 双核性能提升1.7倍 + HT性能提升1.3倍 $$SP=[1.7 \\times (n/2)] \\times 1.3$$ (2) 方法二：n核m线程(乐观估计) 不考虑双核算作单个逻辑核时性能上的减少，单考虑HT性能提升 $$SP=1.3\\times n$$ (3) 方法三：n核n线程无HT 只考虑双核带来的性能降低 $$SP=(1.7-1.9)\\times (n/2)$$ (4) 方法四：n核m线程(带HT，非线性乐观估计) 将2核4线程的$1.3\\times1.7=2.21$作为一个整体代表双核 $$SP=[(1.7\\times1.3)^{log_2n}]$$ (5) 方法五：n核n线程(无HT，非线性保守估计) 将2核2线程的$1.7$作为一个整体代表双核 $$SP=[(1.7)^{log_2n}]$$\n  CPU亲和力和CPU绑定\n   软亲和力：内核进程调度分配到原先的CPU，不会在处理器中间频繁切换； 硬亲和力：进程固定/==绑定==在某个处理器运行   分解的子问题数目越多，利用多个CPU/计算核同时并行求解的子问题越多，算法复杂性越低，程序运行速度越快！\n 并行算法加速比 同一个任务在单处理器系统和并行处理器系统中运行消耗的时间的比率 $$SP=T_1/T_p$$ 其中$T_1$：单处理器下的运行时间，$T_p$：有p个处理器并行系统中的运行时间   当SP=P时，称为线性加速比\n 阿姆达尔定律 在固定负载情况下，加速比SP： $$SP=\\frac{1}{a+(1-a)/n}$$ 其中a：串行计算部分所占比例，n：并行处理节点个数。 极限加速比：当$n\\rightarrow\\infin$，$s\\rightarrow1/a$  ","date":"September 28, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_2/","summary":"Chapter 3: Process 3.1 Process Concept 3.1.1 Process Definition A process is\n a program in execution the unit of resources (CPU, memory, I/O devices) allocation runs concurrently with other processes   Another definition: 具有一定独立功能的某个程序关于某个数据集合的一次运行活动。是系统进行资源分配和调度的独立单位。(又称task，job)\n 进程的特点：\n 动态性：可动态地创建、结束进程 并发性：进程可以被独立调度并占用处理机运行(并发\u0026amp;并行) 独立性：不同进程的工作不相互影响 制约性：因访问共享数据/资源或进程同步而产生制约(进程之间有交互、共享等情况) Process = program + data + PCB.   3.1.2 Process State A process is dynamic, and has its lifetime. As a process executes, it changes among the following states:  new: the process is being created.","tags":null,"title":"Process \u0026 Threads"},{"categories":null,"contents":"I. 词法分析 词法分析器作用：\n 扫描源程序字符流 按照源语言的词法规则识别出各类单词符号 产生用于语法分析的记号序列 词法检查 创建符号表 \u0026mdash; 为语法分析使用 接口：跳过注释、标注错误信息  一、词法分析程序与语法分析程序的关系 1.1 词法分析程序作为独立的一遍 1.2 词法分析程序作为语法分析程序的子程序 词法分析相当于函数，语法分析每一次需要记号时即调用词法分析函数获得记号。  这种操作的好处可以避免中间文件、省去取送记号的工作。\n 1.3 词法分析程序与语法分析程序作为协同程序 这种方法不常用。\n二、词法分析程序的输入与输出 2.1 配对缓冲区  哨兵！！！(EOF)\n 把一个缓冲器分为大小相同的两半，每半各含N个字符，一般N=1KB或4KB。为了使得判断更为优化，在每个半区后面加上EOF。 这种做法是为了让缓冲区可以处理超出其范围的字符串。\n2.2 词法分析程序的输出——记号  记号：某一类单词符号的类别编码，如id，num 模式：某一类单词符号的构成规则，如“有字母开头的字母字符串” 单词：某一类单词符号的具体实例，如hello   记号的属性  作用：记号影响语法分析的决策，属性影响记号的翻译。     三、记号的描述和识别 3.1 词法与正规文法 描述语言的标识符、常数、运算符和标点符号等记号的文法\n3.2 记号的文法  标识符 描述标识符集合的正则表达式： 标识符的正规文法(右线性文法)  常数   整数 描述整数结构的正则表达式：\n  无符号数\n   运算符 分界符 关键字  四、词法分析程序的设计与实现 五、软件工具LEX II. 语法分析 一、简介  语法分析程序的任务   从源程序记号序列中识别出各类语法成分 进行语法检查  三种分析器的关系  语法分析程序的作用   输入：记号流/记号序列 工作依据：语法规则 功能：将记号组合成语法成分、语法检查 输出：分析树 错误处理  常用的分析方法：   自顶向下的分析方法 自底向上的分析方法   对输入符号串的扫描顺序：==自左向右==\n 错误恢复策略：   紧急方式恢复： 抛弃出错的语法结构，方法简单，不会陷入死循环 短语级恢复： 局部纠错，防止发生死循环 出错产生式： 扩充文法，增加产生错误的产生式 全局纠错： 时空代价太大，只有理论上的意义  二、自顶向下分析方法 1. 递归下降分析 为输入符号串建立一个最左推导序列的过程\n 文法的==每一个非终结符号对应一个递归过程==，即可实现这种带回溯的递归下降分析方法。 每个过程作为一个布尔过程，一旦发现它的某个产生式与输入串匹配，则用该产生式展开分析树，并返回true，否则分析树不变，返回false。  问题：回溯、穷举\n2. 递归调用预测分析   如何克服回溯 让每一个候选式的开始符号各不相同\n  对文法的要求\n   文法不含左递归 非终结符号A的所有候选式的首符集两两互不相交    预测分析程序的构造\n 预测分析程序的转换图 转换图的工作过程  转换图的化简 预测分析程序的实现  如果是终结符，则程序为==匹配该终结符==，输入指针移向下一个记号 如果是非终结符，则程序为==调用==该非终结符所对应的子程序，输入指针不移动 如果有两个以上的射出边，则使用==分枝语句==在程序中产生相应的分支部分      LL(1)文法 (1) FIRST集合的构造 (2) FOLLOW集合的构造 (3) LL(1)文法的定义   3. 非递归预测分析 使用一张分析表和一个栈联合控制，实现对输入符号串的自顶向下分析。\n 预测分析程序的模型及工作过程   判断LL(1)文法的标准，用第一种方法更为简单\n 关键：决定哪个产生式运用于非终结符 模型:  预测分析控制程序 ==这个程序的执行直接看作业例题。==\n  预测分析表的构造方法   预测分析方法中的错误处理示例\n   栈顶终结符与输入串的首字符不匹配： 解决方法：弹出栈顶的终结符号 寻找的分析表表项为空： 解决方法：跳过剩余输入符号串中的若干个符号，直到可以继续进行分析为止(相当于丢掉该非终结符带来的一个树分支)  带有同步化信息的分析表 ==相当于通过FOLLOW集进行判断，若表项没有就加上synch==  三、自底向上分析方法 “可归约串”是句型的“最左素短语”。\n 素短语：句型的一个短语，至少含有一个终结符号，并且除它自身之外不再含有其他更小的素短语。 最左素短语：处于句型最左边的那个素短语。  1. “移进-归约”分析方法  符号栈：存放文法符号 分析过程： （1）把输入符号一个个地移进栈中。 （2）当栈顶的符号串形成某个产生式的一个候选式时，在一定条件下，把该符号串替换(即归约)为该产生式的左部符号。 （3）重复(2)，直到栈顶符号串不再是“可归约串”为止。 （4）重复(1)-(3)，直到最终归约出文法开始符号S。  2. 规范规约 最右推导的逆过程 最右推导又称规范推导，由最右推导得到的右句型也称为规范句型，规范推导的逆过程成为规范归约/最左规约。\n规范规约的中心问题:如何寻找或确定一个右句型的句柄。\n 规范句型的特点：句柄之后没有非终结符号\n 3. “移进-归约”方法实现  移进：把下一个输入符号移进到栈顶。 归约：用适当的归约符号去替换这个串。 接受：宣布分析成功，停止分析。 错误处理：调用错误处理程序进行诊断和恢复。  两种冲突：\n “移进-归约”冲突 “归约-归约”冲突  4. LR分析方法 4.1 LR分析程序  LR分析程序的模型    这里需注意栈的存放方式是底$S_0X_1S_1…X_mS_m$顶\n 分析表的结构   状态转换表$goto[S_m，X]$ 动作表$action[S_m，a_i]$  构造LR分析表的三种技术   简单的LR方法（SLR)：易于实现，功能较弱 规范的LR方法（LR(1))：功能最强，代价最大 向前看的LR方法（LALR)：功能和代价介于两者之间，能分析大多数程序设计语言的结构，并且能比较有效地实现  LR分析控制程序   活前缀：一个规范句型的一个前缀，如果不含句柄之后的任何符号，则称它为该句型的一个活前缀。    该程序具体执行结合作业和例子进行复习\n 4.2 LR分析表构造  SLR(1)分析表的构造 中心思想：为给定的文法构造一个识别它所有==活前缀==的DFA  ","date":"September 22, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/compiler/compilers_1/","summary":"I. 词法分析 词法分析器作用：\n 扫描源程序字符流 按照源语言的词法规则识别出各类单词符号 产生用于语法分析的记号序列 词法检查 创建符号表 \u0026mdash; 为语法分析使用 接口：跳过注释、标注错误信息  一、词法分析程序与语法分析程序的关系 1.1 词法分析程序作为独立的一遍 1.2 词法分析程序作为语法分析程序的子程序 词法分析相当于函数，语法分析每一次需要记号时即调用词法分析函数获得记号。  这种操作的好处可以避免中间文件、省去取送记号的工作。\n 1.3 词法分析程序与语法分析程序作为协同程序 这种方法不常用。\n二、词法分析程序的输入与输出 2.1 配对缓冲区  哨兵！！！(EOF)\n 把一个缓冲器分为大小相同的两半，每半各含N个字符，一般N=1KB或4KB。为了使得判断更为优化，在每个半区后面加上EOF。 这种做法是为了让缓冲区可以处理超出其范围的字符串。\n2.2 词法分析程序的输出——记号  记号：某一类单词符号的类别编码，如id，num 模式：某一类单词符号的构成规则，如“有字母开头的字母字符串” 单词：某一类单词符号的具体实例，如hello   记号的属性  作用：记号影响语法分析的决策，属性影响记号的翻译。     三、记号的描述和识别 3.1 词法与正规文法 描述语言的标识符、常数、运算符和标点符号等记号的文法\n3.2 记号的文法  标识符 描述标识符集合的正则表达式： 标识符的正规文法(右线性文法)  常数   整数 描述整数结构的正则表达式：\n  无符号数\n   运算符 分界符 关键字  四、词法分析程序的设计与实现 五、软件工具LEX II.","tags":null,"title":"Lexcial Analysis \u0026 Parsing"},{"categories":null,"contents":"Chapter 1 I. 算法及算法复杂度 1. Definition  Input Output Definiteness Finiteness Effectiveness   note: program vs algorithm program: A program is written in some programming language, and does not have to be finite. algorithm: An algorithm can be described by human languages, flow charts, some programming languages, or pseudo-code.\n 2. 算法的评价  正确性 健壮性 复杂性  时间复杂度 空间复杂度   可读性 简单性  II. 算法复杂度分析 1. 指标  平均时间复杂度 最坏时间复杂度  2. 渐进性复杂度分析 基于渐进性原理，可以对两个算法复杂度进行分析。对于求出算法的复杂度，只要求出算法的阶，便可以确定其复杂度。\n Asymptotic Notation    此处对于各种阶的形象描述，请看PPT对应内容 大多数情况下，我们分析算法是用大O阶，少数情况用大$\\Omega$阶\n 3. 递归方程渐近阶的求解 (1) 代入法 对递推关系式估测一个上限，再用数学归纳法证明其正确(高手使用)\n(2) 迭代法 \u0026mdash; 较为常用 画图方法 (3) 套用公式法 该方法非常不常用，因为有很多的限制条件！！！ (4) 用特征方程解递归方程 \u0026mdash; 考点   解题原理： 1）求解特征方程的根，得到递归方程的通解 2）利用递归方程初始条件，确定通解中待定系数，得到递归方程的解\n  考虑2种情况： 1）特征方程的k个根不相同 2）特征方程有相重的根\n  4. 证明阶关系 \u0026mdash; 考点 利用$O$阶及$\\Omega$阶的定义来进行证明： Chapter 2 I. Divide and Conquer  Balancing 平衡子思想：尽量划分成两个规模相同的子问题  1. Binary Search Code int binarySearch(int a[], int x, int n) { int left = 0, right = n-1; while(left \u0026lt;= right) { int middle = (left + right) / 2; if(a[middle] == x) return middle; else if(a[middle] \u0026lt; x) left = middle + 1; else right = middle - 1; } return -1; }  分析：算法复杂度：$O(logn)$  之所以可以减少复杂度，是由于在每次划分完子问题之后，我们只对一个子问题进行求解，不去处理另一个子问题(抛弃)，所以会效率会更高。\n思考题：利用分治法求出数组的最大值和最小值\n 结束条件：  若子数组长度为1，则该子问题的最大最小值已然确定； 若子数字长度为2，则该子问题的最大最小值可以一步确定下来；   递归过程： 将数组分成左右两个部分，分别得出左边和右边的最大最小值，然后进行比较得出哪个最大值更大，哪个最小值最小。 代码部分：  void findMaxandMin(int a[], int left, int right, int \u0026amp;max, int \u0026amp;min) { if(left == right){ min = a[left]; max = a[right]; return ; } else if(left + 1 == right){ if(a[left] \u0026gt; a[right]){ min = a[right]; max = a[left]; } else { min = a[left]; max = a[right]; } return ; } else { int left_min, left_max, right_min, right_max; int mid = (left + right) / 2; findMaxandMin(a, left, mid-1, left_min, left_max); findMaxandMin(a, mid, right, right_min, right_max); if(left_min \u0026gt; right_min) min = right_min; else min = left_min; if(left_max \u0026gt; right_max) max = left_max; else max = right_max; } } 2. 大整数乘法 3. Strassen 矩阵乘法 4. 排序  评价排序的标准：  稳定性 复杂度   直接插入排序  void insertSort(int a[], int n) { for(int i = 0; i \u0026lt; n; i++) { int temp = a[i]; int j = i-1; while(j \u0026gt;= 0 \u0026amp;\u0026amp; temp \u0026lt; a[j]) { a[j+1] = a[j]; j--; } a[j+1] = temp; } } 归并排序  稳定排序    //垃圾版 //渐进O(nlogn) void mergeSort(int a[], int left, int right) { if(left != right){ int mid = (left + right)/2; mergeSort(a, left, mid); //这个过程有必要吗？  //当然是没必要把n个元素的数组划分为n个只有1个元素的数组  mergeSort(a, mid+1, right); merge(a, left, mid, right); } } //升级版 //真正的O(nlogn)  void merge(int a[], int b[], int left, int mid, int right) { int i = left, j = mid + 1, k = 0; while(i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= right){ if(a[i] \u0026lt;= a[j]) b[k++] = a[i++]; else b[k++] = a[j++]; } if(i \u0026gt; mid){ for(int m = j; m \u0026lt;= right; m++) b[k++] = a[m]; } else{ for(int m = i; m \u0026lt;= mid; m++) b[k++] = a[m]; } } 快速排序  int partition(int A[], int p, int q) { } void quickSort(int A[], int p, int r) { if(p \u0026lt; r){ q = partition(A, p, r); quickSort(A, p, q-1); quickSort(A, q+1, r); } }  复杂度分析：  最坏时间复杂度：退化成插入排序，$O(n^2)$ 平均时间复杂度：$O(nlogn)$    5. 应用  最小线性表选择 利用快速排序的Partition函数，通过中间值来寻找第k小元素。  int partition(int A[], int p, int q) { //快排的partition } void search(int A[], int p, int r, int k) { int mid = partition(A, p, r); int i = mid - p + 1; //得算一下距离  if(k \u0026gt; i) search(A, mid+1, r, k-i); else search(A, p, mid, k); } 若想在线性的时间内$O(n)$的时间内完成算法，需要让每一次Partition的返回值尽量在Pivot位置。下面是改进的思路： 最接近点对问题    问题提出：给定平面上n个点找其中的一对点，使得在n个点的所有点对中该点对的距离最小。\n  分治法的思路\n 划分成子规模点集 S1 和 S2 找到 S1 和 S2 的最小距离 d。 合并 S1 和 S2 ：并利用 d 在 S1 的 (md,m 和 S2 的 m,m+d 中找最大点和最小点，即 p3 和 q3 。选出最小距离，完成合并    对于二维平面的时候，一维空间的合并方法不再适用，下面是二维空间的合并方式： 伪代码如下所示： ","date":"September 21, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/algorithms/algorithm_analysis-1/","summary":"Chapter 1 I. 算法及算法复杂度 1. Definition  Input Output Definiteness Finiteness Effectiveness   note: program vs algorithm program: A program is written in some programming language, and does not have to be finite. algorithm: An algorithm can be described by human languages, flow charts, some programming languages, or pseudo-code.\n 2. 算法的评价  正确性 健壮性 复杂性  时间复杂度 空间复杂度   可读性 简单性  II. 算法复杂度分析 1. 指标  平均时间复杂度 最坏时间复杂度  2.","tags":null,"title":"Complexity \u0026 Divide and Conquer"},{"categories":null,"contents":"Chapter 1: Introduction 1.1 OS Definition A program that acts as an intermediary between a user of a computer and the computer hardware.\nComputer System Structure 1.1.1 User View  ease of use Do not care about resource utilization!  1.1.2 System View  resourse allocator control program  1.2 Computer-System Organization bootstrap program:  Typically stored in ROM or EPROM, generally known as firmware. Initializes all aspects of system. Loads operating system kernel and starts execution.  Concurrent execution of CPUs and devices competing for memory cycles. To ensure orderly access to the shared memory, a memory controller is provided whose function is to synchronize access to the memory. 1.2.1 Interrupt An operating system is interrupt driven. Interrupt transfers control to the interrupt service routine generally, through the interrupt vector, which contains the addresses of all the service routines. More recent architectures store the return address on the system stack.\n Hardware interrupt: Caused by hardware interrupt signals, trigged by external events Software interrupt: Caused by privilege instructions, traps may trigger soft interrupts different type of interrupt  polling vectored interrupt system    1.2.2 Storage Structure 1.2.3 I/O Structure  Device Controller: maintains some local buffer storage and a set of special-purpose registers. It is responsible for moving the data between the peripheral devices that it controls and its local buffer storage. Device Driver: understands the device controller and presents a uniform interface to the device to the rest of the operating system. Device Controller informs the Device Driver via an interrupt that it has finished its operation. DMA  Used for high-speed I/O devices able to transmit information at close to memory speeds Device controller transfers blocks of data from buffer storage directly to main memory without CPU intervention Only one interrupt is generated per block, rather than the one interrupt per byte    1.3 Computer-System Architecture 1.3.1 Single-Processor Systems 1.3.2 Multiprocessor Systems Concurrent（并发） vs. Parallel（并行）  Concurrent: concurrent execution can be accomplished on a single processor or by using time-sharing techniques, such as dividing programs into different tasks or threads of execution, or by using multiple processors. Parallel: use of multiple computers/processors to solve problems. Multiple programs simultaneously runs on several computers/processors.  Advantages:  Increased throughput Economy of scale Increased reliability – graceful degradation or fault tolerance  asymmetric multiprocessing Each processor is assigned a specific task.\nsymmetric multiprocessing(SMP) Each processor performs all tasks within the operating system.\n1.3.3 Clustered Systems two or more individual computers (e.g. PC, servers, workstation), coupled together, connected via LAN, to provides high performance computing (HPC) and high reliability.\n as one kind of popular distributed systems\n 1.4 Operating-System Structure 1.4.1 Multiprogramming  Multiprogramming organizes jobs (code and data) so CPU always has one to execute A subset of total jobs in system is kept in memory One job selected and run via job scheduling When it has to wait (for I/O for example), OS switches to another job   1.4.2 Time Sharing Timesharing (multitasking) is logical extension in which CPU switches jobs so frequently that users can interact with each job while it is running, creating interactive computing. CPU times are divided into timeslot (时间片)，each user/process/job/task occupies one timeslot alternatively.  Features:  多路性 交互性 独占性    1.5 Operating-System Operations CPU modes: kernel mode, user mode Trap: a software-generated interrupt caused by an error or a request from user program that a OS service be performed. e.g System call.  1.5.1 Dual-mode operation and privileged instructions  Mode bit: indicate the current operation mode  0: kernel/monitor/supervisor/system/privileged mode, in which OS and privileged instructions are running on CPU; 1：user mode, in which user programs are executing on CPU.    两种mode的切换方式：  notice: 1.if user programs in user mode attempt to execute privileged instructions, the hardware does not execute these instructions, but rather treats them as illegal operations. 2.User programs can execute system calls, a kind of special privileged instructions, to request the system to perform some designated task that only OS be permitted to do. A trap is generated in this process.\n 1.5.2 Timer The timer is usually implemented by the fixed-rate clock and the counter\n clock ticker, 1ms clock, 10-bit counter, 1024ms clock  1.10 Distributed System  Def: a form of information processing in which work is performed by separate computers linked through communication network. Advantages:  resources sharing computation speed up – load sharing reliability communications     note: Required networking infrastructure: Local area networks (LAN) or Wide area networks (WAN, WWW)\n features  task allocation communications via networks — allowing different processors on different computers to exchange information process synchronization load sharing distributed file systems — files sharing across the network    1.11 Specific-Purpose Systems  Real-Time System Embedded System  1.12 Computing Environments   Traditional computing\n  Client-Server Computing\n  Peer-to-Peer Computing\n  Web-Based Computing\n  Chapter 2: OS Structure 2.1 OS Services 2.1.1 Definition Operating systems provide an environment for execution of programs and services to programs and users.\n2.1.2 A view of Services  The following 6 services are user-oriented services.\n  User Interfaces  command-line interface batch interface (off-line interface, 作业控制接口) graphical user interface (GUI)   program execution  system capability to load a program into memory and to run it   I/O operations  since user programs cannot execute I/O operations directly, the operating system must provide some means to perform I/O   file-system manipulation  capability to read, write, create and delete files   communications  exchange of information between processes executing either on the same computer or on different systems tied together by a network   error detection  ensure correct computing by detecting errors in the CPU and memory hardware, in I/O devices or in user programs     The following 3 services are for ensuring efficient system operations.\n resource allocation  allocating resources to multiple users or multiple jobs running at the same time   accounting  keep track of and record which users use how much and what kinds of computer resources for account billing or for accumulating usage statistics   protection and security  ensuring that all access to system resources is controlled    2.2 User Operating-System Interfaces  Command interpreter  Shells   GUI  2.3 System Call ","date":"September 21, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/operating-system/operating_system_concepts_1/","summary":"Chapter 1: Introduction 1.1 OS Definition A program that acts as an intermediary between a user of a computer and the computer hardware.\nComputer System Structure 1.1.1 User View  ease of use Do not care about resource utilization!  1.1.2 System View  resourse allocator control program  1.2 Computer-System Organization bootstrap program:  Typically stored in ROM or EPROM, generally known as firmware. Initializes all aspects of system. Loads operating system kernel and starts execution.","tags":null,"title":"Introduction \u0026 OS Structure"},{"categories":null,"contents":"Chapter 1 Introduction 1.1 DB, DBMS, DBS, DBAS  DB:  a collection of interrelated data stored in systems as files   DBMS:  a set of programs to access the data in DB   DBS: users + DBMS + DB   1.2 View of Data 1.2.1 Levels of Data Abstraction  View Level:  how the data items in DB are used by different users   Logical Level:  e.g. Relational Table   Physical Level:  storage structure and access methods, such as index, physical blocks, access methods for secondary memory.    1.2.2 Instances and Schemas  Instances: The collection of information stored in the database at a particular moment. Schemas: The overall design of the database.  1.2.3 Data Model DBS中对数据组织与管理/使用方式的抽象描述，包括\n 数据组织的语法定义，如数据项、数据项间的联系 数据组织的语义定义，如完整性约束 数据操作 (note: only in some data models, e.g. relational data model)   Relational Data Model All the data is stored in various tables.  Entity-Relationship model Using the entity and the relationship to model the object and the association among objects    E-R Diagram\n 1.2.4 Data Independence  逻辑模式→内模式/物理模式间的映射与physical data independence 外模式→逻辑模式间的映射与logical data independence  上述两者均通过调整映射关系保证上级不发生改变\n1.4 Database Language   DML: DML is the language that enables users to access or manipulate the data as organized by the appropriate data model Two types of DML：\n procedural (过程性) – user specifies what data is required and how to get those data nonprocedural (说明性，declarative) – user specifies what data is required without specifying how to get those data    DDL：Data definition language\n  DCL: Data Control language\n  1.5 DB Access for Application Programs Chapter 2 Introduction to the Relational Model 2.1 Structure of Relational Databases  Basic structure Attributes (columns) tuples (rows)  2.2 Database schema  Relation schema Relation instance  2.3 Keys (键、码)   super key: a subset of R where no two distinct tuples in relation r(R) have the same values on all attributes in K.\n  candidate key: minimal super key(可以为空)\n primary attribute: the attributes in the candidate keys non-primary attribute:    primary key: 主键(不可以为空)\n  主键约束：主键中的数值互不相同 关系表主键的选择方式：尽量选择数值型的候选键\n  foreign key(外键关联的概念)：在本表r1中不为候选键，但在别的表r2中为主键(外键参照关系)\n 参照关系和被参照关系 关系模式图 外键参照性约束：在参照关系表中出现的外键必须在被参照关系表中出现   note: 在进行实验的时候，首先先导入被参照关系表，再建立参照关系表。\n 2.4 Schema Diagrams A database schema, along with primary key and foreign key dependency.\n node schema, attributes, primary key directed arc foreign key dependency  2.5 Relational Query Language  non-procedural fundamental operations  Selection, Projection, Natural join, Cartesian Product(意义不大, 使用where条件可以避免笛卡尔积), Union.   additive operations extended operations  Chapter 3 Introduction to SQL 3.2 Data Definition 3.2.1 Domain Types Def 属性的类型，看看ppt即可\n3.2.2 Schema Definition  Create  Insert Delete Delete vs Drop Delete：删掉关系模式中的所有元组，但并不删掉schema Drop：不仅删掉关系模式中的所有元组，还删掉schema Alter  alter table r drop A  unique key : 候选键声明方式，值唯一  3.3 Basic structure — Select query 3.3.1 Where clause 3.3.2 From clause 3.3.3 Natural Join 3.3.4 The rename operation Other:\n string operation: %aaa%, _ _ _ order by: asc(default) or desc between distinct   Some Example: 3.5 Set Operations  Union Intersect except  Classical Example: 3.7 Aggregate Functions  avg: average value, min: minimum value, max: maximum value, sum: sum of values, count: number of values \u0026mdash; include value NULL   Aggregate with Grouping    Having clause predicates in the having clause are applied after the formation of groups whereas predicates in the where clause are applied before forming groups.  3.8 Nested Subquries The subquery is often nested in the where clause/having clause, from clause\n3.8.1 Set Membership 3.8.2 Set Comparison – “some” Clause some: all: ","date":"September 15, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/database-system/database_system_1/","summary":"Chapter 1 Introduction 1.1 DB, DBMS, DBS, DBAS  DB:  a collection of interrelated data stored in systems as files   DBMS:  a set of programs to access the data in DB   DBS: users + DBMS + DB   1.2 View of Data 1.2.1 Levels of Data Abstraction  View Level:  how the data items in DB are used by different users   Logical Level:  e.","tags":null,"title":"Database System Lecture Note 1"},{"categories":null,"contents":"I. CNN Structure Overview II. Convolution  Note: 1.Every elements in filter are the network parameter to be learned. 2.Stride means each step you walk from previous position. 3.The size of filter is decided by programmer.\n From the picture we could know the largest values of Feature Map means there has a feature. Then we do the same process for every filter and generate more Feature Map.\nIf we deal with colorful image, we will use Filter Cube instead of matrix.\nThen there is a question: What is the relationship between Convolution operator and Fully Connected ? We expand the matrix into the input of the neural network. Each color of line correspond with the weight. It just like a fully connected neural network but dropout some weight. Therefore we have less parameters!\n 这里请注意每一个输出对应连接的weight是共享的（也就是一致的），从图里看出共享weight的颜色是相同的。\n III. Max Pooling We divide the feature map into small partition, and select the largest element in every partition. Then we use these elements to compose a new feature map. This operation is called Max Pooling.\n Note: Each filter is a channel. The number of the channel is the number of filters.\n 在这里着重强调一下，每一次进行重复卷积池化的时候，filter是不一样的。第一次卷积池化时filter是二维向量，但到了第二次卷积池化时，feature map作为新的输入已然是cube的形状，此时并不会需要更多数量的二维向量filter，而是会使用cube形状的filter进行内积操作。\nIV. One Example 请注意这里每一次filter的参数数量，在第一级是25个$3\\times3$的矩阵，所以每个filter参数为9；而在第二级是50个$50\\times3\\times3$的立方体，每个filter参数为225。Max Pooling的size为$2\\times2$，所以每经过一次池化feature map的size就会减少一半。(若是行列为奇数，则自动丢弃多出的行/列)。\nMore Application 对于这种Spectrogram的图像识别而言，运用CNN的技术可以实现对某些task的识别。由于在CNN之后会接上LSTM等架构，导致在CNN模块中对时间域的提取并不会提高模型的性能，所以一般filter只会在频域中进行卷积。 ","date":"September 14, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/cnn/","summary":"I. CNN Structure Overview II. Convolution  Note: 1.Every elements in filter are the network parameter to be learned. 2.Stride means each step you walk from previous position. 3.The size of filter is decided by programmer.\n From the picture we could know the largest values of Feature Map means there has a feature. Then we do the same process for every filter and generate more Feature Map.\nIf we deal with colorful image, we will use Filter Cube instead of matrix.","tags":null,"title":"Convolutional Neural Network"},{"categories":null,"contents":"I. Basic Concepts 1. Fully Connected Feedforward Network 2. Matrix Operation Every layer has weight matrix and bias matrix, using matrix operation we can accumulate the output matrix $y$.  Tips: Using GPU could speed up matrix operation.\n II. Why Deep Learning? 1. Modularization 对neural network而言，并不是神经元越多越好，通过例子可以看出层数的增加（more deep）对于准确率的提升更有效果。这其中就是 Modularization 的思想。For example, while you are trying to train the model below, you can use basic classifiers as module. Each basic classifier can have sufficient training examples. Therefore those classifiers can use output from the basic classifiers, which can be trained by little data. What you should notice is that the modularization is automatically learned form data.\nIII. Backpropagation 1. Concept An Efficient way to compute gradient descent $\\partial{L}/\\partial{\\omega}$ in neural network.\n2. Detailed Process The partial of the loss function can be presented as follow: According to Chain rule, we need to compute two partials: Forward pass and Backward pass.   Forward pass $\\partial{z}/\\partial{\\omega} = $ The value of the output connected by the weight.   Backward pass To compute $\\partial{C}/\\partial{z}$, we need to figure out following neurons' partial.   case 1: Output Layer When we arrive at the output layer of the network, these two partials: $\\partial{C}/\\partial{z'} = \\partial{C}/\\partial{y_1}$ and $\\partial{C}/\\partial{z''} = \\partial{C}/\\partial{y_2}$ could be solved pretty easy.   case 2: Not Output Layer We could compute $\\partial{C}/\\partial{z}$ recursively, until we reach the output layer. Then we start to backward pass. It seems like a new backward neural network (red line in the graph).     The whole process of backpropogation is shown as following graph: IV. Tips for DL 1. New activation function  Vanishing Gradient Descent This phenomenon is mainly caused by the particular activation function: sigmoid funcion. From the analysis above when given a large $\\Delta{\\omega}$ as an input, the output from sigmoid function is become smaller. After going through many layers, the gradient goes to a small value, also called vanished.  (1) ReLU (2) Maxout  ReLU is a special case of Maxout\n For Maxout function, we can design the form of activation funcion by our own.  Activation function in maxout network can be any piecewise linear convex function How many pieces depending on how many elements in a group  Question 1: How to train Maxout network?\nAnswer: 对于每一个神经元，他们先比大小取出较大$z_i$作为输入，再将其对应的linear function拿去作微分\nQuesion 2: 有些output被舍去之后，back propogation无法再对其进行训练？\nAnswer: We can train different thin and linear network for different example.\n2. Adaptive Learning Rate  tips: 实际上对于local minima, 并不会存在过多，local minima的出现需要每一个dimension在附近都会产生local minima。假设一个dimension在这里出现local minima的概率为p，由于神经网路需要非常多的parameters，local minima出现的概率很低。\n 3. Early Stopping Using Validation Set to simulate testing set, then we could find where to stop properly. 4. Regularization New loss function to be minimized:  Find a set of weight not only minimizing original cost but also close to zero.  L2 regularization (二范数)   Weight Decay (权重衰减) 对于一些长期不需要用到的weight，它的值会随着训练次数接近0，这种模式接近于人脑神经元的构造方式。\n  L1 regularization L1 每次减掉固定的值，使得每次迭代后保留很多较大的值，得到的结果会比较稀疏，有很多接近于0，也有很多较大的值；然而对于L2，平均值较小。  5. Dropout The amazing point of dropout:  当你将测试集送入每一个minibatch训练的neural network，再将所有output求average，结果跟将测试集送入完整(未dropout，将所有weight乘以1-p%)的neural network得出来的结果是近似的。这点在线性模型中很容易解释，但对于非线性模型依旧有这个特性。所以Relu+dropout和Maxout+dropout的效果都会不错。\n ","date":"August 10, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/introduction-to-deep-learning/","summary":"I. Basic Concepts 1. Fully Connected Feedforward Network 2. Matrix Operation Every layer has weight matrix and bias matrix, using matrix operation we can accumulate the output matrix $y$.  Tips: Using GPU could speed up matrix operation.\n II. Why Deep Learning? 1. Modularization 对neural network而言，并不是神经元越多越好，通过例子可以看出层数的增加（more deep）对于准确率的提升更有效果。这其中就是 Modularization 的思想。For example, while you are trying to train the model below, you can use basic classifiers as module. Each basic classifier can have sufficient training examples.","tags":null,"title":"Introduction to Deep Learning"},{"categories":null,"contents":"Classification I. Probabilistic Generative Models 1. Detailed Process The basic idea is estimating the probabilities form training data. Let\u0026rsquo;s consider the two classes case: First of all, we need to figure out prior class probabilities $P(C_k)$. It\u0026rsquo;s pretty easy to find that $P(C_k) = \\frac{SizeOf C_k}{SizeOf Training Data}$ Then the task is to find out $P(x|C_k)$. Each data is represented as a vector by its attribute, and it exist as a point in a multidimensional space. We assume the points are sampled from a Gaussian distribution. Once we have specified a parametric functional form for the class-conditional densities $P(x|C_k)$, we can then determine the values of the parameters, together with the prior class probabilities $P(C_k)$, using maximum likelihood. This requires a data set comprising observations of x along with their corresponding class labels. The parameters of Gaussian Distribution are mean $\\mu$ and covariance matrix $\\Sigma$. The Gaussian with any mean $\\mu$ and covariance matrix $\\Sigma$ can generate these points. In this example, we assume $x^1, x^2, \\dots, x^{79}$ generate from the Gaussian ($\\mu^{}$, $\\Sigma^{}$) with the maximum likelihood. Now we come back to the formula given before, all the value of probabilities are figured out and we could substitute in to get this result. 2. Modifying Model While we are training the model, we normally choose the same covariance matrix $\\Sigma$ for each Gaussian Distribution. The reason for doing this is to have less parameters for the training model, also from the result we can see the accuracy has increased (Maybe just in this case). 3. Summary 4. Tips  Posterior Probability \u0026amp; Sigmoid function  Posterior Probability \u0026amp; Logistic Regression   II. Logistic Regression 1. Detailed Process Step 1: Function Set Step 2: Goodness of Function Cross entropy (交叉熵) : 如果两个分布相同，则交叉熵为0。In logistic regression we have training data $(x^n, \\hat{y}^n)$, where $\\hat{y}^n$: 1 for class 1, 0 for class 2. Then we have the loss function as below: Step 3: Gradient Descent The update function of parameter $\\omega$ is: 2. Discriminative vs Generative  The benefits of Generative model  With the assumption of probability distribution, less training data is needed; With the assumption of probability distribution, more robust to the noise; Priors and class-dependent probabilities can be estimated from different sources.  关于第三点需要着重解释一下。对于大部分神经网络都是判别模型，这种模型也确实会有更高的准确率。但对于类似语音识别的task中，priors和class-dependent probabilities是可以分开计算的，例如先验概率是由海量的网上数据计算而得，而class-dependent probabilities是根据语音数据计算得来。\n     3. Multi-class Classification 这种对 $\\hat{y}$ 的编码方式称为独热编码 (One-Hot Encoding)\n4. Limitation of Logistic Regression   XOR Problem can\u0026rsquo;t be solved by Logistic Regression.\n  Solution\n  Feature Transformation It\u0026rsquo;s not always easy to find a good transformation. (与SVM中的Kernel函数类似)\n  Cascading logistic regression models 中间的变换为分线性变换，使得在后续可分。     ","date":"July 28, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/machine-learning/classification/","summary":"Classification I. Probabilistic Generative Models 1. Detailed Process The basic idea is estimating the probabilities form training data. Let\u0026rsquo;s consider the two classes case: First of all, we need to figure out prior class probabilities $P(C_k)$. It\u0026rsquo;s pretty easy to find that $P(C_k) = \\frac{SizeOf C_k}{SizeOf Training Data}$ Then the task is to find out $P(x|C_k)$. Each data is represented as a vector by its attribute, and it exist as a point in a multidimensional space.","tags":null,"title":"Classification"},{"categories":null,"contents":" Some Notation: $\\theta_t$: model parameter at time step t $\\nabla$$L(\\theta_t)$ or $g_t$: gradient at $\\theta_t$, used to compute $\\theta_{t+1}$ $m_{t+1}$: momentum accumlated from time step 0 to time step t, which is used to compute $\\theta_{t+1}$\n I. Adaptive Learning Rates In gradient descent, we need to set the learning rate to converge properly and find the local minima. But sometimes it\u0026rsquo;s difficult to find a proper value of the learning rate. Here are some methods to introduce which can automatically produce the adaptive learning rate based on the certain situation.\n Popular \u0026amp; Simple Idea: Reduce the learning rate by some factor every few epochs (eg. $\\eta^t=\\eta/\\sqrt{t+1}$) Adagard: Divide the learning rate of each parameter by the root mean square of its previous derivatives When $\\eta = \\frac{\\eta^t}{\\sqrt{t+1}}$, we could derive that:   II. Stochastic Gradient Descent SGD update for each example, so it would converge more quickly.\nIII. Limitation of Gradient Descent  Stuck at local minima ($\\frac{\\partial L}{\\partial \\omega}$ = 0) Stuck at saddle point ($\\frac{\\partial L}{\\partial \\omega}$= 0) Very slow at plateau: Actually this is the most serious limitation of Gradient Descent.  So there are other methods perform better than gradient descent.\nIV. New Optimizers  notice: on-line: one pair of ($x_t$, $\\hat{y_t}$) at a time step off-line: pour all ($x_t$, $\\hat{y_t}$) into the model at every time step\n 1. SGD with Momentum (SGDM) Previous gradient will affect the next movement. $v^i$ is actually the weighted sum of all the previous gradient: $\\nabla L(\\theta^0), \\nabla L(\\theta^1), \\dots \\nabla L(\\theta^{i-1})$ Use SGDM could erase the limitation of $\\nabla L(\\theta) = 0$. For momentum, you could imagine it as physical inertia. 2. Root Mean Square Propagation (RMSProp) This optimizer is based on Adagrad, but the main difference is the calculation of the denominator. The reason for this new calculation is to avoid the situation that the learning rate is too small from the first step. In other words, exponential moving average(EMA) of squared gradients is not monotonically increasing. 3. Adam Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. Adam realizes the benefits of both AdaGrad and RMSProp. 4. Adam vs SDGM  Adam: fast training, large generalization gap, unstable SGDM: stable, little generalization gap, better convergence  5. Supplement   ASMGrad (ICLR'18)   AdaBound (ICLR'19)  note: Clip(x, lowerbound, upperbound)\n   SWATS (arXiv'17) Begin with Adam(fast), end with SGDM   RAdam (Adam with warmup, ICLR'20)   RAdam vs SWATS   To be continued \u0026hellip;\n","date":"July 25, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/optimizer-for-dl/","summary":"Some Notation: $\\theta_t$: model parameter at time step t $\\nabla$$L(\\theta_t)$ or $g_t$: gradient at $\\theta_t$, used to compute $\\theta_{t+1}$ $m_{t+1}$: momentum accumlated from time step 0 to time step t, which is used to compute $\\theta_{t+1}$\n I. Adaptive Learning Rates In gradient descent, we need to set the learning rate to converge properly and find the local minima. But sometimes it\u0026rsquo;s difficult to find a proper value of the learning rate.","tags":null,"title":"Optimization for Deep Learning"},{"categories":null,"contents":"Greeting! This is an introduction post. This post tests the followings:\n Hero image is in the same directory as the post. This post should be at top of the sidebar. Post author should be the same as specified in author.yaml file.  ","date":"June 8, 2020","hero":"/posts/introduction/hero.svg","permalink":"https://ztqakita.github.io/posts/introduction/","summary":"Greeting! This is an introduction post. This post tests the followings:\n Hero image is in the same directory as the post. This post should be at top of the sidebar. Post author should be the same as specified in author.yaml file.  ","tags":null,"title":"Introduction"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"}]
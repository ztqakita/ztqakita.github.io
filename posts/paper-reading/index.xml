<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper Reading on ztqakita's Blog</title><link>https://ztqakita.github.io/posts/paper-reading/</link><description>Recent content in Paper Reading on ztqakita's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 23 Sep 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://ztqakita.github.io/posts/paper-reading/index.xml" rel="self" type="application/rss+xml"/><item><title>[CN] Few-Shot Class-Incremental Learning</title><link>https://ztqakita.github.io/posts/paper-reading/few-shot_class_incremental_learning/</link><pubDate>Thu, 23 Sep 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/paper-reading/few-shot_class_incremental_learning/</guid><description>Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12183-12192
Abstract FSCIL: It requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. TOPIC: Topology-preserving knowledge incrementer framework, which is proposed to mitigates the forgetting of the old classes by stabilizing NG’s topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples.</description></item><item><title>[CN] Continuous-attractor Neural Network</title><link>https://ztqakita.github.io/posts/paper-reading/continuous_attractor_nn/</link><pubDate>Mon, 30 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/paper-reading/continuous_attractor_nn/</guid><description>Si Wu, Kosuke Hamaguchi, and Shun-ichi Amari. “Dynamics and computation of continuous attractors.” Neural computation 20.4 (2008): 994-1025.
本篇参考吴思老师公众号的文章《【学术思想】连续吸引子神经网络：神经信息表达的正则化网络模型》，并根据冷泉港亚洲暑期学校的讲座和智源学术沙龙的报告来详细阐释CANN的工作原理，全文用中文行文。
连续吸引子神经网络的数学模型 吸引子指的是一个动力学系统在不接受外界输入情况下靠自身动力学就能维持的非静息的稳定状态（active stationary state）。要构成一个的吸引子网络，需要两个基本条件：
神经元之间有兴奋性的互馈连接 (recurrent connection)，这样在没有外界输入的情况下，靠神经元之间的正反馈，网络就能维持住稳定活动；同时我们也要求兴奋性连接是局部的，这样才能形成有意义的空间局部活动； 网络中要有抑制性作用，这样才能避免系统活动因反复的正反馈而爆炸。 Hopfield模型采用吸引子的思想解释了大脑在收到部分或模糊信息条件下的联想式记忆机制。但经典的Hopfield模型没有考虑神经元之间连接的对称结构，因而其具有的吸引子（多个）在空间上是相互孤立的。
在吸引子网络的基础上，如果我们进一步要求神经元之间的连接具有空间平移不变性的对称结构，这时网络就将具有一簇连续的、而不是孤立的吸引子（注意，考虑到真实生物系统的神经数目足够大，为了方便，后面讨论都假设神经元数目是无穷的）；这些吸引子在参数空间上紧密排列，构成一个连续的状态子空间。
一维CANN数学模型 神经元的突触总输入$u$的动力学方程如下： $$ \tau \, \frac{du(x, t)}{dt} = -u(x,t) + \rho \int dx' J(x,x')\, r(x',t) + I_{ext} $$ 其中$x$表示神经元的参数空间位点，$u(x)$代表在参数空间位点x上的神经元的突触总输入，$r(x^′​,t)$为神经元($x'$)的发放率，由以下公式给出: $$ r(x,t) = \frac{u(x,t)^2}{1+k\rho\int{dx&amp;rsquo;u(x',t)^2}} $$ 该模型没有单独考虑抑制性神经元，而是将其作用效果包含在除法归一化作用中。而神经元($x$)和($x'$)之间的兴奋性连接强度$J(x, x')$由高斯函数给出: $$ J(X,x') = \frac{1}{\sqrt{2\pi} \alpha} \, exp\left( - \frac{|x-x'|^2}{2\alpha^2}\right) $$ 我们看到其具有平移不变性，即其为$(x-x’)$的函数。外界输入$I_{ext}$与位置$z(t)$有关，公式如下： $$ I_{ext}=A \, exp \left[ - \frac{|x-z(t)|^2}{4\alpha^2}\right] $$</description></item><item><title>[CN] Task representations in neural networks trained to perform many cognitive tasks</title><link>https://ztqakita.github.io/posts/paper-reading/task_representation_cognitive_tasks/</link><pubDate>Tue, 24 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/paper-reading/task_representation_cognitive_tasks/</guid><description>Yang, G.R., Joglekar, M.R., Song, H.F. et al. Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci 22, 297–306 (2019). https://doi.org/10.1038/s41593-018-0310-2
Abstract They trained single network models to perform 20 cognitive tasks that depend on working memory, decision making, categorization, and inhibitory control.
What they find: recurrent units can develop into clusters that are functionally specialized for different cognitive processes compositionality of task representations: one task can be performed by recombining instructions for other tasks.</description></item><item><title>[DL &amp; CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</title><link>https://ztqakita.github.io/posts/paper-reading/integrated_understanding_system/</link><pubDate>Wed, 18 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/paper-reading/integrated_understanding_system/</guid><description>McClelland, James L., et al. &amp;ldquo;Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models.&amp;rdquo; Proceedings of the National Academy of Sciences 117.42 (2020): 25966-25974.
Abstract For humans, language is a part of a system for understanding and communication about situations. There are some domain-general principles of biological neural network:
connection-based learning distributed representation context-sensitive mutual constraint satisfaction What they propose: the organization of the brain&amp;rsquo;s distributed understanding system, which includes a fast learning system that addresses the memory problem.</description></item><item><title>[BIC] A brain-inspired computational model for spatio-temporal information processing</title><link>https://ztqakita.github.io/posts/paper-reading/reservoir_decision_making/</link><pubDate>Tue, 20 Jul 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/paper-reading/reservoir_decision_making/</guid><description>Lin, Xiaohan, et al. &amp;ldquo;A brain-inspired computational model for spatio-temporal information processing.&amp;rdquo; Neural Networks 143 (2021): 74-87.
Abstract Current method: Explicit feature extraction, which requires lots of labeled data. Novel brain-inspired computational model: Reservoir Decision-making Network (RDMN) A reservoir model: projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics. (regarded it as SVM) A decision-making model: reads out neural representations via integrating information over time.</description></item><item><title>[BIC] Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently</title><link>https://ztqakita.github.io/posts/paper-reading/push-pull_feedback/</link><pubDate>Mon, 19 Jul 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/paper-reading/push-pull_feedback/</guid><description>Liu, Xiao, et al. &amp;ldquo;Push-pull feedback implements hierarchical information retrieval efficiently.&amp;rdquo; Advances in Neural Information Processing Systems 32 (2019): 5701-5710.
Front Word To understand this paper, you need a strong neuroscience background, especially knowing the Hopfield model and Hebbian theory. So before reading this paper, please preview the theories mentioned above!
To be honest, I still cannot understand the details quite well LoL :)
Abstract In addition to feedforward connections, there exist abundant feedback connections in a neural pathway.</description></item></channel></rss>
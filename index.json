[{"categories":null,"contents":"Recursive Neural Network I. RNN Structure Overview The input is a sequence of vectors.  note: Changing the input sequence order will change the output\n We use the same neural network to train, each color in NN means the same weight. When the values stored in the memory is different, the output will also be different. II. Types of RNN   Elman\u0026rsquo;s memory restore the values of hidden layer output, and Jordan\u0026rsquo;s memory restore the values of output. Usually Jordan network have better performance because output y has target.   Bidirectional RNN Using bidirectional RNN can get the information from the context. For example, for the hidden layer output $y^{t+1}$, its value is determined by not only $x^t$ and $x^{t+1}$ but also $x^{t+1}$ and $x^{t+2}$.\n  III. *Long Short-term Memory (LSTM) 1. LSTM Overview Input gate: Decide whether the input can be written in memory cell; Output gate: Decide whether the output of memory cell can be sent to other part of the network; Forget gate: Decide whether the content in memory cell should be erased. 2. Detailed process In this figure, there is a input $z$ and three other scalar $z_i, z_f, z_o$(信号量). Activation function f is usually a sigmoid function, whose value is between 0 and 1. $f(z_i)$ controls the input value of $g(z)$, and $f(z_f)$ controls the memory cell\u0026rsquo;s value. The new value stored in the memory cell is $c'$.\n3. Example 此处强烈建议看一下PPT中的LSTM讲解： Every scalar is determined by input vector(例子中是三维) through linear transform, which means every input vector multiply the corresponding weight and add the bias. All the weight and bias are trained by trainig data through GD.\n4. LSTM\u0026rsquo;s parameter LSTM的每一块相当于原neural network的神经元，可以直接替换。LSTM的参数量为feed-forward NN的参数量的4倍。\n5. LSTM Model In this figure, every dimension of vector $z$ is a input of LSTM neuron, and the product operator is actually element-wise product. For the standard LSTM, it has multiple layer and the input of every layer consists of current input vector $x^{t+1}$, the output of previous layer $h^t$ and the memory cell $c^t$.\n In fact, you could use model in Keras such as \u0026ldquo;LSTM\u0026rdquo;, \u0026ldquo;GRU\u0026rdquo;(simplified LSTM) and \u0026ldquo;SimpleRNN\u0026rdquo;.\n IV. RNN 1. Loss Function The cross-entropy between RNN output and reference vector for every time slot.  Reference vector就是每一个单词所对应的reference组成的向量。\n 2. Learning Method: Backpropogation through time (BPTT) However, RNN-based network is not always easy to learn. The error surface is either very flat or very steep. When you step into a point whose gradient is relatively large and your learning rate is also large, your learning step will make a really huge move so that the program will tell you segmentation fault. One way to avoid this circumstance is to use Cliping, which means to set a threshold of gradient and every move can not larger than the threshold. The reason for this rough error surface is shown as below: 3. Helpful Techniques  RNN vs LSTM LSTM can deal with gradient vanishing(not gradient explore).  LSTM: Memory and input are added and storedin memory unless forget gate is closed. RNN: Every time memory will be formated. The influence never disappears unless forget gate is closed.     此处老师说有国际大厂面试里考过哦！\n Note: Gate Recurrent Unit(GRU) is simpler than LSTM for it has only two gate. 它将 input gate 和 forget gate 进行了联动。\nOther techniques:  此处老师讲述了Hinton的一篇论文，用一般training的方法，initialize的weight是random的话，sigmoid的效果要好于ReLu；他用indentity matrix来initialize weight，这样使用ReLu作为Activation函数则可以显著提高效果。利用这种方法的RNN在效果上吊打LSTM。非常的玄学\n 4. More Application  Many to one: Input is a vector sequence, but output is only one vector Many to many: Both input and output are both sequences, but the output is shorter.    CTC   Sequence to sequence learning 有一篇论文讲到可以直接将某种语言的语音信号作为输入，得到另一种语言文字形式的输出，并发现其有可行性。\n Seq2seq Auto Encoder(没听懂)    V. Attention-based Model 1. Model Overview 其中的DNN/RNN类似于计算机中的CPU概念。 2. Model version 2 Neural Turing Machine： ","date":"October 15, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/rnn/","summary":"Recursive Neural Network I. RNN Structure Overview The input is a sequence of vectors.  note: Changing the input sequence order will change the output\n We use the same neural network to train, each color in NN means the same weight. When the values stored in the memory is different, the output will also be different. II. Types of RNN   Elman\u0026rsquo;s memory restore the values of hidden layer output, and Jordan\u0026rsquo;s memory restore the values of output.","tags":null,"title":"Recursive Neural Network"},{"categories":null,"contents":"Convolutional Neural Network I. CNN Structure Overview II. Convolution  Note: 1.Every elements in filter are the network parameter to be learned. 2.Stride means each step you walk from previous position. 3.The size of filter is decided by programmer.\n From the picture we could know the largest values of Feature Map means there has a feature. Then we do the same process for every filter and generate more Feature Map.\nIf we deal with colorful image, we will use Filter Cube instead of matrix.\nThen there is a question: What is the relationship between Convolution operator and Fully Connected ? We expand the matrix into the input of the neural network. Each color of line correspond with the weight. It just like a fully connected neural network but dropout some weight. Therefore we have less parameters!\n 这里请注意每一个输出对应连接的weight是共享的（也就是一致的），从图里看出共享weight的颜色是相同的。\n III. Max Pooling We divide the feature map into small partition, and select the largest element in every partition. Then we use these elements to compose a new feature map. This operation is called Max Pooling.\n Note: Each filter is a channel. The number of the channel is the number of filters.\n 在这里着重强调一下，每一次进行重复卷积池化的时候，filter是不一样的。第一次卷积池化时filter是二维向量，但到了第二次卷积池化时，feature map作为新的输入已然是cube的形状，此时并不会需要更多数量的二维向量filter，而是会使用cube形状的filter进行内积操作。\nIV. One Example 请注意这里每一次filter的参数数量，在第一级是25个$3\\times3$的矩阵，所以每个filter参数为9；而在第二级是50个$50\\times3\\times3$的立方体，每个filter参数为225。Max Pooling的size为$2\\times2$，所以每经过一次池化feature map的size就会减少一半。(若是行列为奇数，则自动丢弃多出的行/列)。\nMore Application 对于这种Spectrogram的图像识别而言，运用CNN的技术可以实现对某些task的识别。由于在CNN之后会接上LSTM等架构，导致在CNN模块中对时间域的提取并不会提高模型的性能，所以一般filter只会在频域中进行卷积。 ","date":"September 14, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/cnn/","summary":"Convolutional Neural Network I. CNN Structure Overview II. Convolution  Note: 1.Every elements in filter are the network parameter to be learned. 2.Stride means each step you walk from previous position. 3.The size of filter is decided by programmer.\n From the picture we could know the largest values of Feature Map means there has a feature. Then we do the same process for every filter and generate more Feature Map.","tags":null,"title":"Convolutional Neural Network"},{"categories":null,"contents":"Introduction to Deep Learning I. Basic Concepts 1. Fully Connected Feedforward Network 2. Matrix Operation Every layer has weight matrix and bias matrix, using matrix operation we can accumulate the output matrix $y$.  Tips: Using GPU could speed up matrix operation.\n II. Why Deep Learning? 1. Modularization 对neural network而言，并不是神经元越多越好，通过例子可以看出层数的增加（more deep）对于准确率的提升更有效果。这其中就是 Modularization 的思想。For example, while you are trying to train the model below, you can use basic classifiers as module. Each basic classifier can have sufficient training examples. Therefore those classifiers can use output from the basic classifiers, which can be trained by little data. What you should notice is that the modularization is automatically learned form data.\nIII. Backpropagation 1. Concept An Efficient way to compute gradient descent $\\partial{L}/\\partial{\\omega}$ in neural network.\n2. Detailed Process The partial of the loss function can be presented as follow: According to Chain rule, we need to compute two partials: Forward pass and Backward pass.   Forward pass $\\partial{z}/\\partial{\\omega} = $ The value of the output connected by the weight.   Backward pass To compute $\\partial{C}/\\partial{z}$, we need to figure out following neurons' partial.   case 1: Output Layer When we arrive at the output layer of the network, these two partials: $\\partial{C}/\\partial{z'} = \\partial{C}/\\partial{y_1}$ and $\\partial{C}/\\partial{z''} = \\partial{C}/\\partial{y_2}$ could be solved pretty easy.   case 2: Not Output Layer We could compute $\\partial{C}/\\partial{z}$ recursively, until we reach the output layer. Then we start to backward pass. It seems like a new backward neural network (red line in the graph).     The whole process of backpropogation is shown as following graph: IV. Tips for DL 1. New activation function  Vanishing Gradient Descent This phenomenon is mainly caused by the particular activation function: sigmoid funcion. From the analysis above when given a large $\\Delta{\\omega}$ as an input, the output from sigmoid function is become smaller. After going through many layers, the gradient goes to a small value, also called vanished.  (1) ReLU (2) Maxout  ReLU is a special case of Maxout\n For Maxout function, we can design the form of activation funcion by our own.  Activation function in maxout network can be any piecewise linear convex function How many pieces depending on how many elements in a group  Question 1: How to train Maxout network? Answer: 对于每一个神经元，他们先比大小取出较大$z_i$作为输入，再将其对应的linear function拿去作微分 Quesion 2: 有些output被舍去之后，back propogation无法再对其进行训练？ Answer: We can train different thin and linear network for different example.\n2. Adaptive Learning Rate  tips: 实际上对于local minima, 并不会存在过多，local minima的出现需要每一个dimension在附近都会产生local minima。假设一个dimension在这里出现local minima的概率为p，由于神经网路需要非常多的parameters，local minima出现的概率很低。\n 3. Early Stopping Using Validation Set to simulate testing set, then we could find where to stop properly. 4. Regularization New loss function to be minimized:  Find a set of weight not only minimizing original cost but also close to zero.  L2 regularization (二范数)   Weight Decay (权重衰减) 对于一些长期不需要用到的weight，它的值会随着训练次数接近0，这种模式接近于人脑神经元的构造方式。\n  L1 regularization L1 每次减掉固定的值，使得每次迭代后保留很多较大的值，得到的结果会比较稀疏，有很多接近于0，也有很多较大的值；然而对于L2，平均值较小。  5. Dropout The amazing point of dropout:  当你将测试集送入每一个minibatch训练的neural network，再将所有output求average，结果跟将测试集送入完整(未dropout，将所有weight乘以1-p%)的neural network得出来的结果是近似的。这点在线性模型中很容易解释，但对于非线性模型依旧有这个特性。所以Relu+dropout和Maxout+dropout的效果都会不错。\n ","date":"August 10, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/introduction-to-deep-learning/","summary":"Introduction to Deep Learning I. Basic Concepts 1. Fully Connected Feedforward Network 2. Matrix Operation Every layer has weight matrix and bias matrix, using matrix operation we can accumulate the output matrix $y$.  Tips: Using GPU could speed up matrix operation.\n II. Why Deep Learning? 1. Modularization 对neural network而言，并不是神经元越多越好，通过例子可以看出层数的增加（more deep）对于准确率的提升更有效果。这其中就是 Modularization 的思想。For example, while you are trying to train the model below, you can use basic classifiers as module.","tags":null,"title":"Introduction to Deep Learning"},{"categories":null,"contents":"New Optimizers for Deep Learning  Some Notation: $\\theta_t$: model parameter at time step t $\\nabla$$L(\\theta_t)$ or $g_t$: gradient at $\\theta_t$, used to compute $\\theta_{t+1}$ $m_{t+1}$: momentum accumlated from time step 0 to time step t, which is used to compute $\\theta_{t+1}$\n I. Adaptive Learning Rates In gradient descent, we need to set the learning rate to converge properly and find the local minima. But sometimes it\u0026rsquo;s difficult to find a proper value of the learning rate. Here are some methods to introduce which can automatically produce the adaptive learning rate based on the certain situation.\n Popular \u0026amp; Simple Idea: Reduce the learning rate by some factor every few epochs (eg. $\\eta^t=\\eta/\\sqrt{t+1}$) Adagard: Divide the learning rate of each parameter by the root mean square of its previous derivatives When $\\eta = \\frac{\\eta^t}{\\sqrt{t+1}}$, we could derive that:   II. Stochastic Gradient Descent SGD update for each example, so it would converge more quickly.\nIII. Limitation of Gradient Descent  Stuck at local minima ($\\frac{\\partial L}{\\partial \\omega}$ = 0) Stuck at saddle point ($\\frac{\\partial L}{\\partial \\omega}$= 0) Very slow at plateau: Actually this is the most serious limitation of Gradient Descent.  So there are other methods perform better than gradient descent.\nIV. New Optimizers  notice: on-line: one pair of ($x_t$, $\\hat{y_t}$) at a time step off-line: pour all ($x_t$, $\\hat{y_t}$) into the model at every time step\n 1. SGD with Momentum (SGDM) Previous gradient will affect the next movement. $v^i$ is actually the weighted sum of all the previous gradient: $\\nabla L(\\theta^0), \\nabla L(\\theta^1), \\dots \\nabla L(\\theta^{i-1})$ Use SGDM could erase the limitation of $\\nabla L(\\theta) = 0$. For momentum, you could imagine it as physical inertia. 2. Root Mean Square Propagation (RMSProp) This optimizer is based on Adagrad, but the main difference is the calculation of the denominator. The reason for this new calculation is to avoid the situation that the learning rate is too small from the first step. In other words, exponential moving average(EMA) of squared gradients is not monotonically increasing. 3. Adam Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. Adam realizes the benefits of both AdaGrad and RMSProp. 4. Adam vs SDGM  Adam: fast training, large generalization gap, unstable SGDM: stable, little generalization gap, better convergence  5. Supplement   ASMGrad (ICLR'18)   AdaBound (ICLR'19)  note: Clip(x, lowerbound, upperbound)\n   SWATS (arXiv'17) Begin with Adam(fast), end with SGDM   RAdam (Adam with warmup, ICLR'20)   RAdam vs SWATS   To be continued \u0026hellip;\n","date":"July 25, 2020","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/posts/deep-learning/optimizer-for-dl/","summary":"New Optimizers for Deep Learning  Some Notation: $\\theta_t$: model parameter at time step t $\\nabla$$L(\\theta_t)$ or $g_t$: gradient at $\\theta_t$, used to compute $\\theta_{t+1}$ $m_{t+1}$: momentum accumlated from time step 0 to time step t, which is used to compute $\\theta_{t+1}$\n I. Adaptive Learning Rates In gradient descent, we need to set the learning rate to converge properly and find the local minima. But sometimes it\u0026rsquo;s difficult to find a proper value of the learning rate.","tags":null,"title":"Optimization for Deep Learning"},{"categories":null,"contents":"Greeting! This is an introduction post. This post tests the followings:\n Hero image is in the same directory as the post. This post should be at top of the sidebar. Post author should be the same as specified in author.yaml file.  ","date":"June 8, 2020","hero":"/posts/introduction/hero.svg","permalink":"https://ztqakita.github.io/posts/introduction/","summary":"Greeting! This is an introduction post. This post tests the followings:\n Hero image is in the same directory as the post. This post should be at top of the sidebar. Post author should be the same as specified in author.yaml file.  ","tags":null,"title":"Introduction"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://ztqakita.github.io/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"}]
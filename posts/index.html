<!doctype html><html><head><title>Posts</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/deep-learning/>DL</a><ul><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/posts/machine-learning/support-vector/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Decision Tree</h5><p class="card-text post-summary">支持向量机模型 学习策略：间隔最大化，可形式化为一个求解凸二次规划(convex quadratic programming)的最优化问题 一、线性可分支持向量机与硬间隔最大化 支持向量机的学习是在特征空间上进行的，需要从输入空间转换到特征空间上。
线性可分支持向量机：给定线性可分的训练集，通过间隔最大化或等价地求解相应的凸二次规划问题得到的分离超平面为： $$\omega^* x + b^* = 0$$ 以及相应的分类决策函数： $$f(x) = sign(\omega^* x + b^*)$$
函数间隔和几何间隔
函数间隔：对于给定的训练数据集$T$和超平面$(\omega, b)$，则 $$\hat{\gamma_i}=y_i(\omega \cdot x_i+b)$$
超平面关于数据集$T$的函数间隔为 $$\hat{\gamma}=\min_{i=1,\dots,N}\hat{\gamma_i}$$ 几何间隔：对于给定的训练数据集$T$和超平面$(\omega, b)$，则 $$\hat{\gamma_i}=y_i(\frac{\omega}{||\omega||} \cdot x_i+\frac{b}{||\omega||})$$
其中$||\omega||$是L2范数。 间隔最大化
输入：线性可分数据集$T$ 输出：最大间隔分离超平面和分类决策函数 构造并求解约束最优化问题： $$\min_{\omega,b} \quad \frac{1}{2}||\omega||^2 \
s.t. \qquad y_i(\omega \cdot x_i+b)-1 \geq 0, \quad i=1, 2, \dots, N$$ 求解得到最优解$\omega^*, b^*$。
支持向量 训练集的样本点中与分离超平面距离最近的样本点的实例： $$H_1:\omega \cdot x_i+b=1 \ H_2:\omega \cdot x_i+b=-1$$ 支持向量机由这些很少的“重要”训练样本决定。</p></div><div class=card-footer><span class=float-left>March 29, 2021</span>
<a href=/posts/machine-learning/support-vector/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/decision-tree/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Decision Tree</h5><p class="card-text post-summary">原理 决策树模型与学习 决策树的内部节点表示一个特征或属性，叶节点表示一个类。
if-then规则：互斥且完备 本质： 从训练数据集中归纳出一组分类规则（估计出条件概率模型） 损失函数：正则化的极大似然函数 学习算法：启发式方法，得到(sub-optimal)的决策树 一、特征选择 准则：信息增益
熵： 设$X$是一个取有限个值的离散随机变量，定义为： $$H(X) = -\sum_{i=1}^n p_i\log p_i$$
条件熵： 随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为： $$H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)$$ 当熵和条件熵中的概率由极大似然估计得到时，分别成为经验熵和经验条件熵
信息增益：表示得知特征$X$的信息使得类$Y$的信息不确定性减少的程度。这种差值也称为互信息 $$g(D,A)=H(D)-H(D|A)$$
特征选择方法：对训练数据集D，计算每一个特征的信息增益，选取信息增益最大的特征。
信息增益比 上述的特征选择存在偏向选择取值较多的特征的问题，使用information gain ratio可以解决这个问题。 定义为： $$g_R(D, A) = \frac{g(D,A)}{H_A(D)}$$ 其中，$H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{D}\log_2\frac{|D_i|}{D} $，n是特征A的取值个数。 二、决策树的生成 ID3算法 C4.5算法的基础般，只用信息增益来选取特征。 C4.5算法 三、决策树的剪枝 决策树的生成很容易出现过拟合现象，所以需要利用剪枝(pruning)来简化决策树。 决策树的剪枝往往通过极小化决策树整体的损失函数(loss function)来实现： $$C_{\alpha}(T)=\sum_{i=1}^{|T|}N_tH_t(T) + \alpha|T|=C(T)+\alpha|T|$$ 其中$|T|$表示树T的叶结点个数，t是树T的叶结点，该叶结点上有$N_t$个样本点，$\alpha \geq 0$为参数。
通过式子可以看出$C(T)$代表了模型对训练数据的预测误差，即拟合程度；而$\alpha|T|$代表了模型的复杂度，可以理解为正则化的方式来增强模型的泛化能力。
而树的剪枝算法分为：
预剪枝：不足是基于贪心策略，带来欠拟合的风险 后剪枝 后剪枝与动态规划类似，生成一棵完整的决策树以后，自底向上地对非叶节点进行考察，若剪完后损失函数变小，则进行剪枝。</p></div><div class=card-footer><span class=float-left>March 22, 2021</span>
<a href=/posts/machine-learning/decision-tree/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/basic-concepts/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Basic Concepts</h5><p class="card-text post-summary">统计学习概念辨析 一、基本分类 1. 监督学习 监督学习的本质是学习输入到输出的映射的统计规律。需要注意的有以下要点：
输入空间与特征空间不一定为相同的空间，有时会将实例从输入空间映射到特征空间 训练数据由输入(特征向量)与输出对组成 任务问题分类： 回归问题：输入变量与输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测问题 标注问题：输入变量与输出变量均为变量序列的预测问题 $X$和$Y$具有联合概率分布就是监督学习关于数据的基本假设，即假设训练数据和测试数据是依联合概率分布$P(X,Y)$独立同分布产生的 假设空间的确定意味着学习范围的确定 2. 无监督学习 无监督学习的本质是学习数据中的统计规律或潜在结构，需要注意的有以下要点：
可以用于对已有数据的分析，也可以用于对未来数据的预测 要学习的模型可以表示为$z=g(x)$，条件概率分布$P(z|x)$，或者条件概率分布$P(x|z)$的形式 3. 强化学习 强化学习的本质是学习最优的序贯决策。在学习过程中，系统不断地试错，以达到学习最优策略的目的。
强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组$&lt;S,A,P,r,\gamma>$组成：
$S$是state集合 $A$是action集合 $P$是状态转移概率(transition probability)函数： $$P(s'|s,a)=P(s_{t+1}=s'|s_t=s,a_t=a)$$ $r$是奖励函数(reward function): $$r(s,a)=E(r_{t+1}|s_t=s, a_t=a)$$ $\gamma$是衰减系数(discount factor): $$\gamma \in [0,1]$$ 马尔可夫决策过程具有马尔科夫性，下一个状态只依赖于前一个状态与动作，由状态转移概率函数$P(s'|s,a)$表示。下一个奖励依赖于前一个状态与动作，由奖励函数$r(s,a)$表示。
策略$\pi$：给定状态下动作的函数$a=f(s)$或者条件概率分布$P(a|s)$ 价值函数/状态价值函数：策略$\pi$从某一个状态$s$开始的长期累积奖励的数学期望： $$v_{\pi}(s)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s]$$ 动作价值函数：策略$\pi$从某一个状态$s$和动作$a$开始的长期累积奖励的数学期望： $$q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\dots|s_t=s, a_t=a]$$ 强化学习的目标就是在所有可能的策略中选出价值函数最大的策略$\pi^*$。
强化学习的分类：
policy-based 不直接学习模型，试图求解最优策略$\pi^*$。学习通常从一个具体策略开始，通过搜索更优的策略进行。 value-based 试图求解最有价值函数($q^*(s,a)$)。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。 model-based 直接学习马尔科夫决策过程的模型，通过模型对环境的反馈进行预测。 4.</p></div><div class=card-footer><span class=float-left>January 27, 2021</span>
<a href=/posts/machine-learning/basic-concepts/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/machine-learning/perceptron/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Perceptron</h5><p class="card-text post-summary">原理 参考：统计学习方法|感知机原理剖析及实现
输入：实例的特征向量 输出：实例的类别（二分类） 模型类别：判别模型 学习策略：基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型</p></div><div class=card-footer><span class=float-left>January 27, 2021</span>
<a href=/posts/machine-learning/perceptron/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/database-system/database_system_4/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Database System Lecture Note 4</h5><p class="card-text post-summary">Chapter 12 Query Processing 12.1 Overview 12.2 Measures of Query Cost Disk access is the predominant cost. Use the number of block transfered from disk and the number of seeks as the cost measures.
12.3 Selection Operation Types of query conditions (查询条件类型) equality(等值), e.g.salary = 100 range (范围), e.g. salary between 50 and 400 comparison (比较), e.g. salary >300 NOTE: B+索引可以使用全部查询类型，而Hash索引只能等值查询。
Several file scan algorithms linear search/scan – A1 无索引、乱序：扫描全部blocks，才能找到全部满足查询条件的数据。 Cost estimate = $b_r$ block transfers + 1 seek selections using indices – A2, A3, A4 A2 主索引：Cost = $(h_i + 1) \times (t_T + t_S)$ A3 聚集索引不唯一：Cost = $h_i \times (t_T + t_S) + t_S + t_T\times b$ A4 非聚集索引不唯一：Cost = $(h_i + n) \times (t_T + t_S)$ selections involving comparisons – A5, A6 A5: 主索引，找叶子节点 A6: 辅索引，找叶子节点所指向的记录 complex selections – A7, A8, A9, A10 12.</p></div><div class=card-footer><span class=float-left>December 14, 2020</span>
<a href=/posts/database-system/database_system_4/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/operating-system/operating_system_concepts_6/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>File System</h5><p class="card-text post-summary">Chapter 10 File System Chapter 11 File System Inplementation</p></div><div class=card-footer><span class=float-left>December 10, 2020</span>
<a href=/posts/operating-system/operating_system_concepts_6/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/operating-system/operating_system_concepts_7/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Mass-Storage Structure & I/O System</h5><p class="card-text post-summary">Chapter 12 Mass-Storage Structure hard disk drives(HDDs) and nonvolatile memory (NVM)
12.1-12.2 Disk Structure Mapping between disk and blocks 并行快速读取多个连续的blocks/sectors
Transfer rate: the rate at which data flow between the driver and the computer
The position time:
seek time: the time for the disk to move the disk arm to the cylinder containing the desired sector.平均为移动1/2*(总磁道数)所需的时间 rotational latency: the time for the desired sector to rotate to the disk head.</p></div><div class=card-footer><span class=float-left>December 10, 2020</span>
<a href=/posts/operating-system/operating_system_concepts_7/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/database-system/database_system_3/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Database System Lecture Note 3</h5><p class="card-text post-summary">Chapter 10 Storage and File Structure 对于应用程序和disk而言，中间会有data buffer作为数据交换的缓冲池，替换算法一般采用LRU算法，注意两种可控的参数，在实验中会用到：
连接时长 buffer大小 逻辑结构：
流式文件、基于记录文件、基于索引文件 物理结构：
以block为单位进行存储 contiguous linked indexed 10.5 File Organization Each file is a sequence of records, and a relational table is a set of tuples. A tuple is stored as a record in the DB file.
10.5.1 Fixed-Length Records a block contains several records. Store record i starting from byte n * (i – 1), where n is the size of each record.</p></div><div class=card-footer><span class=float-left>November 23, 2020</span>
<a href=/posts/database-system/database_system_3/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/algorithms/algorithm_analysis-3/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Greedy & Back-track & Branch and Bound</h5><p class="card-text post-summary">贪心算法 算法+证明
一、最优装载问题 算法： void Loading(int x[], int w[], int c, int n) { int *t = new int [n+1]; Sort(w, t, n); for(int i = 0; i &lt; n; i++) x[i] = 0; for(int i = 0; i &lt; n && w[t[i]] &lt; c; i++) { x[t[i]] = 1; c -= w[t[i]]; } } 证明： 最优子结构性质： 二、哈夫曼编码 三、最小生成树 回溯法 子集树：0-1背包问题，从包含n个全集当中去选择一个子集，所有可能解是$O(2^n)$ 排序树：TSP问题，解是一个排列，所有可能解的规模是$O(n!)$
子集树：当所给的问题是从n个元素的集合S中找出满足某种性质的子集时，相应的解空间称为子集树。 遍历子集树 时间复杂度：$O(2^n)$ void backtrack (int t) { if (t>n) output(x); else { for (int i=0;i&lt;=1;i++) { x[t]=i; if (legal(t)) backtrack(t+1); } } } 排列树：当所给的问题是确定n个元素满足某种性质的排列时，相应的解空间树成为排列树。 遍历排序树 时间复杂度：$O(n!</p></div><div class=card-footer><span class=float-left>November 16, 2020</span>
<a href=/posts/algorithms/algorithm_analysis-3/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/operating-system/operating_system_concepts_5/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Memory Management</h5><p class="card-text post-summary">Chapter 8 Memeory Management 8.1 Background Address Space: the memory scope that the process can access(可访问的(内)存储区范围)
A pair of base register and limit register define the logical address space 8.1.2 Address Binding Chapter 9 Virtual Memory</p></div><div class=card-footer><span class=float-left>November 13, 2020</span>
<a href=/posts/operating-system/operating_system_concepts_5/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/compiler/compilers_3/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Semantic Analysis & Runtime Environment</h5><p class="card-text post-summary">语法分析 知识点：
运行环境 知识点：
活动记录、控制栈 栈式存储分配 非局部名字的访问 参数传递方式 7.1 程序运行时的存储组织 7.1.1 程序运行空间的划分 活动与过程的概念 过程：静态概念 活动：一次过程的每次执行，是动态概念 两者可以为1:1或1:m的关系，递归过程中可能一若干个活动活着。
活动的生存期 活动的生存期要么是不重叠的，要么是嵌套的(区别于进程的地方！活动不可以并发执行！)
对于一个程序，它会向操作系统申请一块内存空间： 活动记录会在控制栈中根据栈式存储分配策略来实现。 7.1.2 活动记录与控制栈 控制栈 局部数据的安排 编址限制的影响(padding) 7.1.3 名字的作用域及名字绑定 名字的作用域： 最近嵌套原则
名字绑定： note: 对于递归程序中定义的名字，即使是同一个名字也可能映射到不同的存储空间。
左值：存储空间的地址 右值：存储空间的内容 7.2 存储分配策略 7.2.1 静态存储分配 条件：源程序中声明的各种数据对象所需存储空间的大小在编译时都可以确定。 存储分配：编译时,为他们分配固定的存储空间。 地址绑定：程序装入内存时进行。 运行期间：名字的左值保持不变 不允许递归调用和建立动态数据结构
7.2.2 栈式存储分配(重点) 递归调用：同一个过程在不同存储位置出现。
调用序列 返回序列 其中top_sp = top_ep' - C1 - C2; P是通过top_sp找到返回值并放入局部数据域中。</p></div><div class=card-footer><span class=float-left>October 27, 2020</span>
<a href=/posts/compiler/compilers_3/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/compiler/compilers_2/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Syntax-directed Translation</h5><p class="card-text post-summary">语法制导翻译技术 整体思路：
首先，根据翻译目标来确定每个产生式的语义； 其次，根据产生式的含义，分析每个符号的语义； 再次，把这些语义以属性的形式附加到相应的文法符号上（即把语义和语言结构联系起来）； 然后，根据产生式的语义给出符号属性的求值规则 （即语义规则），从而形成语法制导定义。 ==翻译目标决定产生式的含义、决定文法符号应该具有的属性，也决定了产生式的语义规则。==
两种描述语法制导翻译的形式：
语法制导定义：是对翻译的高层次说明，它隐蔽了一些实现细节，无须指明翻译时语义规则的计算次序 L-属性可以一遍扫描，省去分析树和依赖图的步骤。 翻译方案：指明了语义规则的计算次序，规定了语义动作的执行时机。 5.1 语法制导定义以及翻译方案 5.1.1 语法制导定义 对上下文无关文法的推广
综合属性 左部符号的综合属性是从该产生式右部文法符号的属性值计算出来的；在分析树中，一个内部结点的综合属性是从其子结点的属性值计算出来的。 在分析树中，若一个结点的某一属性由其子节点属性决定，则为综合属性 若在一个语法制导定义仅仅使用综合属性，则称之为S-属性定义。而对于这种属性，通常采用自底向上的方法进行注释。 继承属性 出现在产生式右部的某文法符号的继承属性是从其所在产生式的左部非终结符号和/或右部文法符号的属性值计算出来的；在分析树中，一个结点的继承属性是从其兄弟结点和/或父结点的属性值计算出来的。 在分析树中，一个结点的继承属性由其父节点属性或它的兄弟节点属性值决定。 可以用继承属性表示程序设计语言中的上下文之间的依赖关系。 当一个语义规则的唯一目的是产生某个副作用，则通常写成过程调用或程序段，看成是产生式左部非终结符的虚拟综合属性
5.1.2 依赖图 在依赖图中：
为每个属性设置一个结点 如果属性b依赖于c，那么从属性c的结点有一条有向边连 到属性b的结点。 5.1.3 拓扑排序 首先找到入度为0的节点，删除该点及其边，重复该过程至所有点均被删除。
5.1.4 计算顺序 根据拓扑排序的顺序进行求值得到翻译。
总结：最基本的文法用于建立输入符号串的分析树；
为分析树构造依赖图； 对依赖图进行拓扑排序； 从这个序列得到语义规则的计算顺序； 照此计算顺序进行求值，得到对输入符号串的翻译。 5.1.5 S-属性与L-属性 S属性定义：仅仅涉及综合属性，是L属性的子集 L属性定义：要么是综合属性，要么是由父节点和左兄弟节点决定的继承属性。(右边的不行) 对于L属性计算顺序：深度优先遍历分析树 进入节点前，计算它的继承属性 从节点返回时，计算它的综合属性 5.</p></div><div class=card-footer><span class=float-left>October 27, 2020</span>
<a href=/posts/compiler/compilers_2/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/posts/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/posts/>1</a></li><li class=page-item><a class=page-link href=/posts/page/2/>2</a></li><li class=page-item><a class=page-link href=/posts/page/3/>3</a></li><li class=page-item><a href=/posts/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/posts/page/3/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=/js/list.js></script></body></html>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Life Learning on ztqakita's Blog</title><link>https://ztqakita.github.io/posts/life-learning/</link><description>Recent content in Life Learning on ztqakita's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 29 Aug 2021 06:00:20 +0600</lastBuildDate><atom:link href="https://ztqakita.github.io/posts/life-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Approaching Scientific Question &amp; How to Model</title><link>https://ztqakita.github.io/posts/life-learning/how_to_model/</link><pubDate>Sun, 29 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/how_to_model/</guid><description>Marr’s 3 levels of analysis Brain: hierarchy of complexities
Computational level - 1 What is the objective of the system? How close is it to optimal? Algorithmic level - 2 What are the data structures? What are the approximations? What is the runtime? Implementation level - 3 What is the hardware? Neurons? Synapses? Molecules? Diversity of modeling goals Useful: good at solving real-world problems?</description></item><item><title>Lecture: Are People Still Smarter than Machines?</title><link>https://ztqakita.github.io/posts/life-learning/lecture_james_mcclleland/</link><pubDate>Tue, 17 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/lecture_james_mcclleland/</guid><description>How can a neural network learn to do something cognitively interesting? The Biologically Inspired Approach If neuron A participates in firing neuron B, strengthen the connection from A to B The Optimization Approach Adjust each connection to minimize the network&amp;rsquo;s deviation from a desired output Backpropogation algorithm became the basis of sbsequent research in the PDP framework, as well as almost all of Deep Learning Their Early Success of the Approach Models that could learn to read words and generalize to pronounceable non-words, capturing human-like response patterns: they refect statistical patterns and similarity relationships Models that captured many aspects of human semantic cognition and the disintegration of human semantic abilities resulting from neurodegenerative disease Models that showed in principle how aspects of language knowledge could be captured in a simple artificial neural network Nowadays AlexNet, ResNet and so on.</description></item><item><title>每日学习———类脑智能的另一种思路</title><link>https://ztqakita.github.io/posts/life-learning/lecture_yao_xin/</link><pubDate>Mon, 09 Aug 2021 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/life-learning/lecture_yao_xin/</guid><description>写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。
姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。</description></item></channel></rss>
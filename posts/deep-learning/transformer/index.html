<!doctype html><html><head><title>Transformer</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><meta name=description content="Transformer"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/neural-computation/basic-neuro-knowledge/>Neuroscience Basic Knowledge</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/deep-learning/>DL</a><ul class=active><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a class=active href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/life-learning/>Life Learning</a><ul><li><a href=/posts/life-learning/lecture_james_mcclleland/>Lecture James McClleland</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/integrated_understanding_system/>Integrated understanding system</a></li><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li><li><a href=/posts/paper-reading/task_representation_cognitive_tasks/>Task representations in neural networks</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ztqakita.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ztq.png><h5 class=author-name>Brandon Zhang</h5><p>June 11, 2021</p></div><div class=title><h1>Transformer</h1></div><div class=post-content id=post-content><h2 id=i-transformer-overview>I. Transformer Overview</h2><ul><li><p><strong>Seq2seq model</strong></p></li><li><p>Input: sequence of vector</p></li><li><p>Output: sequence (<strong>not fixed size</strong>)</p><p><img src=/images/posts/DL/transformer.JPG alt></p></li></ul><h2 id=ii-encoder>II. Encoder</h2><p>It is actually a <strong>Self-Attention</strong> Model!</p><p>对于一个block，它的结构可以理解为以下的形式：</p><p><img src=/images/posts/DL/encoder-1.JPG alt></p><ol><li>与self-attention不同的是，会采用residual add的方式，将self-attention得到的中间结果$a$加上输出$b$</li><li>经过layer normalization得到新的输出</li><li>将新的output输入到FC中，并加上residual</li><li>再次经过layer normalization得到这一个block的结果。</li></ol><p>理解了一个block以后，整个Encoder就是由n个这种block组成的network。</p><p><img src=/images/posts/DL/encoder-2.JPG alt></p><ol><li>首先将word进行self-attention得到word embeddding，并在其中加入positional encoding信息</li><li>经过multi-head Attention或者Feed Forward Network后，都要接上<strong>residual + layer normalization</strong>，而这种设计结构就是Transformer Encoder的创新点所在。</li></ol><h2 id=iii-decoder--autoregressiveat>III. Decoder —— Autoregressive(AT)</h2><p>The model structure of Decoder is shown as follow:</p><p><img src=/images/posts/DL/decoder-1.JPG alt></p><p>根据上图我们将逐步逐层地解释结构：</p><ol><li><strong>Masked Multi-head Self-attention</strong>
在产生每一个$b^i$ vector时，不能再看后面的信息，只能和前面的输入进行关系：</li></ol><p><img src=/images/posts/DL/mask.JPG alt></p><p>从细节上来看，要想得出$b^2$，我们只需将其和$k^1, k^2$做dot-product。</p><p><img src=/images/posts/DL/mask-1.JPG alt></p><p>这么做的原因在于对于Decoder，它每一次的输入是来自上一次的输出，所以模型不存在右边输入，意味者self-atttention只能去考虑当前计算输出左边的输入做关联。</p><p>而对于Decoder而言，需要模型自己判断什么时候结束输出，得到not fixed sequence。我们需要对于输出的vector中添加新的一个维度[END]，表示输出的结束，然后将这个输出的vector经过softmax得到概率最大的字符，若是END则输出结束。</p><p><img src=/images/posts/DL/stop.JPG alt></p><h2 id=iv-decoder--non-autoregressivenat>IV. Decoder —— Non Autoregressive(NAT)</h2><p><img src=/images/posts/DL/NAT.JPG alt></p><p>How to decide the output length for NAT decoder?</p><ul><li>Another predictor for output length<ul><li>Classifier: Encoder input $\rightarrow$ output length</li></ul></li><li>Output a veryh long sequence, ignore tokens after END</li></ul><p>Advantage: parallel, controllable output length.</p><p>NAT: usually worse than AT</p><ul><li>reason: multi-modality</li></ul><h2 id=encoder-decoder>Encoder-Decoder</h2><p><img src=/images/posts/DL/transformer2.JPG alt></p><p>两者的连接最重要的就是<strong>Cross Attention</strong>，其工作原理如下所示：</p><ul><li>Encoder：提供$k, v$，两个输入</li><li>Decoder：提供$q$，一个输入</li></ul><p><img src=/images/posts/DL/cross-attention.JPG alt></p><p>其实Decoder的每一层cross attention不一定要看encoder的最后一层输出，也可以看中间层的输出。对于cross attention的连接方式是各式各样的，可以作为一个sutdy进行研究。</p><h2 id=training>Training</h2><ul><li>Ground Truth: one-hot encoding.</li><li>Prediction: Distribution of probability.</li><li>We will minimize cross entropy between ground truth and prediction.</li></ul><h3 id=teacher-forcing>Teacher Forcing</h3><p>using the ground truth as input.
在Decoder训练的时候，我们在输入的时给它正确的答案。</p><p><img src=/images/posts/DL/training-1.JPG alt></p><h3 id=exposure-bias>Exposure Bias</h3><ul><li>起因：由于Decoder会将自己的输出作为下一次的输入，所以如果一步错，就会步步错，Decoder不断得到错误的输入并训练出错误的输出。</li><li>解决方法：Scheduled Sampling</li></ul><h2 id=testing>Testing</h2><p>在Testing中是对整体的输出和Ground Truth句子之间计算<strong>BLEU score</strong>。但之所以不在training phase去将BLEU score作为误差是因为其不可微分，无法使用gradient descent。</p><blockquote><p>这里李老师讲了一个口诀，如果遇到optimization无法解决的问题，使用RL硬train一发就对了hhhh，把无法微分的loss function当作是RL的Reward，把Decoder当作是Agent、</p></blockquote><h2 id=tips>Tips</h2><h3 id=copy-mechanism>Copy Mechanism</h3><p>对于一些专有词汇，如果像Decoder一样根据上文预测下一字会效果比较差，一般而言使用copy mechanism可以将专有词汇进行复制，在一些words后面直接进行输出，这样效果会好很多。</p><h3 id=guided-attention>Guided Attention</h3><p>对于语音识别，Attention scores的峰值应该从左向右，为了使得模型训练没有偏差，可以直接将该模式放入模型的训练当中</p><blockquote><p>Monotonic Attention
Location-aware attention</p></blockquote><h3 id=beam-search>Beam Search</h3><p>从解空间树中找到一个approximate solution，使得结果近似optimal。但这种启发式搜索有时有用，有时不管用。对于一些结果特定的任务，Beam Search会很有帮助，但对于需要有创造力的任务，比如续写后文，Beam Search就没有太大用处。</p></div><div class=btn-improve-page><a href=https://github.com/ztqakita/ztqakita.github.io/edit//content/posts/deep-learning/transformer.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/deep-learning/self-attention/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>Self-attention</span></a></div><div class="col-md-6 next-article"><a href=/posts/machine-learning/knn/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>KNN</span></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#i-transformer-overview>I. Transformer Overview</a></li><li><a href=#ii-encoder>II. Encoder</a></li><li><a href=#iii-decoder--autoregressiveat>III. Decoder —— Autoregressive(AT)</a></li><li><a href=#iv-decoder--non-autoregressivenat>IV. Decoder —— Non Autoregressive(NAT)</a></li><li><a href=#encoder-decoder>Encoder-Decoder</a></li><li><a href=#training>Training</a><ul><li><a href=#teacher-forcing>Teacher Forcing</a></li><li><a href=#exposure-bias>Exposure Bias</a></li></ul></li><li><a href=#testing>Testing</a></li><li><a href=#tips>Tips</a><ul><li><a href=#copy-mechanism>Copy Mechanism</a></li><li><a href=#guided-attention>Guided Attention</a></li><li><a href=#beam-search>Beam Search</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>
<!doctype html><html><head><title>Optimization for Deep Learning</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><meta name=description content="Optimization for Deep Learning"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/deep-learning/>DL</a><ul class=active><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a class=active href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><a href=/posts/life-learning/>Life Learning</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://ztqakita.github.io/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/ztq.png><h5 class=author-name>Brandon Zhang</h5><p>July 25, 2020</p></div><div class=title><h1>Optimization for Deep Learning</h1></div><div class=post-content id=post-content><blockquote><p>Some Notation:
$\theta_t$: model parameter at time step t
$\nabla$$L(\theta_t)$ or $g_t$: gradient at $\theta_t$, used to compute $\theta_{t+1}$
$m_{t+1}$: momentum accumlated from time step 0 to time step t, which is used to compute $\theta_{t+1}$</p></blockquote><h2 id=i-adaptive-learning-rates>I. Adaptive Learning Rates</h2><p>In gradient descent, we need to set the learning rate to converge properly and find the local minima. But sometimes it&rsquo;s difficult to find a proper value of the learning rate. Here are some methods to introduce which can automatically produce the adaptive learning rate based on the certain situation.</p><ul><li><strong>Popular & Simple Idea</strong>: Reduce the learning rate by some factor every few epochs (eg. $\eta^t=\eta/\sqrt{t+1}$)</li><li><strong>Adagard</strong>: Divide the learning rate of each parameter by the <em><strong>root mean square of its previous derivatives</strong></em>
<img src=/images/posts/DL/grad.JPG alt>
<img src=/images/posts/DL/adg.JPG alt>
When $\eta = \frac{\eta^t}{\sqrt{t+1}}$, we could derive that:
<img src=/images/posts/DL/adg2.JPG alt></li></ul><h2 id=ii-stochastic-gradient-descent>II. Stochastic Gradient Descent</h2><p><img src=/images/posts/DL/sgd.JPG alt>
SGD update for each example, so it would converge more quickly.</p><h2 id=iii-limitation-of-gradient-descent>III. Limitation of Gradient Descent</h2><ul><li>Stuck at local minima ($\frac{\partial L}{\partial \omega}$ = 0)</li><li>Stuck at saddle point ($\frac{\partial L}{\partial \omega}$= 0)</li><li><strong>Very slow at plateau</strong>: Actually this is the most serious limitation of Gradient Descent.</li></ul><p>So there are other methods perform better than gradient descent.</p><h2 id=iv-new-optimizers>IV. New Optimizers</h2><blockquote><p>notice:
on-line: one pair of ($x_t$, $\hat{y_t}$) at a time step
off-line: pour all ($x_t$, $\hat{y_t}$) into the model at every time step</p></blockquote><h3 id=1-sgd-with-momentum-sgdm>1. SGD with Momentum (SGDM)</h3><p><img src=/images/posts/DL/sgdm.JPG alt>
Previous gradient will affect the next movement. $v^i$ is actually the weighted sum of all the previous gradient:
$\nabla L(\theta^0), \nabla L(\theta^1), \dots \nabla L(\theta^{i-1})$
Use SGDM could erase the limitation of $\nabla L(\theta) = 0$.
For momentum, you could imagine it as physical inertia.
<img src=/images/posts/DL/sgdm2.JPG alt></p><h3 id=2-root-mean-square-propagation-rmsprop>2. Root Mean Square Propagation (RMSProp)</h3><p>This optimizer is based on <strong>Adagrad</strong>, but the main difference is the calculation of the denominator. The reason for this new calculation is to avoid the situation that <em><strong>the learning rate is too small from the first step</strong></em>. In other words, exponential moving average(EMA) of squared gradients is not monotonically increasing.
<img src=/images/posts/DL/sgdm3.JPG alt></p><h3 id=3-adam>3. Adam</h3><p>Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.
Adam realizes the benefits of both AdaGrad and RMSProp.
<img src=/images/posts/DL/adam.JPG alt></p><h3 id=4-adam-vs-sdgm>4. Adam vs SDGM</h3><ul><li>Adam: fast training, large generalization gap, unstable</li><li>SGDM: stable, little generalization gap, better convergence</li></ul><h3 id=5-supplement>5. Supplement</h3><ul><li><p>ASMGrad (ICLR'18)
<img src=/images/posts/DL/asm.JPG alt></p></li><li><p>AdaBound (ICLR'19)
<img src=/images/posts/DL/adab.JPG alt></p><blockquote><p>note: Clip(x, lowerbound, upperbound)</p></blockquote></li><li><p>SWATS (arXiv'17)
Begin with Adam(fast), end with SGDM
<img src=/images/posts/DL/swats.JPG alt></p></li><li><p>RAdam (Adam with warmup, ICLR'20)
<img src=/images/posts/DL/radam.JPG alt></p></li><li><p>RAdam vs SWATS
<img src=/images/posts/DL/cmp.JPG alt></p></li></ul><p>To be continued &mldr;</p></div><div class=btn-improve-page><a href=https://github.com/ztqakita/ztqakita.github.io/edit//content/posts/deep-learning/optimizer-for-DL.md><i class="fas fa-code-branch"></i>Improve this page</a></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/machine-learning/classification/ class="btn btn-outline-info"><span><i class="fas fa-chevron-circle-left"></i>Prev</span><br><span>Classification</span></a></div><div class="col-md-6 next-article"><a href=/posts/introduction/ class="btn btn-outline-info"><span>Next <i class="fas fa-chevron-circle-right"></i></span><br><span>Introduction</span></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#i-adaptive-learning-rates>I. Adaptive Learning Rates</a></li><li><a href=#ii-stochastic-gradient-descent>II. Stochastic Gradient Descent</a></li><li><a href=#iii-limitation-of-gradient-descent>III. Limitation of Gradient Descent</a></li><li><a href=#iv-new-optimizers>IV. New Optimizers</a><ul><li><a href=#1-sgd-with-momentum-sgdm>1. SGD with Momentum (SGDM)</a></li><li><a href=#2-root-mean-square-propagation-rmsprop>2. Root Mean Square Propagation (RMSProp)</a></li><li><a href=#3-adam>3. Adam</a></li><li><a href=#4-adam-vs-sdgm>4. Adam vs SDGM</a></li><li><a href=#5-supplement>5. Supplement</a></li></ul></li></ul></nav></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><script type=text/javascript async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">MathJax.Hub.Config({tex2jax:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$']],processEscapes:!0,processEnvironments:!0,skipTags:['script','noscript','style','textarea','pre'],TeX:{equationNumbers:{autoNumber:"AMS"},extensions:["AMSmath.js","AMSsymbols.js"]}}}),MathJax.Hub.Queue(function(){var b=MathJax.Hub.getAllJax(),a;for(a=0;a<b.length;a+=1)b[a].SourceElement().parentNode.className+=' has-jax'}),MathJax.Hub.Config({TeX:{equationNumbers:{autoNumber:"AMS"}}})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css integrity=sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js integrity=sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>
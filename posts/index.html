<!doctype html><html><head><title>Posts</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/navigators/navbar.css><link href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/favicon_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/layouts/list.css><link rel=stylesheet href=/css/navigators/sidebar.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png>ztqakita's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/images/main-logo_hu864bbe108f1be1ae04b57f7f2fd9d631_5637_42x0_resize_box_2.png class=d-none id=main-logo>
<img src=/images/inverted-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_42x0_resize_box_2.png class=d-none id=inverted-logo></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=https://ztqakita.github.io/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/algorithms/>Algorithms</a><ul><li><a href=/posts/algorithms/algorithm_analysis-1/>Complexity & Divide and Conquer</a></li><li><a href=/posts/algorithms/algorithm_analysis-2/>Dynamic Programming</a></li><li><a href=/posts/algorithms/algorithm_analysis-3/>Greedy & Back-track & Branch and Bound</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/compiler/>Compiler</a><ul><li><a href=/posts/compiler/compilers_1/>Lexcial Analysis & Parsing</a></li><li><a href=/posts/compiler/compilers_3/>Semantic Analysis & Runtime Environment</a></li><li><a href=/posts/compiler/compilers_2/>Syntax-directed Translation</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/neural-computation/>Computational Neuroscience</a><ul><li><a href=/posts/neural-computation/1-ionic_currents/>Ionic Currents</a></li><li><a href=/posts/neural-computation/basic-neuro-knowledge/>Neuroscience Basic Knowledge</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/database-system/>Database System</a><ul><li><a href=/posts/database-system/database_system_1/>Database System Lecture Note 1</a></li><li><a href=/posts/database-system/database_system_2/>Database System Lecture Note 2</a></li><li><a href=/posts/database-system/database_system_3/>Database System Lecture Note 3</a></li><li><a href=/posts/database-system/database_system_4/>Database System Lecture Note 4</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/deep-learning/>DL</a><ul><li><a href=/posts/deep-learning/cnn/>Convolutional Neural Network</a></li><li><a href=/posts/deep-learning/introduction-to-deep-learning/>Introduction to Deep Learning</a></li><li><a href=/posts/deep-learning/optimizer-for-dl/>Optimization for Deep Learning</a></li><li><a href=/posts/deep-learning/rnn/>Recursive Neural Network</a></li><li><a href=/posts/deep-learning/self-attention/>Self-attention</a></li><li><a href=/posts/deep-learning/transformer/>Transformer</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/life-learning/>Life Learning</a><ul><li><a href=/posts/life-learning/architectures_of_neuronal_circuits/>Architectures of neuronal circuits</a></li><li><a href=/posts/life-learning/how_to_model/>how to model</a></li><li><a href=/posts/life-learning/lecture_james_mcclleland/>Lecture James McClleland</a></li><li><a href=/posts/life-learning/lecture_yao_xin/>Lecture Yao Xin</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/machine-learning/>ML</a><ul><li><a href=/posts/machine-learning/basic-concepts/>Basic Concepts</a></li><li><a href=/posts/machine-learning/classification/>Classification</a></li><li><a href=/posts/machine-learning/decision-tree/>Decision Tree</a></li><li><a href=/posts/machine-learning/knn/>KNN</a></li><li><a href=/posts/machine-learning/perceptron/>Perceptron</a></li><li><a href=/posts/machine-learning/support-vector/>Support Vector Machines</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/operating-system/>Operating System</a><ul><li><a href=/posts/operating-system/operating_system_concepts_3/>CPU Scheduling</a></li><li><a href=/posts/operating-system/operating_system_concepts_6/>File System</a></li><li><a href=/posts/operating-system/operating_system_concepts_1/>Introduction & OS Structure</a></li><li><a href=/posts/operating-system/operating_system_concepts_7/>Mass-Storage Structure & I/O System</a></li><li><a href=/posts/operating-system/operating_system_concepts_5/>Memory Management</a></li><li><a href=/posts/operating-system/operating_system_concepts_2/>Process & Threads</a></li><li><a href=/posts/operating-system/operating_system_concepts_4/>Process Synchronization</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/paper-reading/>Paper Reading</a><ul><li><a href=/posts/paper-reading/continuous_attractor_nn/>Continuous-attractor Neural Network</a></li><li><a href=/posts/paper-reading/few-shot_class_incremental_learning/>Few-Shot Class-Incremental Learning</a></li><li><a href=/posts/paper-reading/integrated_understanding_system/>Integrated understanding system</a></li><li><a href=/posts/paper-reading/push-pull_feedback/>Push-pull feedback</a></li><li><a href=/posts/paper-reading/reservoir_decision_making/>reservoir decision making network</a></li><li><a href=/posts/paper-reading/task_representation_cognitive_tasks/>Task representations in neural networks</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=/posts/paper-reading/few-shot_class_incremental_learning/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[CN] Few-Shot Class-Incremental Learning</h5><p class="card-text post-summary">Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12183-12192
Abstract FSCIL: It requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. TOPIC: Topology-preserving knowledge incrementer framework, which is proposed to mitigates the forgetting of the old classes by stabilizing NG’s topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples.</p></div><div class=card-footer><span class=float-left>September 23, 2021</span>
<a href=/posts/paper-reading/few-shot_class_incremental_learning/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/architectures_of_neuronal_circuits/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Architectures of neuronal circuits</h5><p class="card-text post-summary">参考智源社区发布文章《Science重磅综述：神经环路结构——使大脑“计算机”高速运转的系统》 https://hub.baai.ac.cn/view/9949
常见的环路模式（circuit motifs） 一、前馈兴奋（feedforward excitation） 兴奋性神经元之间的一系列连续连接构成前馈兴奋，是信号从一个神经区域传递到另一个区域的主要方式。在每个阶段，神经元通常从多个突触前连接接收信号输入，并通过分支轴突向多个突触后连接（发散式兴奋）输出信号。
收敛式兴奋（convergent excitation）能使突触后连接选择性地对突触前连接的确定神经元作出反应。当多个输入神经元携带相同但不相关的信号时，也能提高信号的信噪比。 发散式兴奋（divergent excitation）则是将同一信号被多个下游通路处理。 比如哺乳动物的视觉系统，信号从光感受器→双极细胞→视网膜神经节细胞→外侧膝状核（LGN）神经元→第4层初级视觉皮层（V1）神经元→其他层的V1神经元→皮层上层神经元
二、前馈抑制和反馈抑制（Feedforward and feedback inhibition） 两种广泛使用的模式是前馈抑制和反馈抑制。
在前馈抑制中，抑制性神经元从突触前的兴奋性神经元接收信号输入，因此抑制性和突触前兴奋性输入都汇聚于突触后神经元。 在反馈抑制中，抑制性神经元接收来自兴奋性神经元的输入，并将其投射回兴奋性神经元。 在上述的视觉通路中，几乎每一个兴奋性连接都伴随着前馈抑制、反馈抑制或两者兼有。例如LGN神经元直接激活V1 GABA释放神经元，对第4层兴奋性神经元提供前馈抑制，第4层兴奋性神经元也激活V1 GABA释放神经元，对自身提供反馈抑制。
三、侧抑制（Lateral inhibition） 侧抑制是一种广泛存在的环路模式。它通过放大平行通路之间的活性差异来选择要传递到下游环路的信息。
例如，脊椎动物视网膜中光感受器神经元激活水平细胞（horizontal cell），对附近的许多光感受器神经元进行反馈抑制。这一行为作用于下游神经节细胞的典型中央-周围感受区域，从而增强这些下游神经元对空间或颜色对比信息的提取能力
四、相互抑制（Mutual inhibition） 相互抑制被广泛应用于具有节律性活动的环路中，如涉及运动的环路。在更长的时间范围内，相互抑制也可以用来调节大脑状态，如睡眠-觉醒周期。
具有特定功能的特殊环路结构 一、连续性拓扑映射（Continuous topographic mapping） 连续拓扑映射是神经系统中呈现信息的一种常用组织方式。相邻的输入神经元通过轴突有序的投射连接到相邻的目标神经元。由于其稳健的发育机制，拓扑映射可以提供处理连续层次信息的便捷方式。它具有很多信息处理计算上的优势。
例如，视网膜拓扑映射通过侧抑制来促进局部对比度的提取能力，从而加强物体识别。此外，通过在相邻的地方安置环路元件，映射可以通过最小化环路长度来节省能源。“回旋式神经网络”（convolutional neural networks（包含回旋计算且具有深度结构的前馈神经网络，是深度学习的代表算法之一）的设计就借鉴了拓扑映射，极大地减少了调整人工神经网络所需的变量数，从而加快了计算速度。
二、离散并行处理（Discrete parallel processing） 离散并行处理通过离散的信息通道并行呈现和处理信号。
三、维度扩展（Dimensionality expansion） 在维度扩展结构中，来自相对少量的输入神经元的信号发散到数量多得多的输出神经元上（图5C），允许输出神经元呈现不同的信号输入组合。
待求证：这是否与 reservoir network 相关呢？该网络也是将低维的神经信号模式投射到高维空间，使得这些模式特征可以在高维空间中线性可分。
一个维度扩张的例子是内嗅皮层→齿状回颗粒细胞→CA3锥体神经环路。大量的齿状回颗粒细胞可以对来自内嗅皮层的关于空间和物体的信息进行模式分离，并由海马下游环路进一步处理。但与蘑菇体和小脑皮层不同，该环路不能进行学习和训练。这可能是因为海马区微环路进行的是无监督学习，而小脑和蘑菇体微环路进行的是有监督的强化学习。
四、循环回路（Recurrent loops） 神经系统充满了循环回路，在这些回路中，神经元通常通过中间神经元相互连接。这些循环回路从特定的神经区域到大脑的大部分区域，在规模上是不均一的。</p></div><div class=card-footer><span class=float-left>September 14, 2021</span>
<a href=/posts/life-learning/architectures_of_neuronal_circuits/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/continuous_attractor_nn/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[CN] Continuous-attractor Neural Network</h5><p class="card-text post-summary">Si Wu, Kosuke Hamaguchi, and Shun-ichi Amari. “Dynamics and computation of continuous attractors.” Neural computation 20.4 (2008): 994-1025.
本篇参考吴思老师公众号的文章《【学术思想】连续吸引子神经网络：神经信息表达的正则化网络模型》，并根据冷泉港亚洲暑期学校的讲座和智源学术沙龙的报告来详细阐释CANN的工作原理，全文用中文行文。
连续吸引子神经网络的数学模型 吸引子指的是一个动力学系统在不接受外界输入情况下靠自身动力学就能维持的非静息的稳定状态（active stationary state）。要构成一个的吸引子网络，需要两个基本条件：
神经元之间有兴奋性的互馈连接 (recurrent connection)，这样在没有外界输入的情况下，靠神经元之间的正反馈，网络就能维持住稳定活动；同时我们也要求兴奋性连接是局部的，这样才能形成有意义的空间局部活动； 网络中要有抑制性作用，这样才能避免系统活动因反复的正反馈而爆炸。 Hopfield模型采用吸引子的思想解释了大脑在收到部分或模糊信息条件下的联想式记忆机制。但经典的Hopfield模型没有考虑神经元之间连接的对称结构，因而其具有的吸引子（多个）在空间上是相互孤立的。
在吸引子网络的基础上，如果我们进一步要求神经元之间的连接具有空间平移不变性的对称结构，这时网络就将具有一簇连续的、而不是孤立的吸引子（注意，考虑到真实生物系统的神经数目足够大，为了方便，后面讨论都假设神经元数目是无穷的）；这些吸引子在参数空间上紧密排列，构成一个连续的状态子空间。
一维CANN数学模型 神经元的突触总输入$u$的动力学方程如下： $$ \tau \, \frac{du(x, t)}{dt} = -u(x,t) + \rho \int dx' J(x,x')\, r(x',t) + I_{ext} $$ 其中$x$表示神经元的参数空间位点，$u(x)$代表在参数空间位点x上的神经元的突触总输入，$r(x^′​,t)$为神经元($x'$)的发放率，由以下公式给出: $$ r(x,t) = \frac{u(x,t)^2}{1+k\rho\int{dx&rsquo;u(x',t)^2}} $$ 该模型没有单独考虑抑制性神经元，而是将其作用效果包含在除法归一化作用中。而神经元($x$)和($x'$)之间的兴奋性连接强度$J(x, x')$由高斯函数给出: $$ J(X,x') = \frac{1}{\sqrt{2\pi} \alpha} \, exp\left( - \frac{|x-x'|^2}{2\alpha^2}\right) $$ 我们看到其具有平移不变性，即其为$(x-x’)$的函数。外界输入$I_{ext}$与位置$z(t)$有关，公式如下： $$ I_{ext}=A \, exp \left[ - \frac{|x-z(t)|^2}{4\alpha^2}\right] $$</p></div><div class=card-footer><span class=float-left>August 30, 2021</span>
<a href=/posts/paper-reading/continuous_attractor_nn/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/how_to_model/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Approaching Scientific Question & How to Model</h5><p class="card-text post-summary">Marr’s 3 levels of analysis Brain: hierarchy of complexities
Computational level - 1 What is the objective of the system? How close is it to optimal? Algorithmic level - 2 What are the data structures? What are the approximations? What is the runtime? Implementation level - 3 What is the hardware? Neurons? Synapses? Molecules? Diversity of modeling goals Useful: good at solving real-world problems?</p></div><div class=card-footer><span class=float-left>August 29, 2021</span>
<a href=/posts/life-learning/how_to_model/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/task_representation_cognitive_tasks/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[CN] Task representations in neural networks trained to perform many cognitive tasks</h5><p class="card-text post-summary">Yang, G.R., Joglekar, M.R., Song, H.F. et al. Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci 22, 297–306 (2019). https://doi.org/10.1038/s41593-018-0310-2
Abstract They trained single network models to perform 20 cognitive tasks that depend on working memory, decision making, categorization, and inhibitory control.
What they find: recurrent units can develop into clusters that are functionally specialized for different cognitive processes compositionality of task representations: one task can be performed by recombining instructions for other tasks.</p></div><div class=card-footer><span class=float-left>August 24, 2021</span>
<a href=/posts/paper-reading/task_representation_cognitive_tasks/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/integrated_understanding_system/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[DL & CN] Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</h5><p class="card-text post-summary">McClelland, James L., et al. &ldquo;Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models.&rdquo; Proceedings of the National Academy of Sciences 117.42 (2020): 25966-25974.
Abstract For humans, language is a part of a system for understanding and communication about situations. There are some domain-general principles of biological neural network:
connection-based learning distributed representation context-sensitive mutual constraint satisfaction What they propose: the organization of the brain&rsquo;s distributed understanding system, which includes a fast learning system that addresses the memory problem.</p></div><div class=card-footer><span class=float-left>August 18, 2021</span>
<a href=/posts/paper-reading/integrated_understanding_system/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/lecture_james_mcclleland/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Lecture: Are People Still Smarter than Machines?</h5><p class="card-text post-summary">How can a neural network learn to do something cognitively interesting? The Biologically Inspired Approach If neuron A participates in firing neuron B, strengthen the connection from A to B The Optimization Approach Adjust each connection to minimize the network&rsquo;s deviation from a desired output Backpropogation algorithm became the basis of sbsequent research in the PDP framework, as well as almost all of Deep Learning Their Early Success of the Approach Models that could learn to read words and generalize to pronounceable non-words, capturing human-like response patterns: they refect statistical patterns and similarity relationships Models that captured many aspects of human semantic cognition and the disintegration of human semantic abilities resulting from neurodegenerative disease Models that showed in principle how aspects of language knowledge could be captured in a simple artificial neural network Nowadays AlexNet, ResNet and so on.</p></div><div class=card-footer><span class=float-left>August 17, 2021</span>
<a href=/posts/life-learning/lecture_james_mcclleland/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/neural-computation/basic-neuro-knowledge/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Neuroscience Basic Knowledge</h5><p class="card-text post-summary">The LIF model LIF: Leaky Integrated-and -Fire
1. Membrane Equation $$ \begin{align*} \
&\tau_m\,\frac{d}{dt}\,V(t) = E_{L} - V(t) + R\,I(t) &\text{if }\quad V(t) \leq V_{th} \\
\\
&V(t) = V_{reset} &\text{otherwise}\
\
\end{align*} $$
where $V(t)$ is the membrane potential(膜电势), $\tau_m$ is the membrane time constant, $E_{L}$ is the leak potential, $R$ is the membrane resistance, $I(t)$ is the synaptic input current, $V_{th}$ is the firing threshold, and $V_{reset}$ is the reset voltage.</p></div><div class=card-footer><span class=float-left>August 9, 2021</span>
<a href=/posts/neural-computation/basic-neuro-knowledge/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/life-learning/lecture_yao_xin/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>每日学习———类脑智能的另一种思路</h5><p class="card-text post-summary">写在前面 作为不那么学术的博客主题，我将使用中文作为主题的第一语言。终身学习可以让我不局限于自己所研究的范围中，可以和各式各样的人进行交流。
姚新教授的报告——类脑智能研究的新思路 演化计算 姚老师在报告中提出了一个非常犀利的问题，我们的终极目标是创造出一个人工大脑，但大脑是自然演化的产物，大脑进化的过程是否被当今的脑科学研究忽视了？ 让我们停下来思考一下，当今的ANN都是由专家去构造一个结构，并实现某种功能，这种结构往往是人为预设好的，只需要对参数进行调整。最近也有很多自动训练模型的出现，甚至还有李沐教授设计出来的AutoGlon，可以针对某一个task自动选择一个算法来完成任务，某些任务的准确率可以在Kaggle上有很高的排名。但人脑的构造远远不是我们可以单纯设计出来的，这也是姚老师所质疑的一点：我们真的可以单纯地构造出一个大脑出来吗？ 大脑是进化的产物，是有一个过程的，而这个过程却被我们所忽视了。</p></div><div class=card-footer><span class=float-left>August 9, 2021</span>
<a href=/posts/life-learning/lecture_yao_xin/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/reservoir_decision_making/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[BIC] A brain-inspired computational model for spatio-temporal information processing</h5><p class="card-text post-summary">Lin, Xiaohan, et al. &ldquo;A brain-inspired computational model for spatio-temporal information processing.&rdquo; Neural Networks 143 (2021): 74-87.
Abstract Current method: Explicit feature extraction, which requires lots of labeled data. Novel brain-inspired computational model: Reservoir Decision-making Network (RDMN) A reservoir model: projects complex spatio-temporal patterns into spatially separated neural representations via its recurrent dynamics. (regarded it as SVM) A decision-making model: reads out neural representations via integrating information over time.</p></div><div class=card-footer><span class=float-left>July 20, 2021</span>
<a href=/posts/paper-reading/reservoir_decision_making/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/paper-reading/push-pull_feedback/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>[BIC] Push-pull Feedback Implements Hierarchical Information Retrieval Efficiently</h5><p class="card-text post-summary">Liu, Xiao, et al. &ldquo;Push-pull feedback implements hierarchical information retrieval efficiently.&rdquo; Advances in Neural Information Processing Systems 32 (2019): 5701-5710.
Front Word To understand this paper, you need a strong neuroscience background, especially knowing the Hopfield model and Hebbian theory. So before reading this paper, please preview the theories mentioned above!
To be honest, I still cannot understand the details quite well LoL :)
Abstract In addition to feedforward connections, there exist abundant feedback connections in a neural pathway.</p></div><div class=card-footer><span class=float-left>July 19, 2021</span>
<a href=/posts/paper-reading/push-pull_feedback/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=/posts/deep-learning/self-attention/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/images/default-hero.jpg></div><div class=card-body><h5 class=card-title>Self-attention</h5><p class="card-text post-summary">I. Self-attention Overview Input: A sequence of vectors (size not fixed) Output: Each vector has a label (POS tagging) The whole sequence has a label (sentiment analysis) Model decides the number of labels itself (seq2seq) Self-attention can handle global iformation, and FC can handle local information. Self-attention is the key module of Transformer, which will be shared in other articles.
II. How to work? Firstly, we should figure out the relevance between each vector.</p></div><div class=card-footer><span class=float-left>June 11, 2021</span>
<a href=/posts/deep-learning/self-attention/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator><ul class=pagination><li class=page-item><a href=/posts/ class=page-link aria-label=First><span aria-hidden=true>&#171;&#171;</span></a></li><li class="page-item disabled"><a class=page-link aria-label=Previous><span aria-hidden=true>&#171;</span></a></li><li class="page-item active"><a class=page-link href=/posts/>1</a></li><li class=page-item><a class=page-link href=/posts/page/2/>2</a></li><li class=page-item><a class=page-link href=/posts/page/3/>3</a></li><li class=page-item><a class=page-link href=/posts/page/4/>4</a></li><li class=page-item><a href=/posts/page/2/ class=page-link aria-label=Next><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/posts/page/4/ class=page-link aria-label=Last><span aria-hidden=true>&#187;&#187;</span></a></li></ul></div></div></section></div><footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=#about>About</a></li><li class=nav-item><a class=smooth-scroll href=#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=#achievements>Achievements</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><span>Email:</span> <span>ztqakita@163.com</span></li><li><span>Phone:</span> <span>(+86)18618180071</span></li></ul></div><div class="col-md-4 col-sm-12"><p>Stay up to date with email notification</p><form><div class=form-group><input type=email class=form-control id=exampleInputEmail1 aria-describedby=emailHelp placeholder="Enter email">
<small id=emailHelp class="form-text text-muted">We'll never share your email with anyone else.</small></div><button type=submit class="btn btn-info">Submit</button></form></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=#><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png>
Toha</a></div><div class="col-md-4 text-center">© 2021 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/js/jquery-3.4.1.min.js></script><script src=/js/popper.min.js></script><script src=/js/bootstrap.min.js></script><script src=/js/navbar.js></script><script src=/js/main.js></script><script src=/js/list.js></script></body></html>
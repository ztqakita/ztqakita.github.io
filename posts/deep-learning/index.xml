<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DL on ztqakita's Blog</title><link>https://ztqakita.github.io/posts/deep-learning/</link><description>Recent content in DL on ztqakita's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 Oct 2020 06:00:20 +0600</lastBuildDate><atom:link href="https://ztqakita.github.io/posts/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Recursive Neural Network</title><link>https://ztqakita.github.io/posts/deep-learning/rnn/</link><pubDate>Thu, 15 Oct 2020 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/deep-learning/rnn/</guid><description>Recursive Neural Network I. RNN Structure Overview The input is a sequence of vectors. note: Changing the input sequence order will change the output
We use the same neural network to train, each color in NN means the same weight. When the values stored in the memory is different, the output will also be different. II. Types of RNN Elman&amp;rsquo;s memory restore the values of hidden layer output, and Jordan&amp;rsquo;s memory restore the values of output.</description></item><item><title>Convolutional Neural Network</title><link>https://ztqakita.github.io/posts/deep-learning/cnn/</link><pubDate>Mon, 14 Sep 2020 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/deep-learning/cnn/</guid><description>Convolutional Neural Network I. CNN Structure Overview II. Convolution Note: 1.Every elements in filter are the network parameter to be learned. 2.Stride means each step you walk from previous position. 3.The size of filter is decided by programmer.
From the picture we could know the largest values of Feature Map means there has a feature. Then we do the same process for every filter and generate more Feature Map.</description></item><item><title>Introduction to Deep Learning</title><link>https://ztqakita.github.io/posts/deep-learning/introduction-to-deep-learning/</link><pubDate>Mon, 10 Aug 2020 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/deep-learning/introduction-to-deep-learning/</guid><description>Introduction to Deep Learning I. Basic Concepts 1. Fully Connected Feedforward Network 2. Matrix Operation Every layer has weight matrix and bias matrix, using matrix operation we can accumulate the output matrix $y$. Tips: Using GPU could speed up matrix operation.
II. Why Deep Learning? 1. Modularization 对neural network而言，并不是神经元越多越好，通过例子可以看出层数的增加（more deep）对于准确率的提升更有效果。这其中就是 Modularization 的思想。For example, while you are trying to train the model below, you can use basic classifiers as module.</description></item><item><title>Optimization for Deep Learning</title><link>https://ztqakita.github.io/posts/deep-learning/optimizer-for-dl/</link><pubDate>Sat, 25 Jul 2020 06:00:20 +0600</pubDate><guid>https://ztqakita.github.io/posts/deep-learning/optimizer-for-dl/</guid><description>New Optimizers for Deep Learning Some Notation: $\theta_t$: model parameter at time step t $\nabla$$L(\theta_t)$ or $g_t$: gradient at $\theta_t$, used to compute $\theta_{t+1}$ $m_{t+1}$: momentum accumlated from time step 0 to time step t, which is used to compute $\theta_{t+1}$
I. Adaptive Learning Rates In gradient descent, we need to set the learning rate to converge properly and find the local minima. But sometimes it&amp;rsquo;s difficult to find a proper value of the learning rate.</description></item></channel></rss>
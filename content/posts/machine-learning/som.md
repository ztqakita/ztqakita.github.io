---
title: "Self-organizing Map"
date: 2021-10-14T06:00:20+06:00
hero: /images/posts/DL/139-deep-learning.svg
menu:
  sidebar:
    name: SOM
    identifier: som
    parent: machine-learning
    weight: 10
math: true
---

## 自组织特征映射SOM

引言：SOM是一种典型的“无监督学习”模型，一般用法是将高维的input data在低位空间表示，也是一种降维方法。

> 值得注意的是在《Few-Shot Class-Incremental Learning》这篇论文中，作者提出的是
**Topology-preserving Knowledge Incremental framework**也是一种自适应的聚类算法，但它的创新点在于**incremental learning**，可以为新增的分类来增加特征空间的质点，并不改变原有的拓扑结构。

> 除SOM之外，其他常见的自组织（竞争型）神经网络还有Counter propogation and Adaptive Resonance Theory等

### 生物学背景
生物学研究表明，在人脑的感觉通道上，神经元的组织原理是有序排列的。当外界的特定时空信息输入时，大脑皮层的特定区域兴奋，而且类似的外界信息在对应的区域是连续映像的。生物视网膜中有许多特定的细胞对特定的图形比较敏感，当视网膜中有若干个接收单元同时受特定模式刺激时，就使大脑皮层中的特定神经元开始兴奋，输入模式接近，与之对应的兴奋神经元也接近；在听觉通道上，神经元在结构排列上与频率的关系十分密切，对于某个频率，特定的神经元具有最大的响应，位置相邻的神经元具有相近的频率特征，而远离的神经元具有的频率特征差别也较大。大脑皮层中神经元的这种响应特点不是先天安排好的，而是通过后天的学习自组织形成的。

SOM模拟了生物神经系统的侧抑制现象，一个神经细胞兴奋以后，会对周围其他神经细胞产生抑制作用。

在网络结构上来看，它的经典范式非常简单：
- 一层输入层
- 一层竞争层

两层之间各神经元实现双向连接，竞争层之间的神经元还存在横向连接。

在学习算法上，不同于MLP以网络误差为训练准则，而是**模拟生物神经元之间的兴奋、协调与抑制、竞争作用的信息处理的动力学原理来指导网络的学习**。

> **必须要说明的是如今的som网络设计可以突破传统的结构，我觉得引入reservoir network等网络可以做到更多的事情。**

### 竞争学习的概念与原理
![](/images/posts/ML/WTA.png)

Winner-Take-All 规则就是网络的输出神经元之间相互竞争以求被激活，结果在每一时刻只有一个输出神经元被激活。

> 我在这里想到了Decision-making model的相关特性，相邻神经元之间使用动力学方程建立相互作用的过程，我认为这种建模方式更有生物可解释性，从那篇gait recognition论文中也可以看出优势。

对于SOM，获胜神经元的获取来源于权重向量和输入模式向量的相似性度量，最相似的权重向量判别为竞争获胜神经元。

对于知道哪个神经元获胜之后，我们可以调整神经元的输出和训练调整权重：

竞争学习的步骤为：
1. 向量归一化
2. 寻找获胜神经元
3. 网络输出与权值调整

步骤3完成以后返回步骤1继续训练知道学习率衰减至0。

### SOM 典型结构
![](/images/posts/ML/soms.png)

#### SOM网权值调整




